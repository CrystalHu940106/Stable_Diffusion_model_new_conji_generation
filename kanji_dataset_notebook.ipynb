{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸ“š Kanji Dataset Processing Components\n",
    "\n",
    "**Missing components for Kanji generation task**  \n",
    "This notebook provides the complete dataset processing pipeline for real Kanji generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required dependencies\n",
    "!pip install requests svgpathtools cairosvg pillow tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kanji Dataset Processing Components\n",
    "# This code provides the missing components for real Kanji generation\n",
    "\n",
    "import xml.etree.ElementTree as ET\n",
    "import requests\n",
    "import svgpathtools\n",
    "from cairosvg import svg2png\n",
    "from PIL import Image\n",
    "import io\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import os\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "\n",
    "class KanjiDataProcessor:\n",
    "    \"\"\"\n",
    "    Process KANJIDIC2 and KanjiVG data for Stable Diffusion training\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, data_dir=\"kanji_data\"):\n",
    "        self.data_dir = data_dir\n",
    "        os.makedirs(data_dir, exist_ok=True)\n",
    "        \n",
    "    def download_kanjidic2(self):\n",
    "        \"\"\"Download KANJIDIC2 XML file\"\"\"\n",
    "        url = \"http://www.edrdg.org/kanjidic/kanjidic2.xml.gz\"\n",
    "        print(\"ðŸ“¥ Downloading KANJIDIC2...\")\n",
    "        \n",
    "        import gzip\n",
    "        response = requests.get(url)\n",
    "        \n",
    "        # Decompress and save\n",
    "        xml_path = os.path.join(self.data_dir, \"kanjidic2.xml\")\n",
    "        with gzip.open(io.BytesIO(response.content), 'rb') as gz:\n",
    "            with open(xml_path, 'wb') as f:\n",
    "                f.write(gz.read())\n",
    "        \n",
    "        print(f\"âœ… KANJIDIC2 saved to {xml_path}\")\n",
    "        return xml_path\n",
    "    \n",
    "    def parse_kanjidic2(self, xml_path):\n",
    "        \"\"\"Parse KANJIDIC2 XML to extract kanji and English meanings\"\"\"\n",
    "        print(\"ðŸ“– Parsing KANJIDIC2...\")\n",
    "        \n",
    "        tree = ET.parse(xml_path)\n",
    "        root = tree.getroot()\n",
    "        \n",
    "        kanji_data = {}\n",
    "        \n",
    "        for character in root.findall('.//character'):\n",
    "            # Get the kanji character\n",
    "            literal = character.find('.//literal')\n",
    "            if literal is None:\n",
    "                continue\n",
    "            \n",
    "            kanji = literal.text\n",
    "            \n",
    "            # Get English meanings\n",
    "            meanings = []\n",
    "            reading_meaning = character.find('.//reading_meaning')\n",
    "            if reading_meaning:\n",
    "                for meaning in reading_meaning.findall('.//meaning'):\n",
    "                    # Only get meanings without language attribute (English)\n",
    "                    if 'm_lang' not in meaning.attrib:\n",
    "                        meanings.append(meaning.text)\n",
    "            \n",
    "            if meanings:\n",
    "                # Create a short definition from meanings\n",
    "                definition = \", \".join(meanings[:3])  # Use first 3 meanings\n",
    "                kanji_data[kanji] = {\n",
    "                    'character': kanji,\n",
    "                    'definition': definition,\n",
    "                    'all_meanings': meanings\n",
    "                }\n",
    "        \n",
    "        print(f\"âœ… Parsed {len(kanji_data)} kanji characters\")\n",
    "        return kanji_data\n",
    "    \n",
    "    def download_kanjivg(self, kanji_list):\n",
    "        \"\"\"Download KanjiVG SVG files for specific kanji\"\"\"\n",
    "        base_url = \"https://raw.githubusercontent.com/KanjiVG/kanjivg/master/kanji/\"\n",
    "        svg_dir = os.path.join(self.data_dir, \"svg\")\n",
    "        os.makedirs(svg_dir, exist_ok=True)\n",
    "        \n",
    "        print(\"ðŸ“¥ Downloading KanjiVG SVG files...\")\n",
    "        \n",
    "        downloaded = []\n",
    "        for kanji in tqdm(kanji_list):\n",
    "            # Convert kanji to Unicode hex\n",
    "            unicode_hex = format(ord(kanji), '05x')\n",
    "            svg_filename = f\"{unicode_hex}.svg\"\n",
    "            svg_url = base_url + svg_filename\n",
    "            svg_path = os.path.join(svg_dir, svg_filename)\n",
    "            \n",
    "            try:\n",
    "                if not os.path.exists(svg_path):\n",
    "                    response = requests.get(svg_url)\n",
    "                    if response.status_code == 200:\n",
    "                        with open(svg_path, 'w', encoding='utf-8') as f:\n",
    "                            f.write(response.text)\n",
    "                        downloaded.append((kanji, svg_path))\n",
    "                else:\n",
    "                    downloaded.append((kanji, svg_path))\n",
    "            except Exception as e:\n",
    "                print(f\"Failed to download {kanji}: {e}\")\n",
    "        \n",
    "        print(f\"âœ… Downloaded {len(downloaded)} SVG files\")\n",
    "        return downloaded\n",
    "    \n",
    "    def process_svg_to_image(self, svg_path, size=128):\n",
    "        \"\"\"\n",
    "        Convert SVG to pixel image with pure black strokes\n",
    "        Remove stroke order numbers\n",
    "        \"\"\"\n",
    "        with open(svg_path, 'r', encoding='utf-8') as f:\n",
    "            svg_content = f.read()\n",
    "        \n",
    "        # Remove stroke order numbers (text elements)\n",
    "        import re\n",
    "        # Remove text elements that contain stroke numbers\n",
    "        svg_content = re.sub(r'<text[^>]*>.*?</text>', '', svg_content, flags=re.DOTALL)\n",
    "        \n",
    "        # Ensure pure black strokes\n",
    "        svg_content = svg_content.replace('stroke=\"red\"', 'stroke=\"#000000\"')\n",
    "        svg_content = svg_content.replace('stroke=\"orange\"', 'stroke=\"#000000\"')\n",
    "        \n",
    "        # Add explicit black stroke if not present\n",
    "        if 'stroke=' not in svg_content:\n",
    "            svg_content = svg_content.replace('<path', '<path stroke=\"#000000\" fill=\"none\" stroke-width=\"3\"')\n",
    "        \n",
    "        # Convert to PNG\n",
    "        png_data = svg2png(\n",
    "            bytestring=svg_content.encode('utf-8'),\n",
    "            output_width=size,\n",
    "            output_height=size,\n",
    "            background_color='white'\n",
    "        )\n",
    "        \n",
    "        # Convert to PIL Image\n",
    "        image = Image.open(io.BytesIO(png_data))\n",
    "        \n",
    "        # Ensure it's pure black and white\n",
    "        image = image.convert('L')  # Convert to grayscale\n",
    "        image_array = np.array(image)\n",
    "        \n",
    "        # Threshold to ensure pure black strokes\n",
    "        threshold = 128\n",
    "        image_array = np.where(image_array < threshold, 0, 255).astype(np.uint8)\n",
    "        \n",
    "        # Convert back to PIL Image\n",
    "        image = Image.fromarray(image_array)\n",
    "        \n",
    "        # Convert to RGB (required for Stable Diffusion)\n",
    "        image = image.convert('RGB')\n",
    "        \n",
    "        return image\n",
    "    \n",
    "    def create_dataset(self, num_samples=None):\n",
    "        \"\"\"Create the complete dataset\"\"\"\n",
    "        print(\"ðŸ—ï¸ Creating Kanji dataset...\")\n",
    "        \n",
    "        # Download and parse KANJIDIC2\n",
    "        if not os.path.exists(os.path.join(self.data_dir, \"kanjidic2.xml\")):\n",
    "            xml_path = self.download_kanjidic2()\n",
    "        else:\n",
    "            xml_path = os.path.join(self.data_dir, \"kanjidic2.xml\")\n",
    "        \n",
    "        kanji_data = self.parse_kanjidic2(xml_path)\n",
    "        \n",
    "        # Limit samples if specified\n",
    "        if num_samples:\n",
    "            kanji_list = list(kanji_data.keys())[:num_samples]\n",
    "        else:\n",
    "            kanji_list = list(kanji_data.keys())\n",
    "        \n",
    "        # Download SVG files\n",
    "        svg_files = self.download_kanjivg(kanji_list)\n",
    "        \n",
    "        # Process images\n",
    "        dataset = []\n",
    "        print(\"ðŸŽ¨ Converting SVG to images...\")\n",
    "        \n",
    "        for kanji, svg_path in tqdm(svg_files):\n",
    "            try:\n",
    "                # Convert SVG to image\n",
    "                image = self.process_svg_to_image(svg_path, size=128)\n",
    "                \n",
    "                # Get definition\n",
    "                definition = kanji_data[kanji]['definition']\n",
    "                \n",
    "                ",
    "                # Add to dataset\n",
    "                dataset.append({\n",
    "                    'kanji': kanji,\n",
    "                    'definition': definition,\n",
    "                    'image': image,\n",
    "                    'meanings': kanji_data[kanji]['all_meanings']\n",
    "                })\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Failed to process {kanji}: {e}\")\n",
    "        \n",
    "        print(f\"âœ… Created dataset with {len(dataset)} entries\")\n",
    "        \n",
    "        # Save dataset metadata\n",
    "        metadata_path = os.path.join(self.data_dir, \"dataset_metadata.json\")\n",
    "        metadata = [\n",
    "            {\n",
    "                'kanji': item['kanji'],\n",
    "                'definition': item['definition'],\n",
    "                'meanings': item['meanings']\n",
    "            }\n",
    "            for item in dataset\n",
    "        ]\n",
    "        \n",
    "        with open(metadata_path, 'w', encoding='utf-8') as f:\n",
    "            json.dump(metadata, f, ensure_ascii=False, indent=2)\n",
    "        \n",
    "        print(f\"ðŸ’¾ Metadata saved to {metadata_path}\")\n",
    "        \n",
    "        return dataset\n",
    "\n",
    "\n",
    "class KanjiDataset(Dataset):\n",
    "    \"\"\"PyTorch Dataset for Kanji-Definition pairs\"\"\"\n",
    "    \n",
    "    def __init__(self, data, transform=None):\n",
    "        self.data = data\n",
    "        self.transform = transform\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        item = self.data[idx]\n",
    "        image = item['image']\n",
    "        definition = item['definition']\n",
    "        \n",
    "        # Convert image to tensor\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        else:\n",
    "            # Default transformation\n",
    "            image = np.array(image).astype(np.float32) / 255.0\n",
    "            image = (image - 0.5) * 2  # Normalize to [-1, 1]\n",
    "            image = torch.from_numpy(image).permute(2, 0, 1)\n",
    "        \n",
    "        return {\n",
    "            'image': image,\n",
    "            'text': definition,\n",
    "            'kanji': item['kanji']\n",
    "        }\n",
    "\n",
    "\n",
    "def integrate_with_trainer(trainer, dataset):\n",
    "    \"\"\"\n",
    "    Integrate real Kanji dataset with the existing trainer\n",
    "    Replace the synthetic dataset with real data\n",
    "    \"\"\"\n",
    "    print(\"ðŸ”„ Integrating real Kanji dataset with trainer...\")\n",
    "    \n",
    "    # Create PyTorch dataset\n",
    "    kanji_dataset = KanjiDataset(dataset)\n",
    "    \n",
    "    # Create dataloader\n",
    "    dataloader = DataLoader(\n",
    "        kanji_dataset,\n",
    "        batch_size=trainer.batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=2\n",
    "    )\n",
    "    \n",
    "    # Modify trainer to use real dataset\n",
    "    trainer.kanji_dataloader = dataloader\n",
    "    trainer.kanji_dataset = dataset\n",
    "    \n",
    "    print(\"âœ… Dataset integrated successfully\")\n",
    "    return dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ§ª Test the Dataset Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the dataset processing\n",
    "if __name__ == \"__main__\":\n",
    "    # Initialize data processor\n",
    "    processor = KanjiDataProcessor()\n",
    "    \n",
    "    # Create dataset (limit to 100 for testing)\n",
    "    dataset = processor.create_dataset(num_samples=100)\n",
    "    \n",
    "    # Show sample\n",
    "    if dataset:\n",
    "        sample = dataset[0]\n",
    "        print(f\"\\nðŸ“Š Sample entry:\")\n",
    "        print(f\"  Kanji: {sample['kanji']}\")\n",
    "        print(f\"  Definition: {sample['definition']}\")\n",
    "        print(f\"  Image size: {sample['image'].size}\")\n",
    "        \n",
    "        # Display the image\n",
    "        import matplotlib.pyplot as plt\n",
    "        plt.figure(figsize=(6, 6))\n",
    "        plt.imshow(sample['image'], cmap='gray')\n",
    "        plt.title(f'Kanji: {sample[\"kanji\"]} - {sample[\"definition\"]}', fontsize=12)\n",
    "        plt.axis('off')\n",
    "        plt.show()\n",
    "    \n",
    "    print(f\"\\nâœ… Dataset processing test completed!\")\n",
    "        print(f\"   â€¢ Total entries: {len(dataset)}\")\n",
    "        print(f\"   â€¢ Data directory: {processor.data_dir}\")\n",
    "        print(f\"\\nðŸ’¡ To integrate with existing trainer:\")\n",
    "        print(f\"   trainer = ColabOptimizedTrainer()\")\n",
    "        print(f\"   dataloader = integrate_with_trainer(trainer, dataset)\")\n",
    "        print(f\"   trainer.train()  # Now uses real Kanji data\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
