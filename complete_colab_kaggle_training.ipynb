{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 🚀 Complete Stable Diffusion Kanji Generation - Colab/Kaggle\n",
    "\n",
    "**Single file training notebook** - Upload to Colab/Kaggle and start training immediately!\n",
    "\n",
    "## 🎯 Features\n",
    "- ✅ **Complete Training Pipeline**: VAE + UNet + DDPM\n",
    "- 🚀 **GPU Optimized**: Auto CUDA/MPS detection\n",
    "- 💾 **Auto-save**: Checkpoints every 5 epochs\n",
    "- 📊 **Real-time Monitoring**: Loss curves and GPU stats\n",
    "- 🔄 **Resume Training**: Continue from any checkpoint\n",
    "- 🎌 **Kanji Generation**: Text-to-Kanji capabilities\n",
    "\n",
    "## 🚀 Quick Start\n",
    "1. Upload this notebook to Colab/Kaggle\n",
    "2. Select GPU runtime\n",
    "3. Run all cells\n",
    "4. Start training!\n",
    "\n",
    "**Expected Training Time**:\n",
    "- Colab Free (T4): 50 epochs in 2-3 hours\n",
    "- Colab Pro (V100/P100): 50 epochs in 1-1.5 hours\n",
    "- Kaggle (P100): 50 epochs in 1-2 hours"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 📦 Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install transformers pillow matplotlib scikit-image opencv-python tqdm\n",
    "!pip install --upgrade torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
    "\n",
    "print(\"✅ Dependencies installed successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🔧 Check GPU and Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "\n",
    "# Check environment\n",
    "is_colab = 'COLAB_GPU' in os.environ\n",
    "is_kaggle = 'KAGGLE_KERNEL_RUN_TYPE' in os.environ\n",
    "\n",
    "print(f\"🌐 Environment: {'Colab' if is_colab else 'Kaggle' if is_kaggle else 'Local'}\")\n",
    "print(f\"PyTorch: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "elif hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():\n",
    "    print(\"🍎 Apple Silicon (MPS) available\")\n",
    "else:\n",
    "    print(\"⚠️ Using CPU (will be slow!)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🏗️ improved_stable_diffusion.py Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "改进的Stable Diffusion实现\n",
    "借鉴官方CompVis/stable-diffusion的最佳实践\n",
    "\"\"\"\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from transformers import CLIPTokenizer, CLIPTextModel\n",
    "import math\n",
    "from typing import Optional, Union, Tuple\n",
    "import numpy as np\n",
    "\n",
    "class ImprovedVAE(nn.Module):\n",
    "    \"\"\"\n",
    "    改进的VAE实现，借鉴官方架构\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels=3, latent_channels=4, hidden_dims=[128, 256, 512, 1024]):\n",
    "        super().__init__()\n",
    "        self.latent_channels = latent_channels\n",
    "        \n",
    "        # Encoder - 使用更深的网络\n",
    "        encoder_layers = []\n",
    "        in_ch = in_channels\n",
    "        for h_dim in hidden_dims:\n",
    "            # 计算合适的GroupNorm组数\n",
    "            num_groups = min(32, h_dim)\n",
    "            while h_dim % num_groups != 0 and num_groups > 1:\n",
    "                num_groups -= 1\n",
    "            \n",
    "            encoder_layers.extend([\n",
    "                nn.Conv2d(in_ch, h_dim, kernel_size=3, stride=2, padding=1),\n",
    "                nn.GroupNorm(num_groups, h_dim),  # 使用GroupNorm替代BatchNorm\n",
    "                nn.SiLU()  # 使用SiLU替代LeakyReLU\n",
    "            ])\n",
    "            in_ch = h_dim\n",
    "        \n",
    "        # Final encoding layer\n",
    "        final_channels = latent_channels * 2\n",
    "        num_groups = min(32, final_channels)\n",
    "        while final_channels % num_groups != 0 and num_groups > 1:\n",
    "            num_groups -= 1\n",
    "        \n",
    "        encoder_layers.extend([\n",
    "            nn.Conv2d(hidden_dims[-1], final_channels, kernel_size=3, padding=1),\n",
    "            nn.GroupNorm(num_groups, final_channels)\n",
    "        ])\n",
    "        \n",
    "        self.encoder = nn.Sequential(*encoder_layers)\n",
    "        \n",
    "        # Decoder - 确保精确的128x128输出\n",
    "        decoder_layers = []\n",
    "        in_ch = latent_channels\n",
    "        \n",
    "        # 使用hidden_dims的反序进行上采样\n",
    "        hidden_dims_rev = hidden_dims[::-1]\n",
    "        \n",
    "        for i, h_dim in enumerate(hidden_dims_rev):\n",
    "            # 计算合适的GroupNorm组数\n",
    "            num_groups = min(32, h_dim)\n",
    "            while h_dim % num_groups != 0 and num_groups > 1:\n",
    "                num_groups -= 1\n",
    "            \n",
    "            decoder_layers.extend([\n",
    "                nn.ConvTranspose2d(in_ch, h_dim, kernel_size=4, stride=2, padding=1),\n",
    "                nn.GroupNorm(num_groups, h_dim),\n",
    "                nn.SiLU()\n",
    "            ])\n",
    "            in_ch = h_dim\n",
    "        \n",
    "        # 最终输出层\n",
    "        decoder_layers.extend([\n",
    "            nn.Conv2d(in_ch, in_channels, kernel_size=3, stride=1, padding=1),\n",
    "            nn.Tanh()\n",
    "        ])\n",
    "        \n",
    "        self.decoder = nn.Sequential(*decoder_layers)\n",
    "        \n",
    "    def encode(self, x):\n",
    "        # 确保输入是128x128\n",
    "        if x.shape[-1] != 128:\n",
    "            x = F.interpolate(x, size=(128, 128), mode='bilinear', align_corners=False)\n",
    "        \n",
    "        # 编码到潜在空间\n",
    "        encoded = self.encoder(x)\n",
    "        mu, logvar = torch.chunk(encoded, 2, dim=1)\n",
    "        \n",
    "        # KL散度损失\n",
    "        kl_loss = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "        \n",
    "        # 重参数化技巧\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        z = mu + eps * std\n",
    "        \n",
    "        return z, mu, logvar, kl_loss\n",
    "    \n",
    "    def decode(self, z):\n",
    "        return self.decoder(z)\n",
    "\n",
    "class ImprovedCrossAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    改进的交叉注意力实现，借鉴官方版本\n",
    "    \"\"\"\n",
    "    def __init__(self, query_dim, context_dim=None, heads=8, dim_head=64, dropout=0.0):\n",
    "        super().__init__()\n",
    "        inner_dim = dim_head * heads\n",
    "        context_dim = context_dim if context_dim is not None else query_dim\n",
    "        \n",
    "        self.scale = dim_head ** -0.5\n",
    "        self.heads = heads\n",
    "        \n",
    "        self.to_q = nn.Linear(query_dim, inner_dim, bias=False)\n",
    "        self.to_k = nn.Linear(context_dim, inner_dim, bias=False)\n",
    "        self.to_v = nn.Linear(context_dim, inner_dim, bias=False)\n",
    "        \n",
    "        self.to_out = nn.Sequential(\n",
    "            nn.Linear(inner_dim, query_dim),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x, context=None):\n",
    "        context = context if context is not None else x\n",
    "        \n",
    "        q = self.to_q(x)\n",
    "        k = self.to_k(context)\n",
    "        v = self.to_v(context)\n",
    "        \n",
    "        # 重塑为多头注意力\n",
    "        q = q.view(q.shape[0], -1, self.heads, q.shape[-1] // self.heads).transpose(1, 2)\n",
    "        k = k.view(k.shape[0], -1, self.heads, k.shape[-1] // self.heads).transpose(1, 2)\n",
    "        v = v.view(v.shape[0], -1, self.heads, v.shape[-1] // self.heads).transpose(1, 2)\n",
    "        \n",
    "        # 计算注意力\n",
    "        scores = torch.matmul(q, k.transpose(-2, -1)) * self.scale\n",
    "        attn = F.softmax(scores, dim=-1)\n",
    "        \n",
    "        # 应用注意力\n",
    "        out = torch.matmul(attn, v)\n",
    "        out = out.transpose(1, 2).contiguous().view(out.shape[0], -1, out.shape[-1] * self.heads)\n",
    "        \n",
    "        return self.to_out(out)\n",
    "\n",
    "class ImprovedResBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    改进的残差块，借鉴官方实现\n",
    "    \"\"\"\n",
    "    def __init__(self, channels, time_dim, dropout=0.0):\n",
    "        super().__init__()\n",
    "        \n",
    "        # 动态计算GroupNorm的组数，确保channels能被num_groups整除\n",
    "        if channels >= 32:\n",
    "            num_groups = min(32, channels // (channels // 32))\n",
    "        elif channels >= 16:\n",
    "            num_groups = min(16, channels // (channels // 16))\n",
    "        elif channels >= 8:\n",
    "            num_groups = min(8, channels // (channels // 8))\n",
    "        elif channels >= 4:\n",
    "            num_groups = min(4, channels // (channels // 4))\n",
    "        else:\n",
    "            num_groups = 1\n",
    "        \n",
    "        # 确保num_groups能整除channels\n",
    "        while channels % num_groups != 0 and num_groups > 1:\n",
    "            num_groups -= 1\n",
    "        \n",
    "        self.time_mlp = nn.Sequential(\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(time_dim, channels)\n",
    "        )\n",
    "        \n",
    "        self.block1 = nn.Sequential(\n",
    "            nn.GroupNorm(num_groups, channels),\n",
    "            nn.SiLU(),\n",
    "            nn.Conv2d(channels, channels, 3, padding=1)\n",
    "        )\n",
    "        \n",
    "        self.block2 = nn.Sequential(\n",
    "            nn.GroupNorm(num_groups, channels),\n",
    "            nn.SiLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Conv2d(channels, channels, 3, padding=1)\n",
    "        )\n",
    "        \n",
    "        # 时间嵌入投影\n",
    "        self.time_proj = nn.Linear(time_dim, channels)\n",
    "        \n",
    "    def forward(self, x, time_emb):\n",
    "        h = self.block1(x)\n",
    "        \n",
    "        # 时间嵌入处理\n",
    "        time_emb = self.time_proj(time_emb)\n",
    "        time_emb = time_emb.view(x.shape[0], -1, 1, 1)\n",
    "        h = h + time_emb\n",
    "        \n",
    "        h = self.block2(h)\n",
    "        return h + x\n",
    "\n",
    "class ImprovedUNet2DConditionModel(nn.Module):\n",
    "    \"\"\"\n",
    "    改进的UNet实现，借鉴官方架构\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels=4, out_channels=4, model_channels=128, num_res_blocks=2, \n",
    "                 attention_resolutions=(8, 16), dropout=0.1, channel_mult=(1, 2, 4, 8), \n",
    "                 conv_resample=True, num_heads=8, context_dim=512):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.in_channels = in_channels\n",
    "        self.model_channels = model_channels\n",
    "        self.num_res_blocks = num_res_blocks\n",
    "        self.attention_resolutions = attention_resolutions\n",
    "        self.dropout = dropout\n",
    "        self.channel_mult = channel_mult\n",
    "        self.conv_resample = conv_resample\n",
    "        self.num_heads = num_heads\n",
    "        self.context_dim = context_dim\n",
    "        \n",
    "        # 时间嵌入 - 使用更深的网络\n",
    "        time_embed_dim = model_channels * 4\n",
    "        self.time_embedding = nn.Sequential(\n",
    "            nn.Linear(1, time_embed_dim),\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(time_embed_dim, time_embed_dim),\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(time_embed_dim, time_embed_dim)\n",
    "        )\n",
    "        \n",
    "        # 输入投影\n",
    "        self.input_blocks = nn.ModuleList([\n",
    "            nn.Conv2d(in_channels, model_channels, kernel_size=3, padding=1)\n",
    "        ])\n",
    "        \n",
    "        # 下采样块\n",
    "        input_block_chans = [model_channels]\n",
    "        ch = model_channels\n",
    "        \n",
    "        for level, mult in enumerate(channel_mult):\n",
    "            # 添加ResBlock\n",
    "            for _ in range(num_res_blocks):\n",
    "                self.input_blocks.append(\n",
    "                    nn.ModuleList([ImprovedResBlock(ch, time_embed_dim, dropout)])\n",
    "                )\n",
    "                input_block_chans.append(ch)\n",
    "            \n",
    "            # 添加CrossAttention\n",
    "            if level in attention_resolutions:\n",
    "                self.input_blocks.append(\n",
    "                    nn.ModuleList([ImprovedCrossAttention(ch, context_dim, num_heads, dropout=dropout)])\n",
    "                )\n",
    "                input_block_chans.append(ch)\n",
    "            \n",
    "            # 下采样\n",
    "            if level < len(channel_mult) - 1:\n",
    "                ch = mult * model_channels\n",
    "                self.input_blocks.append(\n",
    "                    nn.ModuleList([nn.Conv2d(input_block_chans[-1], ch, 3, stride=2, padding=1)])\n",
    "                )\n",
    "                input_block_chans.append(ch)\n",
    "        \n",
    "        # 中间块\n",
    "        self.middle_block = nn.ModuleList([\n",
    "            ImprovedResBlock(ch, time_embed_dim, dropout),\n",
    "            ImprovedCrossAttention(ch, context_dim, num_heads, dropout=dropout),\n",
    "            ImprovedResBlock(ch, time_embed_dim, dropout)\n",
    "        ])\n",
    "        \n",
    "        # 输出块\n",
    "        self.output_blocks = nn.ModuleList([])\n",
    "        for level, mult in list(enumerate(channel_mult))[::-1]:\n",
    "            # 上采样\n",
    "            if level < len(channel_mult) - 1:\n",
    "                self.output_blocks.append(\n",
    "                    nn.ModuleList([nn.ConvTranspose2d(ch, ch//2, 4, stride=2, padding=1)])\n",
    "                )\n",
    "                ch = ch // 2\n",
    "            \n",
    "            # 添加ResBlock\n",
    "            for _ in range(num_res_blocks + 1):\n",
    "                self.output_blocks.append(\n",
    "                    nn.ModuleList([ImprovedResBlock(ch, time_embed_dim, dropout)])\n",
    "                )\n",
    "            \n",
    "            # 添加CrossAttention\n",
    "            if level in attention_resolutions:\n",
    "                self.output_blocks.append(\n",
    "                    nn.ModuleList([ImprovedCrossAttention(ch, context_dim, num_heads, dropout=dropout)])\n",
    "                )\n",
    "        \n",
    "        # 输出投影\n",
    "        if ch >= 32:\n",
    "            num_groups = 32\n",
    "        elif ch >= 16:\n",
    "            num_groups = 16\n",
    "        elif ch >= 8:\n",
    "            num_groups = 8\n",
    "        elif ch >= 4:\n",
    "            num_groups = 4\n",
    "        else:\n",
    "            num_groups = 1\n",
    "        \n",
    "        self.out = nn.Sequential(\n",
    "            nn.GroupNorm(num_groups, ch),\n",
    "            nn.SiLU(),\n",
    "            nn.Conv2d(ch, out_channels, kernel_size=3, padding=1)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x, timesteps, context=None):\n",
    "        # 时间嵌入\n",
    "        t = self.time_embedding(timesteps.unsqueeze(-1).float())\n",
    "        if t.dim() == 1:\n",
    "            t = t.unsqueeze(0)\n",
    "        \n",
    "        # 输入块\n",
    "        h = x\n",
    "        hs = []\n",
    "        for module in self.input_blocks:\n",
    "            if isinstance(module, nn.ModuleList):\n",
    "                # 处理ModuleList中的模块\n",
    "                for submodule in module:\n",
    "                    if isinstance(submodule, ImprovedCrossAttention):\n",
    "                        h = submodule(h, context)\n",
    "                    elif isinstance(submodule, ImprovedResBlock):\n",
    "                        h = submodule(h, t)\n",
    "                    else:\n",
    "                        h = submodule(h)\n",
    "            else:\n",
    "                # 直接处理单个模块\n",
    "                if isinstance(module, ImprovedCrossAttention):\n",
    "                    h = module(h, context)\n",
    "                elif isinstance(module, ImprovedResBlock):\n",
    "                    h = module(h, t)\n",
    "                else:\n",
    "                    h = module(h)\n",
    "            hs.append(h)\n",
    "        \n",
    "        # 中间块\n",
    "        for module in self.middle_block:\n",
    "            if isinstance(module, ImprovedCrossAttention):\n",
    "                h = module(h, context)\n",
    "            else:\n",
    "                h = module(h, t)\n",
    "        \n",
    "        # 输出块\n",
    "        for module in self.output_blocks:\n",
    "            if isinstance(module, nn.ModuleList):\n",
    "                # 处理ModuleList中的模块\n",
    "                for submodule in module:\n",
    "                    if isinstance(submodule, ImprovedCrossAttention):\n",
    "                        h = submodule(h, context)\n",
    "                    elif isinstance(submodule, ImprovedResBlock):\n",
    "                        h = submodule(h, t)\n",
    "                    else:\n",
    "                        h = submodule(h)\n",
    "            else:\n",
    "                # 直接处理单个模块\n",
    "                if isinstance(module, ImprovedCrossAttention):\n",
    "                    h = module(h, context)\n",
    "                elif isinstance(module, ImprovedResBlock):\n",
    "                    h = module(h, t)\n",
    "                else:\n",
    "                    h = module(h)\n",
    "            \n",
    "            # 跳跃连接\n",
    "            if hs:\n",
    "                h = torch.cat([h, hs.pop()], dim=1)\n",
    "        \n",
    "        return self.out(h)\n",
    "\n",
    "class ImprovedDDPMScheduler:\n",
    "    \"\"\"\n",
    "    改进的DDPM调度器，借鉴官方实现\n",
    "    \"\"\"\n",
    "    def __init__(self, num_train_timesteps=1000, beta_start=0.0001, beta_end=0.02):\n",
    "        self.num_train_timesteps = num_train_timesteps\n",
    "        \n",
    "        # 线性噪声调度\n",
    "        self.betas = torch.linspace(beta_start, beta_end, num_train_timesteps)\n",
    "        self.alphas = 1.0 - self.betas\n",
    "        self.alphas_cumprod = torch.cumprod(self.alphas, dim=0)\n",
    "        self.alphas_cumprod_prev = torch.cat([torch.tensor([1.0]), self.alphas_cumprod[:-1]])\n",
    "        \n",
    "        # 计算噪声预测的系数\n",
    "        self.sqrt_alphas_cumprod = torch.sqrt(self.alphas_cumprod)\n",
    "        self.sqrt_one_minus_alphas_cumprod = torch.sqrt(1.0 - self.alphas_cumprod)\n",
    "        \n",
    "    def add_noise(self, original_samples, noise, timesteps):\n",
    "        \"\"\"添加噪声到原始样本\"\"\"\n",
    "        sqrt_alpha = self.sqrt_alphas_cumprod[timesteps].view(-1, 1, 1, 1)\n",
    "        sqrt_one_minus_alpha = self.sqrt_one_minus_alphas_cumprod[timesteps].view(-1, 1, 1, 1)\n",
    "        \n",
    "        return sqrt_alpha * original_samples + sqrt_one_minus_alpha * noise\n",
    "    \n",
    "    def step(self, model_output, timestep, sample):\n",
    "        \"\"\"去噪步骤\"\"\"\n",
    "        alpha = self.alphas_cumprod[timestep].view(-1, 1, 1, 1)\n",
    "        alpha_prev = self.alphas_cumprod_prev[timestep].view(-1, 1, 1, 1)\n",
    "        \n",
    "        # 预测x0\n",
    "        pred_original_sample = (sample - torch.sqrt(1 - alpha) * model_output) / torch.sqrt(alpha)\n",
    "        \n",
    "        # 预测前一个样本\n",
    "        pred_sample_direction = torch.sqrt(1 - alpha_prev) * model_output\n",
    "        pred_prev_sample = torch.sqrt(alpha_prev) * pred_original_sample + pred_sample_direction\n",
    "        \n",
    "        return pred_prev_sample\n",
    "    \n",
    "    def set_timesteps(self, num_inference_steps):\n",
    "        \"\"\"设置推理时间步\"\"\"\n",
    "        self.num_inference_steps = num_inference_steps\n",
    "        step_ratio = self.num_train_timesteps // num_inference_steps\n",
    "        timesteps = (torch.arange(0, num_inference_steps) * step_ratio).flip(0)\n",
    "        return timesteps\n",
    "\n",
    "class ImprovedStableDiffusionPipeline:\n",
    "    \"\"\"\n",
    "    改进的Stable Diffusion Pipeline，借鉴官方最佳实践\n",
    "    \"\"\"\n",
    "    def __init__(self, device='cuda' if torch.cuda.is_available() else 'cpu'):\n",
    "        self.device = device\n",
    "        \n",
    "        # 初始化组件\n",
    "        self.vae = ImprovedVAE().to(device)\n",
    "        self.unet = ImprovedUNet2DConditionModel(\n",
    "            in_channels=4,\n",
    "            out_channels=4,\n",
    "            model_channels=128,\n",
    "            channel_mult=(1, 2, 4, 8),\n",
    "            attention_resolutions=(8, 16),\n",
    "            context_dim=512\n",
    "        ).to(device)\n",
    "        self.scheduler = ImprovedDDPMScheduler()\n",
    "        \n",
    "        # CLIP文本编码器\n",
    "        self.tokenizer = CLIPTokenizer.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "        self.text_encoder = CLIPTextModel.from_pretrained(\"openai/clip-vit-base-patch32\").to(device)\n",
    "        \n",
    "        # 设置为评估模式\n",
    "        self.text_encoder.eval()\n",
    "        self.vae.eval()\n",
    "        \n",
    "    def _encode_prompt(self, prompt):\n",
    "        \"\"\"编码文本提示\"\"\"\n",
    "        tokens = self.tokenizer(prompt, padding=True, return_tensors=\"pt\").to(self.device)\n",
    "        with torch.no_grad():\n",
    "            text_embeddings = self.text_encoder(**tokens).last_hidden_state\n",
    "        return text_embeddings\n",
    "    \n",
    "    def _parse_kanji_prompt(self, prompt):\n",
    "        \"\"\"解析汉字提示，使用更详细的描述\"\"\"\n",
    "        base_prompt = f\"kanji character representing {prompt}, traditional calligraphy style, black ink on white paper, high contrast, detailed strokes, clear lines, professional quality, artistic interpretation\"\n",
    "        return base_prompt\n",
    "    \n",
    "    def generate(self, prompt, height=128, width=128, num_inference_steps=50, \n",
    "                guidance_scale=7.5, seed=None):\n",
    "        \"\"\"生成图像，使用官方推荐的参数\"\"\"\n",
    "        \n",
    "        # 设置随机种子\n",
    "        if seed is not None:\n",
    "            torch.manual_seed(seed)\n",
    "            torch.cuda.manual_seed(seed)\n",
    "        \n",
    "        # 编码提示\n",
    "        text_embeddings = self._encode_prompt(self._parse_kanji_prompt(prompt))\n",
    "        \n",
    "        # 初始化潜在变量\n",
    "        latent_height = height // 8\n",
    "        latent_width = width // 8\n",
    "        latents = torch.randn(1, 4, latent_height, latent_width, device=self.device)\n",
    "        \n",
    "        # 设置时间步\n",
    "        timesteps = self.scheduler.set_timesteps(num_inference_steps)\n",
    "        timesteps = timesteps.to(self.device)\n",
    "        \n",
    "        # 改进的去噪循环\n",
    "        for i, t in enumerate(timesteps):\n",
    "            # 扩展潜在变量用于批处理\n",
    "            latent_model_input = torch.cat([latents] * 2)\n",
    "            t_expanded = t.expand(2)\n",
    "            \n",
    "            # 预测噪声\n",
    "            with torch.no_grad():\n",
    "                noise_pred = self.unet(latent_model_input, t_expanded, text_embeddings)\n",
    "            \n",
    "            # 执行classifier-free guidance\n",
    "            noise_pred_uncond, noise_pred_text = noise_pred.chunk(2)\n",
    "            \n",
    "            # 使用官方推荐的guidance scale\n",
    "            guidance_scale = torch.clamp(torch.tensor(guidance_scale), min=1.0, max=20.0)\n",
    "            noise_pred = noise_pred_uncond + guidance_scale * (noise_pred_text - noise_pred_uncond)\n",
    "            \n",
    "            # 计算前一个样本\n",
    "            latents = self.scheduler.step(noise_pred, t, latents)\n",
    "        \n",
    "        # 解码潜在变量\n",
    "        with torch.no_grad():\n",
    "            image = self.vae.decode(latents)\n",
    "        \n",
    "        return image\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🏗️ colab_training.py Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Google Colab优化的Stable Diffusion训练脚本\n",
    "专门为Colab GPU环境优化，包含自动检测和性能优化\n",
    "\"\"\"\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import gc\n",
    "\n",
    "# 添加当前目录到路径\n",
    "sys.path.append(os.path.dirname(os.path.abspath(__file__)))\n",
    "\n",
    "from improved_stable_diffusion import (\n",
    "    ImprovedStableDiffusionPipeline,\n",
    "    ImprovedVAE,\n",
    "    ImprovedUNet2DConditionModel,\n",
    "    ImprovedDDPMScheduler\n",
    ")\n",
    "\n",
    "class ColabOptimizedTrainer:\n",
    "    \"\"\"\n",
    "    Colab优化的训练器\n",
    "    \"\"\"\n",
    "    def __init__(self, device='auto'):\n",
    "        # 自动检测设备\n",
    "        if device == 'auto':\n",
    "            if torch.cuda.is_available():\n",
    "                self.device = 'cuda'\n",
    "                print(f\"🚀 检测到CUDA设备: {torch.cuda.get_device_name()}\")\n",
    "                print(f\"   • GPU内存: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "                print(f\"   • CUDA版本: {torch.version.cuda}\")\n",
    "            elif hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():\n",
    "                self.device = 'mps'\n",
    "                print(\"🍎 检测到Apple Silicon (MPS)\")\n",
    "            else:\n",
    "                self.device = 'cpu'\n",
    "                print(\"💻 使用CPU训练\")\n",
    "        else:\n",
    "            self.device = device\n",
    "        \n",
    "        # 初始化模型\n",
    "        self.vae = ImprovedVAE().to(self.device)\n",
    "        self.unet = ImprovedUNet2DConditionModel(\n",
    "            in_channels=4,\n",
    "            out_channels=4,\n",
    "            model_channels=128,\n",
    "            channel_mult=(1, 2, 4, 8),\n",
    "            attention_resolutions=(8, 16),\n",
    "            context_dim=512\n",
    "        ).to(self.device)\n",
    "        self.scheduler = ImprovedDDPMScheduler()\n",
    "        \n",
    "        # 优化器设置\n",
    "        self.optimizer = optim.AdamW([\n",
    "            {'params': self.vae.parameters(), 'lr': 1e-4},\n",
    "            {'params': self.unet.parameters(), 'lr': 1e-4}\n",
    "        ], weight_decay=0.01)\n",
    "        \n",
    "        # 学习率调度器\n",
    "        self.scheduler_lr = optim.lr_scheduler.CosineAnnealingLR(\n",
    "            self.optimizer, T_max=100, eta_min=1e-6\n",
    "        )\n",
    "        \n",
    "        # 混合精度训练\n",
    "        self.scaler = GradScaler()\n",
    "        \n",
    "        # 训练参数\n",
    "        self.num_epochs = 50\n",
    "        self.batch_size = 8  # Colab GPU内存优化\n",
    "        self.gradient_accumulation_steps = 4\n",
    "        self.save_every = 5\n",
    "        \n",
    "        # 损失函数\n",
    "        self.mse_loss = nn.MSELoss()\n",
    "        \n",
    "        print(f\"✅ 模型初始化完成，使用设备: {self.device}\")\n",
    "    \n",
    "    def create_synthetic_dataset(self, num_samples=1000):\n",
    "        \"\"\"\n",
    "        创建合成数据集用于演示\n",
    "        在实际使用中，这里应该加载真实的汉字数据\n",
    "        \"\"\"\n",
    "        print(f\"📊 创建合成数据集 ({num_samples} 样本)...\")\n",
    "        \n",
    "        # 创建128x128的合成图像\n",
    "        images = []\n",
    "        for i in range(num_samples):\n",
    "            # 创建简单的几何图案作为训练数据\n",
    "            img = np.zeros((128, 128, 3), dtype=np.float32)\n",
    "            \n",
    "            # 添加一些随机几何形状\n",
    "            if i % 4 == 0:\n",
    "                # 圆形\n",
    "                y, x = np.ogrid[:128, :128]\n",
    "                mask = (x - 64)**2 + (y - 64)**2 <= 30**2\n",
    "                img[mask] = [0.8, 0.8, 0.8]\n",
    "            elif i % 4 == 1:\n",
    "                # 矩形\n",
    "                img[40:88, 40:88] = [0.7, 0.7, 0.7]\n",
    "            elif i % 4 == 2:\n",
    "                # 三角形\n",
    "                for y in range(128):\n",
    "                    for x in range(128):\n",
    "                        if y >= 64 and abs(x - 64) <= (y - 64):\n",
    "                            img[y, x] = [0.6, 0.6, 0.6]\n",
    "            else:\n",
    "                # 随机噪声\n",
    "                img = np.random.rand(128, 128, 3).astype(np.float32) * 0.5\n",
    "            \n",
    "            # 归一化到[-1, 1]\n",
    "            img = (img - 0.5) * 2\n",
    "            images.append(img)\n",
    "        \n",
    "        # 转换为tensor\n",
    "        images = torch.tensor(images, dtype=torch.float32).permute(0, 3, 1, 2)\n",
    "        print(f\"✅ 数据集创建完成: {images.shape}\")\n",
    "        \n",
    "        return images\n",
    "    \n",
    "    def train_epoch(self, dataloader, epoch):\n",
    "        \"\"\"\n",
    "        训练一个epoch\n",
    "        \"\"\"\n",
    "        self.vae.train()\n",
    "        self.unet.train()\n",
    "        \n",
    "        total_loss = 0\n",
    "        num_batches = len(dataloader)\n",
    "        \n",
    "        for batch_idx, images in enumerate(dataloader):\n",
    "            images = images.to(self.device)\n",
    "            \n",
    "            # 梯度累积\n",
    "            with autocast():\n",
    "                # VAE编码\n",
    "                latents, mu, logvar, kl_loss = self.vae.encode(images)\n",
    "                \n",
    "                # 添加噪声\n",
    "                noise = torch.randn_like(latents)\n",
    "                timesteps = torch.randint(0, self.scheduler.num_train_timesteps, \n",
    "                                       (latents.shape[0],), device=self.device)\n",
    "                noisy_latents = self.scheduler.add_noise(latents, noise, timesteps)\n",
    "                \n",
    "                # UNet预测噪声\n",
    "                noise_pred = self.unet(noisy_latents, timesteps)\n",
    "                \n",
    "                # 计算损失\n",
    "                noise_loss = self.mse_loss(noise_pred, noise)\n",
    "                reconstruction_loss = self.mse_loss(self.vae.decode(latents), images)\n",
    "                \n",
    "                # 总损失\n",
    "                loss = noise_loss + 0.1 * kl_loss + 0.1 * reconstruction_loss\n",
    "                loss = loss / self.gradient_accumulation_steps\n",
    "            \n",
    "            # 反向传播\n",
    "            self.scaler.scale(loss).backward()\n",
    "            \n",
    "            if (batch_idx + 1) % self.gradient_accumulation_steps == 0:\n",
    "                # 梯度裁剪\n",
    "                self.scaler.unscale_(self.optimizer)\n",
    "                torch.nn.utils.clip_grad_norm_(\n",
    "                    list(self.vae.parameters()) + list(self.unet.parameters()), \n",
    "                    max_norm=1.0\n",
    "                )\n",
    "                \n",
    "                # 优化器步进\n",
    "                self.scaler.step(self.optimizer)\n",
    "                self.scaler.update()\n",
    "                self.optimizer.zero_grad()\n",
    "            \n",
    "            total_loss += loss.item() * self.gradient_accumulation_steps\n",
    "            \n",
    "            # 进度显示\n",
    "            if (batch_idx + 1) % 10 == 0:\n",
    "                print(f\"   Epoch {epoch+1}/{self.num_epochs}, \"\n",
    "                      f\"Batch {batch_idx+1}/{num_batches}, \"\n",
    "                      f\"Loss: {loss.item():.6f}\")\n",
    "        \n",
    "        # 学习率调度\n",
    "        self.scheduler_lr.step()\n",
    "        \n",
    "        return total_loss / num_batches\n",
    "    \n",
    "    def save_checkpoint(self, epoch, loss, save_dir=\"colab_checkpoints\"):\n",
    "        \"\"\"\n",
    "        保存检查点\n",
    "        \"\"\"\n",
    "        os.makedirs(save_dir, exist_ok=True)\n",
    "        \n",
    "        checkpoint = {\n",
    "            'epoch': epoch,\n",
    "            'vae_state_dict': self.vae.state_dict(),\n",
    "            'unet_state_dict': self.unet.state_dict(),\n",
    "            'optimizer_state_dict': self.optimizer.state_dict(),\n",
    "            'scheduler_state_dict': self.scheduler_lr.state_dict(),\n",
    "            'loss': loss,\n",
    "            'device': self.device\n",
    "        }\n",
    "        \n",
    "        checkpoint_path = os.path.join(save_dir, f'checkpoint_epoch_{epoch+1}.pth')\n",
    "        torch.save(checkpoint, checkpoint_path)\n",
    "        print(f\"💾 检查点已保存: {checkpoint_path}\")\n",
    "        \n",
    "        # 保存最佳模型\n",
    "        if epoch == 0 or loss < getattr(self, 'best_loss', float('inf')):\n",
    "            self.best_loss = loss\n",
    "            best_model_path = os.path.join(save_dir, 'best_model.pth')\n",
    "            torch.save(checkpoint, best_model_path)\n",
    "            print(f\"🏆 最佳模型已保存: {best_model_path}\")\n",
    "    \n",
    "    def train(self):\n",
    "        \"\"\"\n",
    "        主训练循环\n",
    "        \"\"\"\n",
    "        print(f\"\\n🎯 开始训练...\")\n",
    "        print(f\"   • 设备: {self.device}\")\n",
    "        print(f\"   • 批次大小: {self.batch_size}\")\n",
    "        print(f\"   • 梯度累积步数: {self.gradient_accumulation_steps}\")\n",
    "        print(f\"   • 总epochs: {self.num_epochs}\")\n",
    "        print(f\"   • 混合精度: {'启用' if self.device == 'cuda' else '禁用'}\")\n",
    "        \n",
    "        # 创建数据集\n",
    "        images = self.create_synthetic_dataset()\n",
    "        dataloader = DataLoader(images, batch_size=self.batch_size, shuffle=True)\n",
    "        \n",
    "        # 训练历史\n",
    "        train_losses = []\n",
    "        start_time = time.time()\n",
    "        \n",
    "        try:\n",
    "            for epoch in range(self.num_epochs):\n",
    "                epoch_start = time.time()\n",
    "                \n",
    "                print(f\"\\n🔄 Epoch {epoch+1}/{self.num_epochs}\")\n",
    "                print(\"-\" * 50)\n",
    "                \n",
    "                # 训练\n",
    "                loss = self.train_epoch(dataloader, epoch)\n",
    "                train_losses.append(loss)\n",
    "                \n",
    "                epoch_time = time.time() - epoch_start\n",
    "                print(f\"   ⏱️  Epoch耗时: {epoch_time:.2f}秒\")\n",
    "                print(f\"   📊 平均损失: {loss:.6f}\")\n",
    "                print(f\"   📈 学习率: {self.optimizer.param_groups[0]['lr']:.2e}\")\n",
    "                \n",
    "                # 保存检查点\n",
    "                if (epoch + 1) % self.save_every == 0:\n",
    "                    self.save_checkpoint(epoch, loss)\n",
    "                \n",
    "                # 内存清理 (Colab优化)\n",
    "                if self.device == 'cuda':\n",
    "                    torch.cuda.empty_cache()\n",
    "                    gc.collect()\n",
    "                \n",
    "                # 显示GPU内存使用情况\n",
    "                if self.device == 'cuda':\n",
    "                    memory_allocated = torch.cuda.memory_allocated() / 1e9\n",
    "                    memory_reserved = torch.cuda.memory_reserved() / 1e9\n",
    "                    print(f\"   🧠 GPU内存: {memory_allocated:.2f}GB / {memory_reserved:.2f}GB\")\n",
    "        \n",
    "        except KeyboardInterrupt:\n",
    "            print(f\"\\n⚠️  训练被用户中断\")\n",
    "        except Exception as e:\n",
    "            print(f\"\\n❌ 训练出错: {e}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "        \n",
    "        finally:\n",
    "            # 保存最终模型\n",
    "            final_loss = train_losses[-1] if train_losses else float('inf')\n",
    "            self.save_checkpoint(len(train_losses) - 1, final_loss)\n",
    "            \n",
    "            # 训练总结\n",
    "            total_time = time.time() - start_time\n",
    "            print(f\"\\n🎉 训练完成!\")\n",
    "            print(f\"   ⏱️  总耗时: {total_time:.2f}秒\")\n",
    "            print(f\"   📊 最终损失: {final_loss:.6f}\")\n",
    "            print(f\"   📈 损失变化: {train_losses[0]:.6f} → {final_loss:.6f}\")\n",
    "            \n",
    "            # 绘制损失曲线\n",
    "            self.plot_training_curve(train_losses)\n",
    "    \n",
    "    def plot_training_curve(self, losses):\n",
    "        \"\"\"\n",
    "        绘制训练损失曲线\n",
    "        \"\"\"\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.plot(losses, 'b-', linewidth=2, label='训练损失')\n",
    "        plt.title('Colab训练损失曲线', fontsize=16)\n",
    "        plt.xlabel('Epoch', fontsize=14)\n",
    "        plt.ylabel('损失', fontsize=14)\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        plt.legend(fontsize=12)\n",
    "        plt.tight_layout()\n",
    "        \n",
    "        # 保存图片\n",
    "        plot_path = 'colab_training_curve.png'\n",
    "        plt.savefig(plot_path, dpi=150, bbox_inches='tight')\n",
    "        print(f\"📊 训练曲线已保存: {plot_path}\")\n",
    "        plt.show()\n",
    "    \n",
    "    def test_generation(self, prompt=\"water\"):\n",
    "        \"\"\"\n",
    "        测试生成功能\n",
    "        \"\"\"\n",
    "        print(f\"\\n🧪 测试生成: {prompt}\")\n",
    "        \n",
    "        try:\n",
    "            # 创建pipeline\n",
    "            pipeline = ImprovedStableDiffusionPipeline(device=self.device)\n",
    "            \n",
    "            # 加载训练好的权重\n",
    "            if hasattr(self, 'best_loss'):\n",
    "                checkpoint_path = 'colab_checkpoints/best_model.pth'\n",
    "                if os.path.exists(checkpoint_path):\n",
    "                    checkpoint = torch.load(checkpoint_path, map_location=self.device)\n",
    "                    pipeline.vae.load_state_dict(checkpoint['vae_state_dict'])\n",
    "                    pipeline.unet.load_state_dict(checkpoint['unet_state_dict'])\n",
    "                    print(f\"✅ 已加载最佳模型权重\")\n",
    "            \n",
    "            # 生成图像\n",
    "            print(f\"🌊 生成中...\")\n",
    "            result = pipeline.generate(\n",
    "                prompt,\n",
    "                height=128,\n",
    "                width=128,\n",
    "                num_inference_steps=50,\n",
    "                guidance_scale=7.5,\n",
    "                seed=42\n",
    "            )\n",
    "            \n",
    "            # 保存结果\n",
    "            if isinstance(result, torch.Tensor):\n",
    "                result = (result + 1) / 2\n",
    "                result = torch.clamp(result, 0, 1)\n",
    "                img_array = result.squeeze(0).permute(1, 2, 0).cpu().numpy()\n",
    "                pil_image = Image.fromarray((img_array * 255).astype(np.uint8))\n",
    "            else:\n",
    "                pil_image = result\n",
    "            \n",
    "            output_path = f'colab_generated_{prompt}.png'\n",
    "            pil_image.save(output_path)\n",
    "            print(f\"✅ 生成完成，已保存: {output_path}\")\n",
    "            \n",
    "            # 显示图像\n",
    "            plt.figure(figsize=(6, 6))\n",
    "            plt.imshow(pil_image, cmap='gray')\n",
    "            plt.title(f'Colab生成: {prompt}', fontsize=14)\n",
    "            plt.axis('off')\n",
    "            plt.show()\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"❌ 生成测试失败: {e}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    主函数\n",
    "    \"\"\"\n",
    "    print(\"🚀 Google Colab优化的Stable Diffusion训练器\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # 检查Colab环境\n",
    "    is_colab = 'COLAB_GPU' in os.environ\n",
    "    if is_colab:\n",
    "        print(\"✅ 检测到Google Colab环境\")\n",
    "        print(f\"   • GPU类型: {os.environ.get('COLAB_GPU', 'Unknown')}\")\n",
    "        print(f\"   • 运行时类型: {os.environ.get('COLAB_RUNTIME_TYPE', 'Unknown')}\")\n",
    "    else:\n",
    "        print(\"💻 本地环境运行\")\n",
    "    \n",
    "    # 创建训练器\n",
    "    trainer = ColabOptimizedTrainer(device='auto')\n",
    "    \n",
    "    # 开始训练\n",
    "    trainer.train()\n",
    "    \n",
    "    # 测试生成\n",
    "    trainer.test_generation(\"water\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🚀 Start Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create trainer and start training\n",
    "if 'ColabOptimizedTrainer' in globals():\n",
    "    trainer = ColabOptimizedTrainer()\n",
    "    trainer.train()\n",
    "else:\n",
    "    print(\"⚠️ Trainer class not found. Please run the model implementation cells first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 📥 Download Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download training results\n",
    "from google.colab import files\n",
    "import zipfile\n",
    "\n",
    "def download_results():\n",
    "    print(\"📥 Preparing results for download...\")\n",
    "    \n",
    "    # Create results zip\n",
    "    with zipfile.ZipFile('training_results.zip', 'w') as zipf:\n",
    "        # Add checkpoints\n",
    "        if os.path.exists('checkpoints'):\n",
    "            for root, dirs, files in os.walk('checkpoints'):\n",
    "                for file in files:\n",
    "                    file_path = os.path.join(root, file)\n",
    "                    zipf.write(file_path, os.path.relpath(file_path, '.'))\n",
    "        \n",
    "        # Add training curves\n",
    "        for img_file in ['training_curve.png', 'loss_curve.png']:\n",
    "            if os.path.exists(img_file):\n",
    "                zipf.write(img_file)\n",
    "        \n",
    "        # Add generated images\n",
    "        for i in range(10):\n",
    "            img_file = f'generated_{i}.png'\n",
    "            if os.path.exists(img_file):\n",
    "                zipf.write(img_file)\n",
    "    \n",
    "    print(\"✅ Results packaged: training_results.zip\")\n",
    "    \n",
    "    # Download\n",
    "    try:\n",
    "        files.download('training_results.zip')\n",
    "        print(\"📥 Results downloaded successfully!\")\n",
    "    except:\n",
    "        print(\"⚠️ Download failed (not in Colab)\")\n",
    "        print(\"📁 Files are saved in the current directory\")\n",
    "\n",
    "# Download results\n",
    "download_results()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
