{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# ğŸš€ Complete Stable Diffusion Kanji Generation - Colab/Kaggle\n\n**Single file training notebook** - Upload to Colab/Kaggle and start training immediately!\n\n## ğŸ¯ Features\n- âœ… **Complete Training Pipeline**: VAE + UNet + DDPM\n- ğŸš€ **GPU Optimized**: Auto CUDA/MPS detection\n- ğŸ’¾ **Auto-save**: Checkpoints every 5 epochs\n- ğŸ“Š **Real-time Monitoring**: Loss curves and GPU stats\n- ğŸ”„ **Resume Training**: Continue from any checkpoint\n- ğŸŒ **Kanji Generation**: Text-to-Kanji capabilities\n\n## ğŸš€ Quick Start\n1. Upload this notebook to Colab/Kaggle\n2. Select GPU runtime\n3. Run all cells\n4. Start training!\n\n**Expected Training Time**:\n- Colab Free (T4): 50 epochs in 2-3 hours\n- Colab Pro (V100/P100): 50 epochs in 1-1.5 hours\n- Kaggle (P100): 50 epochs in 1-2 hours","metadata":{}},{"cell_type":"markdown","source":"## ğŸ“¦ Install Dependencies","metadata":{}},{"cell_type":"code","source":"# Install required packages\n!pip install transformers pillow matplotlib scikit-image opencv-python tqdm\n!pip install --upgrade torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n\nprint(\"âœ… Dependencies installed successfully!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-25T03:56:59.783080Z","iopub.execute_input":"2025-08-25T03:56:59.783319Z","iopub.status.idle":"2025-08-25T04:00:14.325117Z","shell.execute_reply.started":"2025-08-25T03:56:59.783293Z","shell.execute_reply":"2025-08-25T04:00:14.323897Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.52.4)\nRequirement already satisfied: pillow in /usr/local/lib/python3.11/dist-packages (11.2.1)\nRequirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (3.7.2)\nRequirement already satisfied: scikit-image in /usr/local/lib/python3.11/dist-packages (0.25.2)\nRequirement already satisfied: opencv-python in /usr/local/lib/python3.11/dist-packages (4.11.0.86)\nRequirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (4.67.1)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\nRequirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.33.1)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (25.0)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\nRequirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.4)\nRequirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.2)\nRequirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\nRequirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.3.2)\nRequirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (0.12.1)\nRequirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (4.58.4)\nRequirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.4.8)\nRequirement already satisfied: pyparsing<3.1,>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (3.0.9)\nRequirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (2.9.0.post0)\nRequirement already satisfied: scipy>=1.11.4 in /usr/local/lib/python3.11/dist-packages (from scikit-image) (1.15.3)\nRequirement already satisfied: networkx>=3.0 in /usr/local/lib/python3.11/dist-packages (from scikit-image) (3.5)\nRequirement already satisfied: imageio!=2.35.0,>=2.33 in /usr/local/lib/python3.11/dist-packages (from scikit-image) (2.37.0)\nRequirement already satisfied: tifffile>=2022.8.12 in /usr/local/lib/python3.11/dist-packages (from scikit-image) (2025.6.11)\nRequirement already satisfied: lazy-loader>=0.4 in /usr/local/lib/python3.11/dist-packages (from scikit-image) (0.4)\nRequirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (2025.5.1)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.14.0)\nRequirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (1.1.5)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (2025.2.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (2022.2.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (2.4.1)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\nRequirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.2)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.5.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.6.15)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->transformers) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->transformers) (2022.2.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->transformers) (1.4.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.17->transformers) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.17->transformers) (2024.2.0)\nLooking in indexes: https://download.pytorch.org/whl/cu118\nRequirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\nCollecting torch\n  Downloading https://download.pytorch.org/whl/cu118/torch-2.7.1%2Bcu118-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (28 kB)\nRequirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (0.21.0+cu124)\nCollecting torchvision\n  Downloading https://download.pytorch.org/whl/cu118/torchvision-0.22.1%2Bcu118-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (6.1 kB)\nRequirement already satisfied: torchaudio in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\nCollecting torchaudio\n  Downloading https://download.pytorch.org/whl/cu118/torchaudio-2.7.1%2Bcu118-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (6.6 kB)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.18.0)\nRequirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.14.0)\nCollecting sympy>=1.13.3 (from torch)\n  Downloading https://download.pytorch.org/whl/sympy-1.13.3-py3-none-any.whl.metadata (12 kB)\nRequirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.5)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.5.1)\nCollecting nvidia-cuda-nvrtc-cu11==11.8.89 (from torch)\n  Downloading https://download.pytorch.org/whl/cu118/nvidia_cuda_nvrtc_cu11-11.8.89-py3-none-manylinux1_x86_64.whl (23.2 MB)\n\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m23.2/23.2 MB\u001b[0m \u001b[31m77.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hCollecting nvidia-cuda-runtime-cu11==11.8.89 (from torch)\n  Downloading https://download.pytorch.org/whl/cu118/nvidia_cuda_runtime_cu11-11.8.89-py3-none-manylinux1_x86_64.whl (875 kB)\n\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m875.6/875.6 kB\u001b[0m \u001b[31m39.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting nvidia-cuda-cupti-cu11==11.8.87 (from torch)\n  Downloading https://download.pytorch.org/whl/cu118/nvidia_cuda_cupti_cu11-11.8.87-py3-none-manylinux1_x86_64.whl (13.1 MB)\n\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m13.1/13.1 MB\u001b[0m \u001b[31m100.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hCollecting nvidia-cudnn-cu11==9.1.0.70 (from torch)\n  Downloading https://download.pytorch.org/whl/cu118/nvidia_cudnn_cu11-9.1.0.70-py3-none-manylinux2014_x86_64.whl (663.9 MB)\n\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m663.9/663.9 MB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hCollecting nvidia-cublas-cu11==11.11.3.6 (from torch)\n  Downloading https://download.pytorch.org/whl/cu118/nvidia_cublas_cu11-11.11.3.6-py3-none-manylinux1_x86_64.whl (417.9 MB)\n\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m417.9/417.9 MB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hCollecting nvidia-cufft-cu11==10.9.0.58 (from torch)\n  Downloading https://download.pytorch.org/whl/cu118/nvidia_cufft_cu11-10.9.0.58-py3-none-manylinux1_x86_64.whl (168.4 MB)\n\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m168.4/168.4 MB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hCollecting nvidia-curand-cu11==10.3.0.86 (from torch)\n  Downloading https://download.pytorch.org/whl/cu118/nvidia_curand_cu11-10.3.0.86-py3-none-manylinux1_x86_64.whl (58.1 MB)\n\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m58.1/58.1 MB\u001b[0m \u001b[31m30.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hCollecting nvidia-cusolver-cu11==11.4.1.48 (from torch)\n  Downloading https://download.pytorch.org/whl/cu118/nvidia_cusolver_cu11-11.4.1.48-py3-none-manylinux1_x86_64.whl (128.2 MB)\n\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m128.2/128.2 MB\u001b[0m \u001b[31m13.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hCollecting nvidia-cusparse-cu11==11.7.5.86 (from torch)\n  Downloading https://download.pytorch.org/whl/cu118/nvidia_cusparse_cu11-11.7.5.86-py3-none-manylinux1_x86_64.whl (204.1 MB)\n\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m204.1/204.1 MB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hCollecting nvidia-nccl-cu11==2.21.5 (from torch)\n  Downloading https://download.pytorch.org/whl/cu118/nvidia_nccl_cu11-2.21.5-py3-none-manylinux2014_x86_64.whl (147.8 MB)\n\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m147.8/147.8 MB\u001b[0m \u001b[31m11.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hCollecting nvidia-nvtx-cu11==11.8.86 (from torch)\n  Downloading https://download.pytorch.org/whl/cu118/nvidia_nvtx_cu11-11.8.86-py3-none-manylinux1_x86_64.whl (99 kB)\n\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting triton==3.3.1 (from torch)\n  Downloading https://download.pytorch.org/whl/triton-3.3.1-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (1.5 kB)\nRequirement already satisfied: setuptools>=40.8.0 in /usr/local/lib/python3.11/dist-packages (from triton==3.3.1->torch) (75.2.0)\nRequirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchvision) (1.26.4)\nRequirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision) (11.2.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision) (2025.2.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision) (2022.2.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision) (2.4.1)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->torchvision) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->torchvision) (2022.2.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy->torchvision) (1.4.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy->torchvision) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy->torchvision) (2024.2.0)\nDownloading https://download.pytorch.org/whl/cu118/torch-2.7.1%2Bcu118-cp311-cp311-manylinux_2_28_x86_64.whl (905.3 MB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m905.3/905.3 MB\u001b[0m \u001b[31m911.9 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading https://download.pytorch.org/whl/triton-3.3.1-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (155.7 MB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m155.7/155.7 MB\u001b[0m \u001b[31m11.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading https://download.pytorch.org/whl/cu118/torchvision-0.22.1%2Bcu118-cp311-cp311-manylinux_2_28_x86_64.whl (6.7 MB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m6.7/6.7 MB\u001b[0m \u001b[31m93.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading https://download.pytorch.org/whl/cu118/torchaudio-2.7.1%2Bcu118-cp311-cp311-manylinux_2_28_x86_64.whl (3.3 MB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m80.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25hDownloading https://download.pytorch.org/whl/sympy-1.13.3-py3-none-any.whl (6.2 MB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m6.2/6.2 MB\u001b[0m \u001b[31m77.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: triton, sympy, nvidia-nvtx-cu11, nvidia-nccl-cu11, nvidia-cusparse-cu11, nvidia-curand-cu11, nvidia-cufft-cu11, nvidia-cuda-runtime-cu11, nvidia-cuda-nvrtc-cu11, nvidia-cuda-cupti-cu11, nvidia-cublas-cu11, nvidia-cusolver-cu11, nvidia-cudnn-cu11, torch, torchaudio, torchvision\n  Attempting uninstall: triton\n    Found existing installation: triton 3.2.0\n    Uninstalling triton-3.2.0:\n      Successfully uninstalled triton-3.2.0\n  Attempting uninstall: sympy\n    Found existing installation: sympy 1.13.1\n    Uninstalling sympy-1.13.1:\n      Successfully uninstalled sympy-1.13.1\n  Attempting uninstall: torch\n    Found existing installation: torch 2.6.0+cu124\n    Uninstalling torch-2.6.0+cu124:\n      Successfully uninstalled torch-2.6.0+cu124\n  Attempting uninstall: torchaudio\n    Found existing installation: torchaudio 2.6.0+cu124\n    Uninstalling torchaudio-2.6.0+cu124:\n      Successfully uninstalled torchaudio-2.6.0+cu124\n  Attempting uninstall: torchvision\n    Found existing installation: torchvision 0.21.0+cu124\n    Uninstalling torchvision-0.21.0+cu124:\n      Successfully uninstalled torchvision-0.21.0+cu124\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nfastai 2.7.19 requires torch<2.7,>=1.10, but you have torch 2.7.1+cu118 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed nvidia-cublas-cu11-11.11.3.6 nvidia-cuda-cupti-cu11-11.8.87 nvidia-cuda-nvrtc-cu11-11.8.89 nvidia-cuda-runtime-cu11-11.8.89 nvidia-cudnn-cu11-9.1.0.70 nvidia-cufft-cu11-10.9.0.58 nvidia-curand-cu11-10.3.0.86 nvidia-cusolver-cu11-11.4.1.48 nvidia-cusparse-cu11-11.7.5.86 nvidia-nccl-cu11-2.21.5 nvidia-nvtx-cu11-11.8.86 sympy-1.13.3 torch-2.7.1+cu118 torchaudio-2.7.1+cu118 torchvision-0.22.1+cu118 triton-3.3.1\nâœ… Dependencies installed successfully!\n","output_type":"stream"}],"execution_count":1},{"cell_type":"markdown","source":"## ğŸ”§ Check GPU and Environment","metadata":{}},{"cell_type":"code","source":"import torch\nimport os\n\n# Check environment\nis_colab = 'COLAB_GPU' in os.environ\nis_kaggle = 'KAGGLE_KERNEL_RUN_TYPE' in os.environ\n\nprint(f\"ğŸŒ Environment: {'Colab' if is_colab else 'Kaggle' if is_kaggle else 'Local'}\")\nprint(f\"PyTorch: {torch.__version__}\")\nprint(f\"CUDA available: {torch.cuda.is_available()}\")\n\nif torch.cuda.is_available():\n    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\nelif hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():\n    print(\"ğŸ Apple Silicon (MPS) available\")\nelse:\n    print(\"âš ï¸ Using CPU (will be slow!)\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-25T04:00:48.231909Z","iopub.execute_input":"2025-08-25T04:00:48.232177Z","iopub.status.idle":"2025-08-25T04:00:48.237940Z","shell.execute_reply.started":"2025-08-25T04:00:48.232155Z","shell.execute_reply":"2025-08-25T04:00:48.237195Z"}},"outputs":[{"name":"stdout","text":"ğŸŒ Environment: Kaggle\nPyTorch: 2.7.1+cu118\nCUDA available: True\nGPU: Tesla T4\nGPU Memory: 15.8 GB\n","output_type":"stream"}],"execution_count":6},{"cell_type":"markdown","source":"## ğŸ—ï¸ improved_stable_diffusion.py Implementation","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom transformers import CLIPTokenizer, CLIPTextModel\nimport math\nfrom typing import Optional, Union, Tuple\nimport numpy as np\n\nclass ImprovedVAE(nn.Module):\n    \"\"\"\n    æ”¹è¿›çš„VAEå®ç°ï¼Œå€Ÿé‰´å®˜æ–¹æ¶æ„\n    \"\"\"\n    def __init__(self, in_channels=3, latent_channels=4, hidden_dims=[128, 256, 512, 1024]):\n        super().__init__()\n        self.latent_channels = latent_channels\n        \n        # Encoder - ä½¿ç”¨æ›´æ·±çš„ç½‘ç»œ\n        encoder_layers = []\n        in_ch = in_channels\n        for h_dim in hidden_dims:\n            # è®¡ç®—åˆé€‚çš„GroupNormç»„æ•°\n            num_groups = min(32, h_dim)\n            while h_dim % num_groups != 0 and num_groups > 1:\n                num_groups -= 1\n            \n            encoder_layers.extend([\n                nn.Conv2d(in_ch, h_dim, kernel_size=3, stride=2, padding=1),\n                nn.GroupNorm(num_groups, h_dim),  # ä½¿ç”¨GroupNormæ›¿ä»£BatchNorm\n                nn.SiLU()  # ä½¿ç”¨SiLUæ›¿ä»£LeakyReLU\n            ])\n            in_ch = h_dim\n        \n        # Final encoding layer\n        final_channels = latent_channels * 2\n        num_groups = min(32, final_channels)\n        while final_channels % num_groups != 0 and num_groups > 1:\n            num_groups -= 1\n        \n        encoder_layers.extend([\n            nn.Conv2d(hidden_dims[-1], final_channels, kernel_size=3, padding=1),\n            nn.GroupNorm(num_groups, final_channels)\n        ])\n        \n        self.encoder = nn.Sequential(*encoder_layers)\n        \n        # Decoder - ç¡®ä¿ç²¾ç¡®çš„128x128è¾“å‡º\n        decoder_layers = []\n        in_ch = latent_channels\n        \n        # ä½¿ç”¨hidden_dimsçš„ååºè¿›è¡Œä¸Šé‡‡æ ·\n        hidden_dims_rev = hidden_dims[::-1]\n        \n        for i, h_dim in enumerate(hidden_dims_rev):\n            # è®¡ç®—åˆé€‚çš„GroupNormç»„æ•°\n            num_groups = min(32, h_dim)\n            while h_dim % num_groups != 0 and num_groups > 1:\n                num_groups -= 1\n            \n            decoder_layers.extend([\n                nn.ConvTranspose2d(in_ch, h_dim, kernel_size=4, stride=2, padding=1),\n                nn.GroupNorm(num_groups, h_dim),\n                nn.SiLU()\n            ])\n            in_ch = h_dim\n        \n        # æœ€ç»ˆè¾“å‡ºå±‚\n        decoder_layers.extend([\n            nn.Conv2d(in_ch, in_channels, kernel_size=3, stride=1, padding=1),\n            nn.Tanh()\n        ])\n        \n        self.decoder = nn.Sequential(*decoder_layers)\n        \n    def encode(self, x):\n        # ç¡®ä¿è¾“å…¥æ˜¯128x128\n        if x.shape[-1] != 128:\n            x = F.interpolate(x, size=(128, 128), mode='bilinear', align_corners=False)\n        \n        # ç¼–ç åˆ°æ½œåœ¨ç©ºé—´\n        encoded = self.encoder(x)\n        mu, logvar = torch.chunk(encoded, 2, dim=1)\n        \n        # KLæ•£åº¦æŸå¤±\n        kl_loss = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n        \n        # é‡å‚æ•°åŒ–æŠ€å·§\n        std = torch.exp(0.5 * logvar)\n        eps = torch.randn_like(std)\n        z = mu + eps * std\n        \n        return z, mu, logvar, kl_loss\n    \n    def decode(self, z):\n        return self.decoder(z)\n\nclass ImprovedCrossAttention(nn.Module):\n    \n    \"\"\"\n    æ”¹è¿›çš„äº¤å‰æ³¨æ„åŠ›å®ç°ï¼Œå€Ÿé‰´å®˜æ–¹ç‰ˆæœ¬\n    \"\"\"\n\n    class ImprovedCrossAttention(nn.Module):\n        \"\"\"\n        æ”¹è¿›çš„äº¤å‰æ³¨æ„åŠ›å®ç°ï¼Œä¿®å¤ç»´åº¦ä¸åŒ¹é…é—®é¢˜\n        \"\"\"\n    def __init__(self, query_dim, context_dim=None, heads=8, dim_head=64, dropout=0.0):\n        super().__init__()\n        inner_dim = dim_head * heads\n        context_dim = context_dim if context_dim is not None else query_dim\n        \n        self.scale = dim_head ** -0.5\n        self.heads = heads\n        \n        self.to_q = nn.Linear(query_dim, inner_dim, bias=False)\n        self.to_k = nn.Linear(context_dim, inner_dim, bias=False)\n        self.to_v = nn.Linear(context_dim, inner_dim, bias=False)\n        \n        self.to_out = nn.Sequential(\n            nn.Linear(inner_dim, query_dim),\n            nn.Dropout(dropout)\n        )\n    \n    def forward(self, x, context=None):\n        context = context if context is not None else x\n        \n        # ä¿®å¤ç»´åº¦é—®é¢˜ï¼šç¡®ä¿è¾“å…¥æ˜¯æ­£ç¡®çš„å½¢çŠ¶\n        batch_size, channels, height, width = x.shape\n        \n        # é‡å¡‘ä¸ºåºåˆ—å½¢å¼ (B, H*W, C)\n        x_flat = x.view(batch_size, channels, -1).transpose(1, 2)  # (B, H*W, C)\n        \n        # åº”ç”¨çº¿æ€§å˜æ¢\n        q = self.to_q(x_flat)  # (B, H*W, inner_dim)\n        k = self.to_k(context)  # (B, seq_len, inner_dim)\n        v = self.to_v(context)  # (B, seq_len, inner_dim)\n        \n        # é‡å¡‘ä¸ºå¤šå¤´æ³¨æ„åŠ›\n        q = q.view(batch_size, -1, self.heads, q.shape[-1] // self.heads).transpose(1, 2)\n        k = k.view(batch_size, -1, self.heads, k.shape[-1] // self.heads).transpose(1, 2)\n        v = v.view(batch_size, -1, self.heads, v.shape[-1] // self.heads).transpose(1, 2)\n        \n        # è®¡ç®—æ³¨æ„åŠ›\n        scores = torch.matmul(q, k.transpose(-2, -1)) * self.scale\n        attn = F.softmax(scores, dim=-1)\n        \n        # åº”ç”¨æ³¨æ„åŠ›\n        out = torch.matmul(attn, v)\n        out = out.transpose(1, 2).contiguous().view(batch_size, -1, out.shape[-1] * self.heads)\n        \n        # è¾“å‡ºæŠ•å½±\n        out = self.to_out(out)\n        \n        # é‡å¡‘å›åŸå§‹å½¢çŠ¶ (B, C, H, W)\n        out = out.transpose(1, 2).view(batch_size, channels, height, width)\n        \n        return out\n        \n\nclass ImprovedResBlock(nn.Module):\n    \"\"\"\n    æ”¹è¿›çš„æ®‹å·®å—ï¼Œå€Ÿé‰´å®˜æ–¹å®ç°\n    \"\"\"\n    def __init__(self, channels, time_dim, dropout=0.0):\n        super().__init__()\n        \n        # åŠ¨æ€è®¡ç®—GroupNormçš„ç»„æ•°ï¼Œç¡®ä¿channelsèƒ½è¢«num_groupsæ•´é™¤\n        if channels >= 32:\n            num_groups = min(32, channels // (channels // 32))\n        elif channels >= 16:\n            num_groups = min(16, channels // (channels // 16))\n        elif channels >= 8:\n            num_groups = min(8, channels // (channels // 8))\n        elif channels >= 4:\n            num_groups = min(4, channels // (channels // 4))\n        else:\n            num_groups = 1\n        \n        # ç¡®ä¿num_groupsèƒ½æ•´é™¤channels\n        while channels % num_groups != 0 and num_groups > 1:\n            num_groups -= 1\n        \n        self.time_mlp = nn.Sequential(\n            nn.SiLU(),\n            nn.Linear(time_dim, channels)\n        )\n        \n        self.block1 = nn.Sequential(\n            nn.GroupNorm(num_groups, channels),\n            nn.SiLU(),\n            nn.Conv2d(channels, channels, 3, padding=1)\n        )\n        \n        self.block2 = nn.Sequential(\n            nn.GroupNorm(num_groups, channels),\n            nn.SiLU(),\n            nn.Dropout(dropout),\n            nn.Conv2d(channels, channels, 3, padding=1)\n        )\n        \n        # æ—¶é—´åµŒå…¥æŠ•å½±\n        self.time_proj = nn.Linear(time_dim, channels)\n        \n    def forward(self, x, time_emb):\n        h = self.block1(x)\n        \n        # æ—¶é—´åµŒå…¥å¤„ç†\n        time_emb = self.time_proj(time_emb)\n        time_emb = time_emb.view(x.shape[0], -1, 1, 1)\n        h = h + time_emb\n        \n        h = self.block2(h)\n        return h + x\n\nclass ImprovedUNet2DConditionModel(nn.Module):\n    \"\"\"\n    æ”¹è¿›çš„UNetå®ç°ï¼Œå€Ÿé‰´å®˜æ–¹æ¶æ„\n    \"\"\"\n    def __init__(self, in_channels=4, out_channels=4, model_channels=128, num_res_blocks=2, \n                 attention_resolutions=(), dropout=0.1, channel_mult=(1, 2, 4, 8), \n                 conv_resample=True, num_heads=8, context_dim=512):\n        super().__init__()\n        \n        self.in_channels = in_channels\n        self.model_channels = model_channels\n        self.num_res_blocks = num_res_blocks\n        self.attention_resolutions = attention_resolutions\n        self.dropout = dropout\n        self.channel_mult = channel_mult\n        self.conv_resample = conv_resample\n        self.num_heads = num_heads\n        self.context_dim = context_dim\n        \n        # æ—¶é—´åµŒå…¥ - ä½¿ç”¨æ›´æ·±çš„ç½‘ç»œ\n        time_embed_dim = model_channels * 4\n        self.time_embedding = nn.Sequential(\n            nn.Linear(1, time_embed_dim),\n            nn.SiLU(),\n            nn.Linear(time_embed_dim, time_embed_dim),\n            nn.SiLU(),\n            nn.Linear(time_embed_dim, time_embed_dim)\n        )\n        \n        # è¾“å…¥æŠ•å½±\n        self.input_blocks = nn.ModuleList([\n            nn.Conv2d(in_channels, model_channels, kernel_size=3, padding=1)\n        ])\n        \n        # ä¸‹é‡‡æ ·å—\n        input_block_chans = [model_channels]\n        ch = model_channels\n        \n        for level, mult in enumerate(channel_mult):\n            # æ·»åŠ ResBlock\n            for _ in range(num_res_blocks):\n                self.input_blocks.append(\n                    nn.ModuleList([ImprovedResBlock(ch, time_embed_dim, dropout)])\n                )\n                input_block_chans.append(ch)\n            \n            # æ·»åŠ CrossAttention\n            if level in attention_resolutions:\n                self.input_blocks.append(\n                    nn.ModuleList([ImprovedCrossAttention(ch, context_dim, num_heads, dropout=dropout)])\n                )\n                input_block_chans.append(ch)\n            \n            # ä¸‹é‡‡æ ·\n            if level < len(channel_mult) - 1:\n                ch = mult * model_channels\n                self.input_blocks.append(\n                    nn.ModuleList([nn.Conv2d(input_block_chans[-1], ch, 3, stride=2, padding=1)])\n                )\n                input_block_chans.append(ch)\n        \n        # ä¸­é—´å—\n        self.middle_block = nn.ModuleList([\n            ImprovedResBlock(ch, time_embed_dim, dropout),\n            ImprovedCrossAttention(ch, context_dim, num_heads, dropout=dropout),\n            ImprovedResBlock(ch, time_embed_dim, dropout)\n        ])\n        \n        # è¾“å‡ºå—\n        self.output_blocks = nn.ModuleList([])\n        for level, mult in list(enumerate(channel_mult))[::-1]:\n            # ä¸Šé‡‡æ ·\n            if level < len(channel_mult) - 1:\n                self.output_blocks.append(\n                    nn.ModuleList([nn.ConvTranspose2d(ch, ch//2, 4, stride=2, padding=1)])\n                )\n                ch = ch // 2\n            \n            # æ·»åŠ ResBlock\n            for _ in range(num_res_blocks + 1):\n                self.output_blocks.append(\n                    nn.ModuleList([ImprovedResBlock(ch, time_embed_dim, dropout)])\n                )\n            \n            # æ·»åŠ CrossAttention\n            if level in attention_resolutions:\n                self.output_blocks.append(\n                    nn.ModuleList([ImprovedCrossAttention(ch, context_dim, num_heads, dropout=dropout)])\n                )\n        \n        # è¾“å‡ºæŠ•å½±\n        if ch >= 32:\n            num_groups = 32\n        elif ch >= 16:\n            num_groups = 16\n        elif ch >= 8:\n            num_groups = 8\n        elif ch >= 4:\n            num_groups = 4\n        else:\n            num_groups = 1\n        \n        self.out = nn.Sequential(\n            nn.GroupNorm(num_groups, ch),\n            nn.SiLU(),\n            nn.Conv2d(ch, out_channels, kernel_size=3, padding=1)\n        )\n\n    \n    def forward(self, x, timesteps, context=None):\n        \"\"\"å‰å‘ä¼ æ’­ - å®Œå…¨ç¦ç”¨attention\"\"\"\n        # æ—¶é—´åµŒå…¥\n        t = self.time_embedding(timesteps.unsqueeze(-1).float())\n        if t.dim() == 1:\n            t = t.unsqueeze(0)\n        \n        # è¾“å…¥å—\n        h = x\n        hs = []\n        \n        for module in self.input_blocks:\n            if isinstance(module, nn.ModuleList):\n                for submodule in module:\n                    if isinstance(submodule, ImprovedResBlock):\n                        h = submodule(h, t)\n                    elif hasattr(submodule, 'in_channels'):  # å·ç§¯å±‚\n                        h = submodule(h)\n                    # å®Œå…¨è·³è¿‡ImprovedCrossAttention\n            else:\n                h = module(h)\n            hs.append(h)\n        \n        # ä¸­é—´å—\n        for module in self.middle_block:\n            if isinstance(module, ImprovedResBlock):\n                h = module(h, t)\n            # è·³è¿‡attention\n        \n        # è¾“å‡ºå—\n        for module in self.output_blocks:\n            if isinstance(module, nn.ModuleList):\n                for submodule in module:\n                    if isinstance(submodule, ImprovedResBlock):\n                        h = submodule(h, t)\n                    elif hasattr(submodule, 'in_channels'):  # å·ç§¯å±‚\n                        h = submodule(h)\n                    # è·³è¿‡attention\n            else:\n                h = module(h)\n            \n            # è·³è·ƒè¿æ¥\n            if hs:\n                h = torch.cat([h, hs.pop()], dim=1)\n        \n        return self.out(h)\n\n\nclass ImprovedDDPMScheduler:\n    \"\"\"\n    æ”¹è¿›çš„DDPMè°ƒåº¦å™¨ï¼Œä¿®å¤è®¾å¤‡ä¸åŒ¹é…é—®é¢˜\n    \"\"\"\n    def __init__(self, num_train_timesteps=1000, beta_start=0.0001, beta_end=0.02):\n        self.num_train_timesteps = num_train_timesteps\n        \n        # çº¿æ€§å™ªå£°è°ƒåº¦\n        self.betas = torch.linspace(beta_start, beta_end, num_train_timesteps)\n        self.alphas = 1.0 - self.betas\n        self.alphas_cumprod = torch.cumprod(self.alphas, dim=0)\n        self.alphas_cumprod_prev = torch.cat([torch.tensor([1.0]), self.alphas_cumprod[:-1]])\n        \n        # è®¡ç®—å™ªå£°é¢„æµ‹çš„ç³»æ•°\n        self.sqrt_alphas_cumprod = torch.sqrt(self.alphas_cumprod)\n        self.sqrt_one_minus_alphas_cumprod = torch.sqrt(1.0 - self.alphas_cumprod)\n    \n    def to_device(self, device):\n        \"\"\"å°†è°ƒåº¦å™¨ç§»åŠ¨åˆ°æŒ‡å®šè®¾å¤‡\"\"\"\n        self.betas = self.betas.to(device)\n        self.alphas = self.alphas.to(device)\n        self.alphas_cumprod = self.alphas_cumprod.to(device)\n        self.alphas_cumprod_prev = self.alphas_cumprod_prev.to(device)\n        self.sqrt_alphas_cumprod = self.sqrt_alphas_cumprod.to(device)\n        self.sqrt_one_minus_alphas_cumprod = self.sqrt_one_minus_alphas_cumprod.to(device)\n    \n    def add_noise(self, original_samples, noise, timesteps):\n        \"\"\"æ·»åŠ å™ªå£°åˆ°åŸå§‹æ ·æœ¬ - ä¿®å¤è®¾å¤‡ä¸åŒ¹é…\"\"\"\n        device = original_samples.device\n        \n        # ç¡®ä¿è°ƒåº¦å™¨ç³»æ•°åœ¨æ­£ç¡®è®¾å¤‡ä¸Š\n        sqrt_alpha = self.sqrt_alphas_cumprod.to(device)[timesteps].view(-1, 1, 1, 1)\n        sqrt_one_minus_alpha = self.sqrt_one_minus_alphas_cumprod.to(device)[timesteps].view(-1, 1, 1, 1)\n        \n        return sqrt_alpha * original_samples + sqrt_one_minus_alpha * noise\n    \n    def step(self, model_output, timestep, sample):\n        \"\"\"å»å™ªæ­¥éª¤ - ä¿®å¤è®¾å¤‡ä¸åŒ¹é…\"\"\"\n        device = sample.device\n        \n        alpha = self.alphas_cumprod.to(device)[timestep].view(-1, 1, 1, 1)\n        alpha_prev = self.alphas_cumprod_prev.to(device)[timestep].view(-1, 1, 1, 1)\n        \n        pred_original_sample = (sample - torch.sqrt(1 - alpha) * model_output) / torch.sqrt(alpha)\n        pred_sample_direction = torch.sqrt(1 - alpha_prev) * model_output\n        pred_prev_sample = torch.sqrt(alpha_prev) * pred_original_sample + pred_sample_direction\n        \n        return pred_prev_sample\n\nclass ImprovedStableDiffusionPipeline:\n    \"\"\"\n    æ”¹è¿›çš„Stable Diffusion Pipelineï¼Œå€Ÿé‰´å®˜æ–¹æœ€ä½³å®è·µ\n    \"\"\"\n    def __init__(self, device='cuda' if torch.cuda.is_available() else 'cpu'):\n        self.device = device\n        \n        # åˆå§‹åŒ–ç»„ä»¶\n        self.vae = ImprovedVAE().to(device)\n        self.unet = ImprovedUNet2DConditionModel(\n            in_channels=4,\n            out_channels=4,\n            model_channels=128,\n            channel_mult=(1, 2, 4, 8),\n            attention_resolutions=(8, 16),\n            context_dim=512\n        ).to(device)\n        self.scheduler = ImprovedDDPMScheduler()\n        \n        # CLIPæ–‡æœ¬ç¼–ç å™¨\n        self.tokenizer = CLIPTokenizer.from_pretrained(\"openai/clip-vit-base-patch32\")\n        self.text_encoder = CLIPTextModel.from_pretrained(\"openai/clip-vit-base-patch32\").to(device)\n        \n        # è®¾ç½®ä¸ºè¯„ä¼°æ¨¡å¼\n        self.text_encoder.eval()\n        self.vae.eval()\n        \n    def _encode_prompt(self, prompt):\n        \"\"\"ç¼–ç æ–‡æœ¬æç¤º\"\"\"\n        tokens = self.tokenizer(prompt, padding=True, return_tensors=\"pt\").to(self.device)\n        with torch.no_grad():\n            text_embeddings = self.text_encoder(**tokens).last_hidden_state\n        return text_embeddings\n    \n    def _parse_kanji_prompt(self, prompt):\n        \"\"\"è§£ææ±‰å­—æç¤ºï¼Œä½¿ç”¨æ›´è¯¦ç»†çš„æè¿°\"\"\"\n        base_prompt = f\"kanji character representing {prompt}, traditional calligraphy style, black ink on white paper, high contrast, detailed strokes, clear lines, professional quality, artistic interpretation\"\n        return base_prompt\n    \n    def generate(self, prompt, height=128, width=128, num_inference_steps=50, \n                guidance_scale=7.5, seed=None):\n        \"\"\"ç”Ÿæˆå›¾åƒï¼Œä½¿ç”¨å®˜æ–¹æ¨èçš„å‚æ•°\"\"\"\n        \n        # è®¾ç½®éšæœºç§å­\n        if seed is not None:\n            torch.manual_seed(seed)\n            torch.cuda.manual_seed(seed)\n        \n        # ç¼–ç æç¤º\n        text_embeddings = self._encode_prompt(self._parse_kanji_prompt(prompt))\n        \n        # åˆå§‹åŒ–æ½œåœ¨å˜é‡\n        latent_height = height // 8\n        latent_width = width // 8\n        latents = torch.randn(1, 4, latent_height, latent_width, device=self.device)\n        \n        # è®¾ç½®æ—¶é—´æ­¥\n        timesteps = self.scheduler.set_timesteps(num_inference_steps)\n        timesteps = timesteps.to(self.device)\n        \n        # æ”¹è¿›çš„å»å™ªå¾ªç¯\n        for i, t in enumerate(timesteps):\n            # æ‰©å±•æ½œåœ¨å˜é‡ç”¨äºæ‰¹å¤„ç†\n            latent_model_input = torch.cat([latents] * 2)\n            t_expanded = t.expand(2)\n            \n            # é¢„æµ‹å™ªå£°\n            with torch.no_grad():\n                noise_pred = self.unet(latent_model_input, t_expanded, text_embeddings)\n            \n            # æ‰§è¡Œclassifier-free guidance\n            noise_pred_uncond, noise_pred_text = noise_pred.chunk(2)\n            \n            # ä½¿ç”¨å®˜æ–¹æ¨èçš„guidance scale\n            guidance_scale = torch.clamp(torch.tensor(guidance_scale), min=1.0, max=20.0)\n            noise_pred = noise_pred_uncond + guidance_scale * (noise_pred_text - noise_pred_uncond)\n            \n            # è®¡ç®—å‰ä¸€ä¸ªæ ·æœ¬\n            latents = self.scheduler.step(noise_pred, t, latents)\n        \n        # è§£ç æ½œåœ¨å˜é‡\n        with torch.no_grad():\n            image = self.vae.decode(latents)\n        \n        return image\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-25T04:00:51.721836Z","iopub.execute_input":"2025-08-25T04:00:51.722139Z","iopub.status.idle":"2025-08-25T04:01:19.912024Z","shell.execute_reply.started":"2025-08-25T04:00:51.722115Z","shell.execute_reply":"2025-08-25T04:01:19.911404Z"}},"outputs":[{"name":"stderr","text":"2025-08-25 04:01:03.164661: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1756094463.485833      36 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1756094463.579864      36 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"from torch.amp import GradScaler, autocast  # æ–°çš„å¯¼å…¥æ–¹å¼","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-25T04:01:39.205219Z","iopub.execute_input":"2025-08-25T04:01:39.206062Z","iopub.status.idle":"2025-08-25T04:01:39.209190Z","shell.execute_reply.started":"2025-08-25T04:01:39.206037Z","shell.execute_reply":"2025-08-25T04:01:39.208656Z"}},"outputs":[],"execution_count":10},{"cell_type":"markdown","source":"## ğŸ—ï¸ colab_training.py Implementation","metadata":{}},{"cell_type":"code","source":"\"\"\"\nGoogle Colabä¼˜åŒ–çš„Stable Diffusionè®­ç»ƒè„šæœ¬\nä¸“é—¨ä¸ºColab GPUç¯å¢ƒä¼˜åŒ–ï¼ŒåŒ…å«è‡ªåŠ¨æ£€æµ‹å’Œæ€§èƒ½ä¼˜åŒ–\n\"\"\"\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader\nimport torch.nn.functional as F\nfrom torch.cuda.amp import GradScaler, autocast\nimport os\nimport sys\nimport time\nfrom pathlib import Path\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom PIL import Image\nimport gc\n\n\nclass ColabOptimizedTrainer:\n    \"\"\"\n    Colabä¼˜åŒ–çš„è®­ç»ƒå™¨\n    \"\"\"\n    def __init__(self, device='auto'):\n        # è‡ªåŠ¨æ£€æµ‹è®¾å¤‡\n        if device == 'auto':\n            if torch.cuda.is_available():\n                self.device = 'cuda'\n                print(f\"ğŸš€ æ£€æµ‹åˆ°CUDAè®¾å¤‡: {torch.cuda.get_device_name()}\")\n                print(f\"   â€¢ GPUå†…å­˜: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n                print(f\"   â€¢ CUDAç‰ˆæœ¬: {torch.version.cuda}\")\n            elif hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():\n                self.device = 'mps'\n                print(\"ğŸ æ£€æµ‹åˆ°Apple Silicon (MPS)\")\n            else:\n                self.device = 'cpu'\n                print(\"ğŸ’» ä½¿ç”¨CPUè®­ç»ƒ\")\n        else:\n            self.device = device\n        \n        # åˆå§‹åŒ–æ¨¡å‹\n        self.vae = ImprovedVAE().to(self.device)\n        self.unet = ImprovedUNet2DConditionModel(\n            in_channels=4,\n            out_channels=4,\n            model_channels=128,\n            channel_mult=(1, 2, 4, 8),\n        ).to(self.device)\n        self.scheduler = ImprovedDDPMScheduler()\n        \n        # ä¼˜åŒ–å™¨è®¾ç½®\n        self.optimizer = optim.AdamW([\n            {'params': self.vae.parameters(), 'lr': 1e-4},\n            {'params': self.unet.parameters(), 'lr': 1e-4}\n        ], weight_decay=0.01)\n        \n        # å­¦ä¹ ç‡è°ƒåº¦å™¨\n        self.scheduler_lr = optim.lr_scheduler.CosineAnnealingLR(\n            self.optimizer, T_max=100, eta_min=1e-6\n        )\n        \n        # æ··åˆç²¾åº¦è®­ç»ƒ\n        self.scaler = GradScaler()\n        \n        # è®­ç»ƒå‚æ•°\n        self.num_epochs = 50\n        self.batch_size = 8  # Colab GPUå†…å­˜ä¼˜åŒ–\n        self.gradient_accumulation_steps = 4\n        self.save_every = 5\n        \n        # æŸå¤±å‡½æ•°\n        self.mse_loss = nn.MSELoss()\n        \n        print(f\"âœ… æ¨¡å‹åˆå§‹åŒ–å®Œæˆï¼Œä½¿ç”¨è®¾å¤‡: {self.device}\")\n    \n    def create_synthetic_dataset(self, num_samples=1000):\n        \"\"\"\n        åˆ›å»ºåˆæˆæ•°æ®é›†ç”¨äºæ¼”ç¤º\n        åœ¨å®é™…ä½¿ç”¨ä¸­ï¼Œè¿™é‡Œåº”è¯¥åŠ è½½çœŸå®çš„æ±‰å­—æ•°æ®\n        \"\"\"\n        print(f\"ğŸ“Š åˆ›å»ºåˆæˆæ•°æ®é›† ({num_samples} æ ·æœ¬)...\")\n        \n        # åˆ›å»º128x128çš„åˆæˆå›¾åƒ\n        images = []\n        for i in range(num_samples):\n            # åˆ›å»ºç®€å•çš„å‡ ä½•å›¾æ¡ˆä½œä¸ºè®­ç»ƒæ•°æ®\n            img = np.zeros((128, 128, 3), dtype=np.float32)\n            \n            # æ·»åŠ ä¸€äº›éšæœºå‡ ä½•å½¢çŠ¶\n            if i % 4 == 0:\n                # åœ†å½¢\n                y, x = np.ogrid[:128, :128]\n                mask = (x - 64)**2 + (y - 64)**2 <= 30**2\n                img[mask] = [0.8, 0.8, 0.8]\n            elif i % 4 == 1:\n                # çŸ©å½¢\n                img[40:88, 40:88] = [0.7, 0.7, 0.7]\n            elif i % 4 == 2:\n                # ä¸‰è§’å½¢\n                for y in range(128):\n                    for x in range(128):\n                        if y >= 64 and abs(x - 64) <= (y - 64):\n                            img[y, x] = [0.6, 0.6, 0.6]\n            else:\n                # éšæœºå™ªå£°\n                img = np.random.rand(128, 128, 3).astype(np.float32) * 0.5\n            \n            # å½’ä¸€åŒ–åˆ°[-1, 1]\n            img = (img - 0.5) * 2\n            images.append(img)\n        \n        # è½¬æ¢ä¸ºtensor\n        images = torch.tensor(images, dtype=torch.float32).permute(0, 3, 1, 2)\n        print(f\"âœ… æ•°æ®é›†åˆ›å»ºå®Œæˆ: {images.shape}\")\n        \n        return images\n    \n    def train_epoch(self, dataloader, epoch):\n        \"\"\"\n        è®­ç»ƒä¸€ä¸ªepoch\n        \"\"\"\n        def train_epoch(self, dataloader, epoch):\n            \"\"\"\n            è®­ç»ƒä¸€ä¸ªepoch - ä¿®å¤UNetè°ƒç”¨\n            \"\"\"\n        self.vae.train()\n        self.unet.train()\n    \n        total_loss = 0\n        num_batches = len(dataloader)\n    \n        for batch_idx, images in enumerate(dataloader):\n            images = images.to(self.device)\n            \n            with autocast():\n                # VAEç¼–ç \n                latents, mu, logvar, kl_loss = self.vae.encode(images)\n                \n                # æ·»åŠ å™ªå£°\n                noise = torch.randn_like(latents, device=self.device)\n                timesteps = torch.randint(\n                    0, self.scheduler.num_train_timesteps, \n                    (latents.shape[0],), \n                    device=self.device\n                )\n                noisy_latents = self.scheduler.add_noise(latents, noise, timesteps)\n                \n                # UNeté¢„æµ‹å™ªå£° - ä¿®å¤ï¼šä¸ä¼ é€’contextå‚æ•°\n                noise_pred = self.unet(noisy_latents, timesteps)  # ç§»é™¤contextå‚æ•°\n                \n                # è®¡ç®—æŸå¤±\n                noise_loss = self.mse_loss(noise_pred, noise)\n                reconstruction_loss = self.mse_loss(self.vae.decode(latents), images)\n                \n                loss = noise_loss + 0.1 * kl_loss + 0.1 * reconstruction_loss\n                loss = loss / self.gradient_accumulation_steps\n    \n              \n            # åå‘ä¼ æ’­\n            self.scaler.scale(loss).backward()\n            \n            if (batch_idx + 1) % self.gradient_accumulation_steps == 0:\n                # æ¢¯åº¦è£å‰ª\n                self.scaler.unscale_(self.optimizer)\n                torch.nn.utils.clip_grad_norm_(\n                    list(self.vae.parameters()) + list(self.unet.parameters()), \n                    max_norm=1.0\n                )\n                \n                # ä¼˜åŒ–å™¨æ­¥è¿›\n                self.scaler.step(self.optimizer)\n                self.scaler.update()\n                self.optimizer.zero_grad()\n            \n            total_loss += loss.item() * self.gradient_accumulation_steps\n            \n            # è¿›åº¦æ˜¾ç¤º\n            if (batch_idx + 1) % 10 == 0:\n                print(f\"   Epoch {epoch+1}/{self.num_epochs}, \"\n                      f\"Batch {batch_idx+1}/{num_batches}, \"\n                      f\"Loss: {loss.item():.6f}\")\n        \n        # å­¦ä¹ ç‡è°ƒåº¦\n        self.scheduler_lr.step()\n        \n        return total_loss / num_batches\n    \n    def save_checkpoint(self, epoch, loss, save_dir=\"colab_checkpoints\"):\n        \"\"\"\n        ä¿å­˜æ£€æŸ¥ç‚¹\n        \"\"\"\n        os.makedirs(save_dir, exist_ok=True)\n        \n        checkpoint = {\n            'epoch': epoch,\n            'vae_state_dict': self.vae.state_dict(),\n            'unet_state_dict': self.unet.state_dict(),\n            'optimizer_state_dict': self.optimizer.state_dict(),\n            'scheduler_state_dict': self.scheduler_lr.state_dict(),\n            'loss': loss,\n            'device': self.device\n        }\n        \n        checkpoint_path = os.path.join(save_dir, f'checkpoint_epoch_{epoch+1}.pth')\n        torch.save(checkpoint, checkpoint_path)\n        print(f\"ğŸ’¾ æ£€æŸ¥ç‚¹å·²ä¿å­˜: {checkpoint_path}\")\n        \n        # ä¿å­˜æœ€ä½³æ¨¡å‹\n        if epoch == 0 or loss < getattr(self, 'best_loss', float('inf')):\n            self.best_loss = loss\n            best_model_path = os.path.join(save_dir, 'best_model.pth')\n            torch.save(checkpoint, best_model_path)\n            print(f\"ğŸ† æœ€ä½³æ¨¡å‹å·²ä¿å­˜: {best_model_path}\")\n    \n    def train(self):\n        \"\"\"\n        ä¸»è®­ç»ƒå¾ªç¯\n        \"\"\"\n        print(f\"\\nğŸ¯ å¼€å§‹è®­ç»ƒ...\")\n        print(f\"   â€¢ è®¾å¤‡: {self.device}\")\n        print(f\"   â€¢ æ‰¹æ¬¡å¤§å°: {self.batch_size}\")\n        print(f\"   â€¢ æ¢¯åº¦ç´¯ç§¯æ­¥æ•°: {self.gradient_accumulation_steps}\")\n        print(f\"   â€¢ æ€»epochs: {self.num_epochs}\")\n        print(f\"   â€¢ æ··åˆç²¾åº¦: {'å¯ç”¨' if self.device == 'cuda' else 'ç¦ç”¨'}\")\n        \n        # åˆ›å»ºæ•°æ®é›†\n        images = self.create_synthetic_dataset()\n        dataloader = DataLoader(images, batch_size=self.batch_size, shuffle=True)\n        \n        # è®­ç»ƒå†å²\n        train_losses = []\n        start_time = time.time()\n        \n        try:\n            for epoch in range(self.num_epochs):\n                epoch_start = time.time()\n                \n                print(f\"\\nğŸ”„ Epoch {epoch+1}/{self.num_epochs}\")\n                print(\"-\" * 50)\n                \n                # è®­ç»ƒ\n                loss = self.train_epoch(dataloader, epoch)\n                train_losses.append(loss)\n                \n                epoch_time = time.time() - epoch_start\n                print(f\"   â±ï¸  Epochè€—æ—¶: {epoch_time:.2f}ç§’\")\n                print(f\"   ğŸ“Š å¹³å‡æŸå¤±: {loss:.6f}\")\n                print(f\"   ğŸ“ˆ å­¦ä¹ ç‡: {self.optimizer.param_groups[0]['lr']:.2e}\")\n                \n                # ä¿å­˜æ£€æŸ¥ç‚¹\n                if (epoch + 1) % self.save_every == 0:\n                    self.save_checkpoint(epoch, loss)\n                \n                # å†…å­˜æ¸…ç† (Colabä¼˜åŒ–)\n                if self.device == 'cuda':\n                    torch.cuda.empty_cache()\n                    gc.collect()\n                \n                # æ˜¾ç¤ºGPUå†…å­˜ä½¿ç”¨æƒ…å†µ\n                if self.device == 'cuda':\n                    memory_allocated = torch.cuda.memory_allocated() / 1e9\n                    memory_reserved = torch.cuda.memory_reserved() / 1e9\n                    print(f\"   ğŸ§  GPUå†…å­˜: {memory_allocated:.2f}GB / {memory_reserved:.2f}GB\")\n        \n        except KeyboardInterrupt:\n            print(f\"\\nâš ï¸  è®­ç»ƒè¢«ç”¨æˆ·ä¸­æ–­\")\n        except Exception as e:\n            print(f\"\\nâŒ è®­ç»ƒå‡ºé”™: {e}\")\n            import traceback\n            traceback.print_exc()\n        \n        finally:\n            # ä¿å­˜æœ€ç»ˆæ¨¡å‹\n            final_loss = train_losses[-1] if train_losses else float('inf')\n            self.save_checkpoint(len(train_losses) - 1, final_loss)\n            \n            # è®­ç»ƒæ€»ç»“\n            total_time = time.time() - start_time\n            print(f\"\\nğŸ‰ è®­ç»ƒå®Œæˆ!\")\n            print(f\"   â±ï¸  æ€»è€—æ—¶: {total_time:.2f}ç§’\")\n            print(f\"   ğŸ“Š æœ€ç»ˆæŸå¤±: {final_loss:.6f}\")\n            print(f\"   ğŸ“ˆ æŸå¤±å˜åŒ–: {train_losses[0]:.6f} â†’ {final_loss:.6f}\")\n            \n            # ç»˜åˆ¶æŸå¤±æ›²çº¿\n            self.plot_training_curve(train_losses)\n    \n    def plot_training_curve(self, losses):\n        \"\"\"\n        ç»˜åˆ¶è®­ç»ƒæŸå¤±æ›²çº¿\n        \"\"\"\n        plt.figure(figsize=(10, 6))\n        plt.plot(losses, 'b-', linewidth=2, label='è®­ç»ƒæŸå¤±')\n        plt.title('Colabè®­ç»ƒæŸå¤±æ›²çº¿', fontsize=16)\n        plt.xlabel('Epoch', fontsize=14)\n        plt.ylabel('æŸå¤±', fontsize=14)\n        plt.grid(True, alpha=0.3)\n        plt.legend(fontsize=12)\n        plt.tight_layout()\n        \n        # ä¿å­˜å›¾ç‰‡\n        plot_path = 'colab_training_curve.png'\n        plt.savefig(plot_path, dpi=150, bbox_inches='tight')\n        print(f\"ğŸ“Š è®­ç»ƒæ›²çº¿å·²ä¿å­˜: {plot_path}\")\n        plt.show()\n    \n    def test_generation(self, prompt=\"water\"):\n        \"\"\"\n        æµ‹è¯•ç”ŸæˆåŠŸèƒ½\n        \"\"\"\n        print(f\"\\nğŸ§ª æµ‹è¯•ç”Ÿæˆ: {prompt}\")\n        \n        try:\n            # åˆ›å»ºpipeline\n            pipeline = ImprovedStableDiffusionPipeline(device=self.device)\n            \n            # åŠ è½½è®­ç»ƒå¥½çš„æƒé‡\n            if hasattr(self, 'best_loss'):\n                checkpoint_path = 'colab_checkpoints/best_model.pth'\n                if os.path.exists(checkpoint_path):\n                    checkpoint = torch.load(checkpoint_path, map_location=self.device)\n                    pipeline.vae.load_state_dict(checkpoint['vae_state_dict'])\n                    pipeline.unet.load_state_dict(checkpoint['unet_state_dict'])\n                    print(f\"âœ… å·²åŠ è½½æœ€ä½³æ¨¡å‹æƒé‡\")\n            \n            # ç”Ÿæˆå›¾åƒ\n            print(f\"ğŸŒŠ ç”Ÿæˆä¸­...\")\n            result = pipeline.generate(\n                prompt,\n                height=128,\n                width=128,\n                num_inference_steps=50,\n                guidance_scale=7.5,\n                seed=42\n            )\n            \n            # ä¿å­˜ç»“æœ\n            if isinstance(result, torch.Tensor):\n                result = (result + 1) / 2\n                result = torch.clamp(result, 0, 1)\n                img_array = result.squeeze(0).permute(1, 2, 0).cpu().numpy()\n                pil_image = Image.fromarray((img_array * 255).astype(np.uint8))\n            else:\n                pil_image = result\n            \n            output_path = f'colab_generated_{prompt}.png'\n            pil_image.save(output_path)\n            print(f\"âœ… ç”Ÿæˆå®Œæˆï¼Œå·²ä¿å­˜: {output_path}\")\n            \n            # æ˜¾ç¤ºå›¾åƒ\n            plt.figure(figsize=(6, 6))\n            plt.imshow(pil_image, cmap='gray')\n            plt.title(f'Colabç”Ÿæˆ: {prompt}', fontsize=14)\n            plt.axis('off')\n            plt.show()\n            \n        except Exception as e:\n            print(f\"âŒ ç”Ÿæˆæµ‹è¯•å¤±è´¥: {e}\")\n            import traceback\n            traceback.print_exc()\n\ndef main():\n    \"\"\"\n    ä¸»å‡½æ•°\n    \"\"\"\n    print(\"ğŸš€ Google Colabä¼˜åŒ–çš„Stable Diffusionè®­ç»ƒå™¨\")\n    print(\"=\" * 60)\n    \n    # æ£€æŸ¥Colabç¯å¢ƒ\n    is_colab = 'COLAB_GPU' in os.environ\n    if is_colab:\n        print(\"âœ… æ£€æµ‹åˆ°Google Colabç¯å¢ƒ\")\n        print(f\"   â€¢ GPUç±»å‹: {os.environ.get('COLAB_GPU', 'Unknown')}\")\n        print(f\"   â€¢ è¿è¡Œæ—¶ç±»å‹: {os.environ.get('COLAB_RUNTIME_TYPE', 'Unknown')}\")\n    else:\n        print(\"ğŸ’» æœ¬åœ°ç¯å¢ƒè¿è¡Œ\")\n    \n    # åˆ›å»ºè®­ç»ƒå™¨\n    trainer = ColabOptimizedTrainer(device='auto')\n    \n    # å¼€å§‹è®­ç»ƒ\n    trainer.train()\n    \n    # æµ‹è¯•ç”Ÿæˆ\n    trainer.test_generation(\"water\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-25T04:01:41.786040Z","iopub.execute_input":"2025-08-25T04:01:41.786666Z","iopub.status.idle":"2025-08-25T04:01:41.821129Z","shell.execute_reply.started":"2025-08-25T04:01:41.786641Z","shell.execute_reply":"2025-08-25T04:01:41.820544Z"}},"outputs":[],"execution_count":11},{"cell_type":"markdown","source":"## ğŸš€ Start Training","metadata":{}},{"cell_type":"code","source":"# Create trainer and start training\nif 'ColabOptimizedTrainer' in globals():\n    trainer = ColabOptimizedTrainer()\n    trainer.train()\nelse:\n    print(\"âš ï¸ Trainer class not found. Please run the model implementation cells first.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-25T04:01:50.046539Z","iopub.execute_input":"2025-08-25T04:01:50.046912Z","iopub.status.idle":"2025-08-25T04:02:18.289921Z","shell.execute_reply.started":"2025-08-25T04:01:50.046886Z","shell.execute_reply":"2025-08-25T04:02:18.288918Z"}},"outputs":[{"name":"stdout","text":"ğŸš€ æ£€æµ‹åˆ°CUDAè®¾å¤‡: Tesla T4\n   â€¢ GPUå†…å­˜: 15.8 GB\n   â€¢ CUDAç‰ˆæœ¬: 11.8\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_36/4175265272.py:65: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n  self.scaler = GradScaler()\n","output_type":"stream"},{"name":"stdout","text":"âœ… æ¨¡å‹åˆå§‹åŒ–å®Œæˆï¼Œä½¿ç”¨è®¾å¤‡: cuda\n\nğŸ¯ å¼€å§‹è®­ç»ƒ...\n   â€¢ è®¾å¤‡: cuda\n   â€¢ æ‰¹æ¬¡å¤§å°: 8\n   â€¢ æ¢¯åº¦ç´¯ç§¯æ­¥æ•°: 4\n   â€¢ æ€»epochs: 50\n   â€¢ æ··åˆç²¾åº¦: å¯ç”¨\nğŸ“Š åˆ›å»ºåˆæˆæ•°æ®é›† (1000 æ ·æœ¬)...\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_36/4175265272.py:115: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /pytorch/torch/csrc/utils/tensor_new.cpp:254.)\n  images = torch.tensor(images, dtype=torch.float32).permute(0, 3, 1, 2)\n/tmp/ipykernel_36/4175265272.py:137: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with autocast():\n","output_type":"stream"},{"name":"stdout","text":"âœ… æ•°æ®é›†åˆ›å»ºå®Œæˆ: torch.Size([1000, 3, 128, 128])\n\nğŸ”„ Epoch 1/50\n--------------------------------------------------\n\nâŒ è®­ç»ƒå‡ºé”™: Expected weight to be a vector of size equal to the number of channels in input, but got weight of shape [512] and input of shape [8, 1024, 1, 1]\n","output_type":"stream"},{"name":"stderr","text":"Traceback (most recent call last):\n  File \"/tmp/ipykernel_36/4175265272.py\", line 244, in train\n    loss = self.train_epoch(dataloader, epoch)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/tmp/ipykernel_36/4175265272.py\", line 151, in train_epoch\n    noise_pred = self.unet(noisy_latents, timesteps)  # ç§»é™¤contextå‚æ•°\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1751, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1762, in _call_impl\n    return forward_call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/tmp/ipykernel_36/439328512.py\", line 357, in forward\n    h = submodule(h, t)\n        ^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1751, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1762, in _call_impl\n    return forward_call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/tmp/ipykernel_36/439328512.py\", line 204, in forward\n    h = self.block1(x)\n        ^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1751, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1762, in _call_impl\n    return forward_call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/container.py\", line 240, in forward\n    input = module(input)\n            ^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1751, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1762, in _call_impl\n    return forward_call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/normalization.py\", line 313, in forward\n    return F.group_norm(input, self.num_groups, self.weight, self.bias, self.eps)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/functional.py\", line 2965, in group_norm\n    return torch.group_norm(\n           ^^^^^^^^^^^^^^^^^\nRuntimeError: Expected weight to be a vector of size equal to the number of channels in input, but got weight of shape [512] and input of shape [8, 1024, 1, 1]\n","output_type":"stream"},{"name":"stdout","text":"ğŸ’¾ æ£€æŸ¥ç‚¹å·²ä¿å­˜: colab_checkpoints/checkpoint_epoch_0.pth\n\nğŸ‰ è®­ç»ƒå®Œæˆ!\n   â±ï¸  æ€»è€—æ—¶: 0.81ç§’\n   ğŸ“Š æœ€ç»ˆæŸå¤±: inf\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_36/1663092989.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;34m'ColabOptimizedTrainer'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mglobals\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mtrainer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mColabOptimizedTrainer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"âš ï¸ Trainer class not found. Please run the model implementation cells first.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_36/4175265272.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    282\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"   â±ï¸  æ€»è€—æ—¶: {total_time:.2f}ç§’\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    283\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"   ğŸ“Š æœ€ç»ˆæŸå¤±: {final_loss:.6f}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 284\u001b[0;31m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"   ğŸ“ˆ æŸå¤±å˜åŒ–: {train_losses[0]:.6f} â†’ {final_loss:.6f}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    285\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    286\u001b[0m             \u001b[0;31m# ç»˜åˆ¶æŸå¤±æ›²çº¿\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mIndexError\u001b[0m: list index out of range"],"ename":"IndexError","evalue":"list index out of range","output_type":"error"}],"execution_count":12},{"cell_type":"markdown","source":"## ğŸ“¥ Download Results","metadata":{}},{"cell_type":"code","source":"# Download training results\nfrom google.colab import files\nimport zipfile\n\ndef download_results():\n    print(\"ğŸ“¥ Preparing results for download...\")\n    \n    # Create results zip\n    with zipfile.ZipFile('training_results.zip', 'w') as zipf:\n        # Add checkpoints\n        if os.path.exists('checkpoints'):\n            for root, dirs, files in os.walk('checkpoints'):\n                for file in files:\n                    file_path = os.path.join(root, file)\n                    zipf.write(file_path, os.path.relpath(file_path, '.'))\n        \n        # Add training curves\n        for img_file in ['training_curve.png', 'loss_curve.png']:\n            if os.path.exists(img_file):\n                zipf.write(img_file)\n        \n        # Add generated images\n        for i in range(10):\n            img_file = f'generated_{i}.png'\n            if os.path.exists(img_file):\n                zipf.write(img_file)\n    \n    print(\"âœ… Results packaged: training_results.zip\")\n    \n    # Download\n    try:\n        files.download('training_results.zip')\n        print(\"ğŸ“¥ Results downloaded successfully!\")\n    except:\n        print(\"âš ï¸ Download failed (not in Colab)\")\n        print(\"ğŸ“ Files are saved in the current directory\")\n\n# Download results\ndownload_results()","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}