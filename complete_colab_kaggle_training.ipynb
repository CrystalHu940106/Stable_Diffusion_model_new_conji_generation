{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# 🚀 Complete Stable Diffusion Kanji Generation - Colab/Kaggle\n\n**Single file training notebook** - Upload to Colab/Kaggle and start training immediately!\n\n## 🎯 Features\n- ✅ **Complete Training Pipeline**: VAE + UNet + DDPM\n- 🚀 **GPU Optimized**: Auto CUDA/MPS detection\n- 💾 **Auto-save**: Checkpoints every 5 epochs\n- 📊 **Real-time Monitoring**: Loss curves and GPU stats\n- 🔄 **Resume Training**: Continue from any checkpoint\n- 🎌 **Kanji Generation**: Text-to-Kanji capabilities\n\n## 🚀 Quick Start\n1. Upload this notebook to Colab/Kaggle\n2. Select GPU runtime\n3. Run all cells\n4. Start training!\n\n**Expected Training Time**:\n- Colab Free (T4): 50 epochs in 2-3 hours\n- Colab Pro (V100/P100): 50 epochs in 1-1.5 hours\n- Kaggle (P100): 50 epochs in 1-2 hours","metadata":{}},{"cell_type":"markdown","source":"## 📦 Install Dependencies","metadata":{}},{"cell_type":"code","source":"# Install required packages\n!pip install transformers pillow matplotlib scikit-image opencv-python tqdm\n!pip install --upgrade torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n\nprint(\"✅ Dependencies installed successfully!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-25T03:56:59.783080Z","iopub.execute_input":"2025-08-25T03:56:59.783319Z","iopub.status.idle":"2025-08-25T04:00:14.325117Z","shell.execute_reply.started":"2025-08-25T03:56:59.783293Z","shell.execute_reply":"2025-08-25T04:00:14.323897Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.52.4)\nRequirement already satisfied: pillow in /usr/local/lib/python3.11/dist-packages (11.2.1)\nRequirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (3.7.2)\nRequirement already satisfied: scikit-image in /usr/local/lib/python3.11/dist-packages (0.25.2)\nRequirement already satisfied: opencv-python in /usr/local/lib/python3.11/dist-packages (4.11.0.86)\nRequirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (4.67.1)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\nRequirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.33.1)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (25.0)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\nRequirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.4)\nRequirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.2)\nRequirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\nRequirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.3.2)\nRequirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (0.12.1)\nRequirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (4.58.4)\nRequirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.4.8)\nRequirement already satisfied: pyparsing<3.1,>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (3.0.9)\nRequirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (2.9.0.post0)\nRequirement already satisfied: scipy>=1.11.4 in /usr/local/lib/python3.11/dist-packages (from scikit-image) (1.15.3)\nRequirement already satisfied: networkx>=3.0 in /usr/local/lib/python3.11/dist-packages (from scikit-image) (3.5)\nRequirement already satisfied: imageio!=2.35.0,>=2.33 in /usr/local/lib/python3.11/dist-packages (from scikit-image) (2.37.0)\nRequirement already satisfied: tifffile>=2022.8.12 in /usr/local/lib/python3.11/dist-packages (from scikit-image) (2025.6.11)\nRequirement already satisfied: lazy-loader>=0.4 in /usr/local/lib/python3.11/dist-packages (from scikit-image) (0.4)\nRequirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (2025.5.1)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.14.0)\nRequirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (1.1.5)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (2025.2.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (2022.2.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (2.4.1)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\nRequirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.2)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.5.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.6.15)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->transformers) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->transformers) (2022.2.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->transformers) (1.4.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.17->transformers) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.17->transformers) (2024.2.0)\nLooking in indexes: https://download.pytorch.org/whl/cu118\nRequirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\nCollecting torch\n  Downloading https://download.pytorch.org/whl/cu118/torch-2.7.1%2Bcu118-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (28 kB)\nRequirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (0.21.0+cu124)\nCollecting torchvision\n  Downloading https://download.pytorch.org/whl/cu118/torchvision-0.22.1%2Bcu118-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (6.1 kB)\nRequirement already satisfied: torchaudio in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\nCollecting torchaudio\n  Downloading https://download.pytorch.org/whl/cu118/torchaudio-2.7.1%2Bcu118-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (6.6 kB)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.18.0)\nRequirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.14.0)\nCollecting sympy>=1.13.3 (from torch)\n  Downloading https://download.pytorch.org/whl/sympy-1.13.3-py3-none-any.whl.metadata (12 kB)\nRequirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.5)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.5.1)\nCollecting nvidia-cuda-nvrtc-cu11==11.8.89 (from torch)\n  Downloading https://download.pytorch.org/whl/cu118/nvidia_cuda_nvrtc_cu11-11.8.89-py3-none-manylinux1_x86_64.whl (23.2 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.2/23.2 MB\u001b[0m \u001b[31m77.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hCollecting nvidia-cuda-runtime-cu11==11.8.89 (from torch)\n  Downloading https://download.pytorch.org/whl/cu118/nvidia_cuda_runtime_cu11-11.8.89-py3-none-manylinux1_x86_64.whl (875 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m875.6/875.6 kB\u001b[0m \u001b[31m39.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting nvidia-cuda-cupti-cu11==11.8.87 (from torch)\n  Downloading https://download.pytorch.org/whl/cu118/nvidia_cuda_cupti_cu11-11.8.87-py3-none-manylinux1_x86_64.whl (13.1 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.1/13.1 MB\u001b[0m \u001b[31m100.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hCollecting nvidia-cudnn-cu11==9.1.0.70 (from torch)\n  Downloading https://download.pytorch.org/whl/cu118/nvidia_cudnn_cu11-9.1.0.70-py3-none-manylinux2014_x86_64.whl (663.9 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m663.9/663.9 MB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hCollecting nvidia-cublas-cu11==11.11.3.6 (from torch)\n  Downloading https://download.pytorch.org/whl/cu118/nvidia_cublas_cu11-11.11.3.6-py3-none-manylinux1_x86_64.whl (417.9 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m417.9/417.9 MB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hCollecting nvidia-cufft-cu11==10.9.0.58 (from torch)\n  Downloading https://download.pytorch.org/whl/cu118/nvidia_cufft_cu11-10.9.0.58-py3-none-manylinux1_x86_64.whl (168.4 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m168.4/168.4 MB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hCollecting nvidia-curand-cu11==10.3.0.86 (from torch)\n  Downloading https://download.pytorch.org/whl/cu118/nvidia_curand_cu11-10.3.0.86-py3-none-manylinux1_x86_64.whl (58.1 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.1/58.1 MB\u001b[0m \u001b[31m30.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hCollecting nvidia-cusolver-cu11==11.4.1.48 (from torch)\n  Downloading https://download.pytorch.org/whl/cu118/nvidia_cusolver_cu11-11.4.1.48-py3-none-manylinux1_x86_64.whl (128.2 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m128.2/128.2 MB\u001b[0m \u001b[31m13.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hCollecting nvidia-cusparse-cu11==11.7.5.86 (from torch)\n  Downloading https://download.pytorch.org/whl/cu118/nvidia_cusparse_cu11-11.7.5.86-py3-none-manylinux1_x86_64.whl (204.1 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m204.1/204.1 MB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hCollecting nvidia-nccl-cu11==2.21.5 (from torch)\n  Downloading https://download.pytorch.org/whl/cu118/nvidia_nccl_cu11-2.21.5-py3-none-manylinux2014_x86_64.whl (147.8 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m147.8/147.8 MB\u001b[0m \u001b[31m11.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hCollecting nvidia-nvtx-cu11==11.8.86 (from torch)\n  Downloading https://download.pytorch.org/whl/cu118/nvidia_nvtx_cu11-11.8.86-py3-none-manylinux1_x86_64.whl (99 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting triton==3.3.1 (from torch)\n  Downloading https://download.pytorch.org/whl/triton-3.3.1-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (1.5 kB)\nRequirement already satisfied: setuptools>=40.8.0 in /usr/local/lib/python3.11/dist-packages (from triton==3.3.1->torch) (75.2.0)\nRequirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchvision) (1.26.4)\nRequirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision) (11.2.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision) (2025.2.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision) (2022.2.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision) (2.4.1)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->torchvision) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->torchvision) (2022.2.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy->torchvision) (1.4.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy->torchvision) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy->torchvision) (2024.2.0)\nDownloading https://download.pytorch.org/whl/cu118/torch-2.7.1%2Bcu118-cp311-cp311-manylinux_2_28_x86_64.whl (905.3 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m905.3/905.3 MB\u001b[0m \u001b[31m911.9 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading https://download.pytorch.org/whl/triton-3.3.1-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (155.7 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m155.7/155.7 MB\u001b[0m \u001b[31m11.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading https://download.pytorch.org/whl/cu118/torchvision-0.22.1%2Bcu118-cp311-cp311-manylinux_2_28_x86_64.whl (6.7 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.7/6.7 MB\u001b[0m \u001b[31m93.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading https://download.pytorch.org/whl/cu118/torchaudio-2.7.1%2Bcu118-cp311-cp311-manylinux_2_28_x86_64.whl (3.3 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m80.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25hDownloading https://download.pytorch.org/whl/sympy-1.13.3-py3-none-any.whl (6.2 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.2/6.2 MB\u001b[0m \u001b[31m77.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: triton, sympy, nvidia-nvtx-cu11, nvidia-nccl-cu11, nvidia-cusparse-cu11, nvidia-curand-cu11, nvidia-cufft-cu11, nvidia-cuda-runtime-cu11, nvidia-cuda-nvrtc-cu11, nvidia-cuda-cupti-cu11, nvidia-cublas-cu11, nvidia-cusolver-cu11, nvidia-cudnn-cu11, torch, torchaudio, torchvision\n  Attempting uninstall: triton\n    Found existing installation: triton 3.2.0\n    Uninstalling triton-3.2.0:\n      Successfully uninstalled triton-3.2.0\n  Attempting uninstall: sympy\n    Found existing installation: sympy 1.13.1\n    Uninstalling sympy-1.13.1:\n      Successfully uninstalled sympy-1.13.1\n  Attempting uninstall: torch\n    Found existing installation: torch 2.6.0+cu124\n    Uninstalling torch-2.6.0+cu124:\n      Successfully uninstalled torch-2.6.0+cu124\n  Attempting uninstall: torchaudio\n    Found existing installation: torchaudio 2.6.0+cu124\n    Uninstalling torchaudio-2.6.0+cu124:\n      Successfully uninstalled torchaudio-2.6.0+cu124\n  Attempting uninstall: torchvision\n    Found existing installation: torchvision 0.21.0+cu124\n    Uninstalling torchvision-0.21.0+cu124:\n      Successfully uninstalled torchvision-0.21.0+cu124\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nfastai 2.7.19 requires torch<2.7,>=1.10, but you have torch 2.7.1+cu118 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed nvidia-cublas-cu11-11.11.3.6 nvidia-cuda-cupti-cu11-11.8.87 nvidia-cuda-nvrtc-cu11-11.8.89 nvidia-cuda-runtime-cu11-11.8.89 nvidia-cudnn-cu11-9.1.0.70 nvidia-cufft-cu11-10.9.0.58 nvidia-curand-cu11-10.3.0.86 nvidia-cusolver-cu11-11.4.1.48 nvidia-cusparse-cu11-11.7.5.86 nvidia-nccl-cu11-2.21.5 nvidia-nvtx-cu11-11.8.86 sympy-1.13.3 torch-2.7.1+cu118 torchaudio-2.7.1+cu118 torchvision-0.22.1+cu118 triton-3.3.1\n✅ Dependencies installed successfully!\n","output_type":"stream"}],"execution_count":1},{"cell_type":"markdown","source":"## 🔧 Check GPU and Environment","metadata":{}},{"cell_type":"code","source":"import torch\nimport os\n\n# Check environment\nis_colab = 'COLAB_GPU' in os.environ\nis_kaggle = 'KAGGLE_KERNEL_RUN_TYPE' in os.environ\n\nprint(f\"🌐 Environment: {'Colab' if is_colab else 'Kaggle' if is_kaggle else 'Local'}\")\nprint(f\"PyTorch: {torch.__version__}\")\nprint(f\"CUDA available: {torch.cuda.is_available()}\")\n\nif torch.cuda.is_available():\n    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\nelif hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():\n    print(\"🍎 Apple Silicon (MPS) available\")\nelse:\n    print(\"⚠️ Using CPU (will be slow!)\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-25T04:00:48.231909Z","iopub.execute_input":"2025-08-25T04:00:48.232177Z","iopub.status.idle":"2025-08-25T04:00:48.237940Z","shell.execute_reply.started":"2025-08-25T04:00:48.232155Z","shell.execute_reply":"2025-08-25T04:00:48.237195Z"}},"outputs":[{"name":"stdout","text":"🌐 Environment: Kaggle\nPyTorch: 2.7.1+cu118\nCUDA available: True\nGPU: Tesla T4\nGPU Memory: 15.8 GB\n","output_type":"stream"}],"execution_count":6},{"cell_type":"markdown","source":"## 🏗️ improved_stable_diffusion.py Implementation","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom transformers import CLIPTokenizer, CLIPTextModel\nimport math\nfrom typing import Optional, Union, Tuple\nimport numpy as np\n\nclass ImprovedVAE(nn.Module):\n    \"\"\"\n    改进的VAE实现，借鉴官方架构\n    \"\"\"\n    def __init__(self, in_channels=3, latent_channels=4, hidden_dims=[128, 256, 512, 1024]):\n        super().__init__()\n        self.latent_channels = latent_channels\n        \n        # Encoder - 使用更深的网络\n        encoder_layers = []\n        in_ch = in_channels\n        for h_dim in hidden_dims:\n            # 计算合适的GroupNorm组数\n            num_groups = min(32, h_dim)\n            while h_dim % num_groups != 0 and num_groups > 1:\n                num_groups -= 1\n            \n            encoder_layers.extend([\n                nn.Conv2d(in_ch, h_dim, kernel_size=3, stride=2, padding=1),\n                nn.GroupNorm(num_groups, h_dim),  # 使用GroupNorm替代BatchNorm\n                nn.SiLU()  # 使用SiLU替代LeakyReLU\n            ])\n            in_ch = h_dim\n        \n        # Final encoding layer\n        final_channels = latent_channels * 2\n        num_groups = min(32, final_channels)\n        while final_channels % num_groups != 0 and num_groups > 1:\n            num_groups -= 1\n        \n        encoder_layers.extend([\n            nn.Conv2d(hidden_dims[-1], final_channels, kernel_size=3, padding=1),\n            nn.GroupNorm(num_groups, final_channels)\n        ])\n        \n        self.encoder = nn.Sequential(*encoder_layers)\n        \n        # Decoder - 确保精确的128x128输出\n        decoder_layers = []\n        in_ch = latent_channels\n        \n        # 使用hidden_dims的反序进行上采样\n        hidden_dims_rev = hidden_dims[::-1]\n        \n        for i, h_dim in enumerate(hidden_dims_rev):\n            # 计算合适的GroupNorm组数\n            num_groups = min(32, h_dim)\n            while h_dim % num_groups != 0 and num_groups > 1:\n                num_groups -= 1\n            \n            decoder_layers.extend([\n                nn.ConvTranspose2d(in_ch, h_dim, kernel_size=4, stride=2, padding=1),\n                nn.GroupNorm(num_groups, h_dim),\n                nn.SiLU()\n            ])\n            in_ch = h_dim\n        \n        # 最终输出层\n        decoder_layers.extend([\n            nn.Conv2d(in_ch, in_channels, kernel_size=3, stride=1, padding=1),\n            nn.Tanh()\n        ])\n        \n        self.decoder = nn.Sequential(*decoder_layers)\n        \n    def encode(self, x):\n        # 确保输入是128x128\n        if x.shape[-1] != 128:\n            x = F.interpolate(x, size=(128, 128), mode='bilinear', align_corners=False)\n        \n        # 编码到潜在空间\n        encoded = self.encoder(x)\n        mu, logvar = torch.chunk(encoded, 2, dim=1)\n        \n        # KL散度损失\n        kl_loss = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n        \n        # 重参数化技巧\n        std = torch.exp(0.5 * logvar)\n        eps = torch.randn_like(std)\n        z = mu + eps * std\n        \n        return z, mu, logvar, kl_loss\n    \n    def decode(self, z):\n        return self.decoder(z)\n\nclass ImprovedCrossAttention(nn.Module):\n    \n    \"\"\"\n    改进的交叉注意力实现，借鉴官方版本\n    \"\"\"\n\n    class ImprovedCrossAttention(nn.Module):\n        \"\"\"\n        改进的交叉注意力实现，修复维度不匹配问题\n        \"\"\"\n    def __init__(self, query_dim, context_dim=None, heads=8, dim_head=64, dropout=0.0):\n        super().__init__()\n        inner_dim = dim_head * heads\n        context_dim = context_dim if context_dim is not None else query_dim\n        \n        self.scale = dim_head ** -0.5\n        self.heads = heads\n        \n        self.to_q = nn.Linear(query_dim, inner_dim, bias=False)\n        self.to_k = nn.Linear(context_dim, inner_dim, bias=False)\n        self.to_v = nn.Linear(context_dim, inner_dim, bias=False)\n        \n        self.to_out = nn.Sequential(\n            nn.Linear(inner_dim, query_dim),\n            nn.Dropout(dropout)\n        )\n    \n    def forward(self, x, context=None):\n        context = context if context is not None else x\n        \n        # 修复维度问题：确保输入是正确的形状\n        batch_size, channels, height, width = x.shape\n        \n        # 重塑为序列形式 (B, H*W, C)\n        x_flat = x.view(batch_size, channels, -1).transpose(1, 2)  # (B, H*W, C)\n        \n        # 应用线性变换\n        q = self.to_q(x_flat)  # (B, H*W, inner_dim)\n        k = self.to_k(context)  # (B, seq_len, inner_dim)\n        v = self.to_v(context)  # (B, seq_len, inner_dim)\n        \n        # 重塑为多头注意力\n        q = q.view(batch_size, -1, self.heads, q.shape[-1] // self.heads).transpose(1, 2)\n        k = k.view(batch_size, -1, self.heads, k.shape[-1] // self.heads).transpose(1, 2)\n        v = v.view(batch_size, -1, self.heads, v.shape[-1] // self.heads).transpose(1, 2)\n        \n        # 计算注意力\n        scores = torch.matmul(q, k.transpose(-2, -1)) * self.scale\n        attn = F.softmax(scores, dim=-1)\n        \n        # 应用注意力\n        out = torch.matmul(attn, v)\n        out = out.transpose(1, 2).contiguous().view(batch_size, -1, out.shape[-1] * self.heads)\n        \n        # 输出投影\n        out = self.to_out(out)\n        \n        # 重塑回原始形状 (B, C, H, W)\n        out = out.transpose(1, 2).view(batch_size, channels, height, width)\n        \n        return out\n        \n\nclass ImprovedResBlock(nn.Module):\n    \"\"\"\n    改进的残差块，借鉴官方实现\n    \"\"\"\n    def __init__(self, channels, time_dim, dropout=0.0):\n        super().__init__()\n        \n        # 动态计算GroupNorm的组数，确保channels能被num_groups整除\n        if channels >= 32:\n            num_groups = min(32, channels // (channels // 32))\n        elif channels >= 16:\n            num_groups = min(16, channels // (channels // 16))\n        elif channels >= 8:\n            num_groups = min(8, channels // (channels // 8))\n        elif channels >= 4:\n            num_groups = min(4, channels // (channels // 4))\n        else:\n            num_groups = 1\n        \n        # 确保num_groups能整除channels\n        while channels % num_groups != 0 and num_groups > 1:\n            num_groups -= 1\n        \n        self.time_mlp = nn.Sequential(\n            nn.SiLU(),\n            nn.Linear(time_dim, channels)\n        )\n        \n        self.block1 = nn.Sequential(\n            nn.GroupNorm(num_groups, channels),\n            nn.SiLU(),\n            nn.Conv2d(channels, channels, 3, padding=1)\n        )\n        \n        self.block2 = nn.Sequential(\n            nn.GroupNorm(num_groups, channels),\n            nn.SiLU(),\n            nn.Dropout(dropout),\n            nn.Conv2d(channels, channels, 3, padding=1)\n        )\n        \n        # 时间嵌入投影\n        self.time_proj = nn.Linear(time_dim, channels)\n        \n    def forward(self, x, time_emb):\n        h = self.block1(x)\n        \n        # 时间嵌入处理\n        time_emb = self.time_proj(time_emb)\n        time_emb = time_emb.view(x.shape[0], -1, 1, 1)\n        h = h + time_emb\n        \n        h = self.block2(h)\n        return h + x\n\nclass ImprovedUNet2DConditionModel(nn.Module):\n    \"\"\"\n    改进的UNet实现，借鉴官方架构\n    \"\"\"\n    def __init__(self, in_channels=4, out_channels=4, model_channels=128, num_res_blocks=2, \n                 attention_resolutions=(), dropout=0.1, channel_mult=(1, 2, 4, 8), \n                 conv_resample=True, num_heads=8, context_dim=512):\n        super().__init__()\n        \n        self.in_channels = in_channels\n        self.model_channels = model_channels\n        self.num_res_blocks = num_res_blocks\n        self.attention_resolutions = attention_resolutions\n        self.dropout = dropout\n        self.channel_mult = channel_mult\n        self.conv_resample = conv_resample\n        self.num_heads = num_heads\n        self.context_dim = context_dim\n        \n        # 时间嵌入 - 使用更深的网络\n        time_embed_dim = model_channels * 4\n        self.time_embedding = nn.Sequential(\n            nn.Linear(1, time_embed_dim),\n            nn.SiLU(),\n            nn.Linear(time_embed_dim, time_embed_dim),\n            nn.SiLU(),\n            nn.Linear(time_embed_dim, time_embed_dim)\n        )\n        \n        # 输入投影\n        self.input_blocks = nn.ModuleList([\n            nn.Conv2d(in_channels, model_channels, kernel_size=3, padding=1)\n        ])\n        \n        # 下采样块\n        input_block_chans = [model_channels]\n        ch = model_channels\n        \n        for level, mult in enumerate(channel_mult):\n            # 添加ResBlock\n            for _ in range(num_res_blocks):\n                self.input_blocks.append(\n                    nn.ModuleList([ImprovedResBlock(ch, time_embed_dim, dropout)])\n                )\n                input_block_chans.append(ch)\n            \n            # 添加CrossAttention\n            if level in attention_resolutions:\n                self.input_blocks.append(\n                    nn.ModuleList([ImprovedCrossAttention(ch, context_dim, num_heads, dropout=dropout)])\n                )\n                input_block_chans.append(ch)\n            \n            # 下采样\n            if level < len(channel_mult) - 1:\n                ch = mult * model_channels\n                self.input_blocks.append(\n                    nn.ModuleList([nn.Conv2d(input_block_chans[-1], ch, 3, stride=2, padding=1)])\n                )\n                input_block_chans.append(ch)\n        \n        # 中间块\n        self.middle_block = nn.ModuleList([\n            ImprovedResBlock(ch, time_embed_dim, dropout),\n            ImprovedCrossAttention(ch, context_dim, num_heads, dropout=dropout),\n            ImprovedResBlock(ch, time_embed_dim, dropout)\n        ])\n        \n        # 输出块\n        self.output_blocks = nn.ModuleList([])\n        for level, mult in list(enumerate(channel_mult))[::-1]:\n            # 上采样\n            if level < len(channel_mult) - 1:\n                self.output_blocks.append(\n                    nn.ModuleList([nn.ConvTranspose2d(ch, ch//2, 4, stride=2, padding=1)])\n                )\n                ch = ch // 2\n            \n            # 添加ResBlock\n            for _ in range(num_res_blocks + 1):\n                self.output_blocks.append(\n                    nn.ModuleList([ImprovedResBlock(ch, time_embed_dim, dropout)])\n                )\n            \n            # 添加CrossAttention\n            if level in attention_resolutions:\n                self.output_blocks.append(\n                    nn.ModuleList([ImprovedCrossAttention(ch, context_dim, num_heads, dropout=dropout)])\n                )\n        \n        # 输出投影\n        if ch >= 32:\n            num_groups = 32\n        elif ch >= 16:\n            num_groups = 16\n        elif ch >= 8:\n            num_groups = 8\n        elif ch >= 4:\n            num_groups = 4\n        else:\n            num_groups = 1\n        \n        self.out = nn.Sequential(\n            nn.GroupNorm(num_groups, ch),\n            nn.SiLU(),\n            nn.Conv2d(ch, out_channels, kernel_size=3, padding=1)\n        )\n\n    \n    def forward(self, x, timesteps, context=None):\n        \"\"\"前向传播 - 完全禁用attention\"\"\"\n        # 时间嵌入\n        t = self.time_embedding(timesteps.unsqueeze(-1).float())\n        if t.dim() == 1:\n            t = t.unsqueeze(0)\n        \n        # 输入块\n        h = x\n        hs = []\n        \n        for module in self.input_blocks:\n            if isinstance(module, nn.ModuleList):\n                for submodule in module:\n                    if isinstance(submodule, ImprovedResBlock):\n                        h = submodule(h, t)\n                    elif hasattr(submodule, 'in_channels'):  # 卷积层\n                        h = submodule(h)\n                    # 完全跳过ImprovedCrossAttention\n            else:\n                h = module(h)\n            hs.append(h)\n        \n        # 中间块\n        for module in self.middle_block:\n            if isinstance(module, ImprovedResBlock):\n                h = module(h, t)\n            # 跳过attention\n        \n        # 输出块\n        for module in self.output_blocks:\n            if isinstance(module, nn.ModuleList):\n                for submodule in module:\n                    if isinstance(submodule, ImprovedResBlock):\n                        h = submodule(h, t)\n                    elif hasattr(submodule, 'in_channels'):  # 卷积层\n                        h = submodule(h)\n                    # 跳过attention\n            else:\n                h = module(h)\n            \n            # 跳跃连接\n            if hs:\n                h = torch.cat([h, hs.pop()], dim=1)\n        \n        return self.out(h)\n\n\nclass ImprovedDDPMScheduler:\n    \"\"\"\n    改进的DDPM调度器，修复设备不匹配问题\n    \"\"\"\n    def __init__(self, num_train_timesteps=1000, beta_start=0.0001, beta_end=0.02):\n        self.num_train_timesteps = num_train_timesteps\n        \n        # 线性噪声调度\n        self.betas = torch.linspace(beta_start, beta_end, num_train_timesteps)\n        self.alphas = 1.0 - self.betas\n        self.alphas_cumprod = torch.cumprod(self.alphas, dim=0)\n        self.alphas_cumprod_prev = torch.cat([torch.tensor([1.0]), self.alphas_cumprod[:-1]])\n        \n        # 计算噪声预测的系数\n        self.sqrt_alphas_cumprod = torch.sqrt(self.alphas_cumprod)\n        self.sqrt_one_minus_alphas_cumprod = torch.sqrt(1.0 - self.alphas_cumprod)\n    \n    def to_device(self, device):\n        \"\"\"将调度器移动到指定设备\"\"\"\n        self.betas = self.betas.to(device)\n        self.alphas = self.alphas.to(device)\n        self.alphas_cumprod = self.alphas_cumprod.to(device)\n        self.alphas_cumprod_prev = self.alphas_cumprod_prev.to(device)\n        self.sqrt_alphas_cumprod = self.sqrt_alphas_cumprod.to(device)\n        self.sqrt_one_minus_alphas_cumprod = self.sqrt_one_minus_alphas_cumprod.to(device)\n    \n    def add_noise(self, original_samples, noise, timesteps):\n        \"\"\"添加噪声到原始样本 - 修复设备不匹配\"\"\"\n        device = original_samples.device\n        \n        # 确保调度器系数在正确设备上\n        sqrt_alpha = self.sqrt_alphas_cumprod.to(device)[timesteps].view(-1, 1, 1, 1)\n        sqrt_one_minus_alpha = self.sqrt_one_minus_alphas_cumprod.to(device)[timesteps].view(-1, 1, 1, 1)\n        \n        return sqrt_alpha * original_samples + sqrt_one_minus_alpha * noise\n    \n    def step(self, model_output, timestep, sample):\n        \"\"\"去噪步骤 - 修复设备不匹配\"\"\"\n        device = sample.device\n        \n        alpha = self.alphas_cumprod.to(device)[timestep].view(-1, 1, 1, 1)\n        alpha_prev = self.alphas_cumprod_prev.to(device)[timestep].view(-1, 1, 1, 1)\n        \n        pred_original_sample = (sample - torch.sqrt(1 - alpha) * model_output) / torch.sqrt(alpha)\n        pred_sample_direction = torch.sqrt(1 - alpha_prev) * model_output\n        pred_prev_sample = torch.sqrt(alpha_prev) * pred_original_sample + pred_sample_direction\n        \n        return pred_prev_sample\n\nclass ImprovedStableDiffusionPipeline:\n    \"\"\"\n    改进的Stable Diffusion Pipeline，借鉴官方最佳实践\n    \"\"\"\n    def __init__(self, device='cuda' if torch.cuda.is_available() else 'cpu'):\n        self.device = device\n        \n        # 初始化组件\n        self.vae = ImprovedVAE().to(device)\n        self.unet = ImprovedUNet2DConditionModel(\n            in_channels=4,\n            out_channels=4,\n            model_channels=128,\n            channel_mult=(1, 2, 4, 8),\n            attention_resolutions=(8, 16),\n            context_dim=512\n        ).to(device)\n        self.scheduler = ImprovedDDPMScheduler()\n        \n        # CLIP文本编码器\n        self.tokenizer = CLIPTokenizer.from_pretrained(\"openai/clip-vit-base-patch32\")\n        self.text_encoder = CLIPTextModel.from_pretrained(\"openai/clip-vit-base-patch32\").to(device)\n        \n        # 设置为评估模式\n        self.text_encoder.eval()\n        self.vae.eval()\n        \n    def _encode_prompt(self, prompt):\n        \"\"\"编码文本提示\"\"\"\n        tokens = self.tokenizer(prompt, padding=True, return_tensors=\"pt\").to(self.device)\n        with torch.no_grad():\n            text_embeddings = self.text_encoder(**tokens).last_hidden_state\n        return text_embeddings\n    \n    def _parse_kanji_prompt(self, prompt):\n        \"\"\"解析汉字提示，使用更详细的描述\"\"\"\n        base_prompt = f\"kanji character representing {prompt}, traditional calligraphy style, black ink on white paper, high contrast, detailed strokes, clear lines, professional quality, artistic interpretation\"\n        return base_prompt\n    \n    def generate(self, prompt, height=128, width=128, num_inference_steps=50, \n                guidance_scale=7.5, seed=None):\n        \"\"\"生成图像，使用官方推荐的参数\"\"\"\n        \n        # 设置随机种子\n        if seed is not None:\n            torch.manual_seed(seed)\n            torch.cuda.manual_seed(seed)\n        \n        # 编码提示\n        text_embeddings = self._encode_prompt(self._parse_kanji_prompt(prompt))\n        \n        # 初始化潜在变量\n        latent_height = height // 8\n        latent_width = width // 8\n        latents = torch.randn(1, 4, latent_height, latent_width, device=self.device)\n        \n        # 设置时间步\n        timesteps = self.scheduler.set_timesteps(num_inference_steps)\n        timesteps = timesteps.to(self.device)\n        \n        # 改进的去噪循环\n        for i, t in enumerate(timesteps):\n            # 扩展潜在变量用于批处理\n            latent_model_input = torch.cat([latents] * 2)\n            t_expanded = t.expand(2)\n            \n            # 预测噪声\n            with torch.no_grad():\n                noise_pred = self.unet(latent_model_input, t_expanded, text_embeddings)\n            \n            # 执行classifier-free guidance\n            noise_pred_uncond, noise_pred_text = noise_pred.chunk(2)\n            \n            # 使用官方推荐的guidance scale\n            guidance_scale = torch.clamp(torch.tensor(guidance_scale), min=1.0, max=20.0)\n            noise_pred = noise_pred_uncond + guidance_scale * (noise_pred_text - noise_pred_uncond)\n            \n            # 计算前一个样本\n            latents = self.scheduler.step(noise_pred, t, latents)\n        \n        # 解码潜在变量\n        with torch.no_grad():\n            image = self.vae.decode(latents)\n        \n        return image\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-25T04:00:51.721836Z","iopub.execute_input":"2025-08-25T04:00:51.722139Z","iopub.status.idle":"2025-08-25T04:01:19.912024Z","shell.execute_reply.started":"2025-08-25T04:00:51.722115Z","shell.execute_reply":"2025-08-25T04:01:19.911404Z"}},"outputs":[{"name":"stderr","text":"2025-08-25 04:01:03.164661: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1756094463.485833      36 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1756094463.579864      36 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"from torch.amp import GradScaler, autocast  # 新的导入方式","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-25T04:01:39.205219Z","iopub.execute_input":"2025-08-25T04:01:39.206062Z","iopub.status.idle":"2025-08-25T04:01:39.209190Z","shell.execute_reply.started":"2025-08-25T04:01:39.206037Z","shell.execute_reply":"2025-08-25T04:01:39.208656Z"}},"outputs":[],"execution_count":10},{"cell_type":"markdown","source":"## 🏗️ colab_training.py Implementation","metadata":{}},{"cell_type":"code","source":"\"\"\"\nGoogle Colab优化的Stable Diffusion训练脚本\n专门为Colab GPU环境优化，包含自动检测和性能优化\n\"\"\"\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader\nimport torch.nn.functional as F\nfrom torch.cuda.amp import GradScaler, autocast\nimport os\nimport sys\nimport time\nfrom pathlib import Path\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom PIL import Image\nimport gc\n\n\nclass ColabOptimizedTrainer:\n    \"\"\"\n    Colab优化的训练器\n    \"\"\"\n    def __init__(self, device='auto'):\n        # 自动检测设备\n        if device == 'auto':\n            if torch.cuda.is_available():\n                self.device = 'cuda'\n                print(f\"🚀 检测到CUDA设备: {torch.cuda.get_device_name()}\")\n                print(f\"   • GPU内存: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n                print(f\"   • CUDA版本: {torch.version.cuda}\")\n            elif hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():\n                self.device = 'mps'\n                print(\"🍎 检测到Apple Silicon (MPS)\")\n            else:\n                self.device = 'cpu'\n                print(\"💻 使用CPU训练\")\n        else:\n            self.device = device\n        \n        # 初始化模型\n        self.vae = ImprovedVAE().to(self.device)\n        self.unet = ImprovedUNet2DConditionModel(\n            in_channels=4,\n            out_channels=4,\n            model_channels=128,\n            channel_mult=(1, 2, 4, 8),\n        ).to(self.device)\n        self.scheduler = ImprovedDDPMScheduler()\n        \n        # 优化器设置\n        self.optimizer = optim.AdamW([\n            {'params': self.vae.parameters(), 'lr': 1e-4},\n            {'params': self.unet.parameters(), 'lr': 1e-4}\n        ], weight_decay=0.01)\n        \n        # 学习率调度器\n        self.scheduler_lr = optim.lr_scheduler.CosineAnnealingLR(\n            self.optimizer, T_max=100, eta_min=1e-6\n        )\n        \n        # 混合精度训练\n        self.scaler = GradScaler()\n        \n        # 训练参数\n        self.num_epochs = 50\n        self.batch_size = 8  # Colab GPU内存优化\n        self.gradient_accumulation_steps = 4\n        self.save_every = 5\n        \n        # 损失函数\n        self.mse_loss = nn.MSELoss()\n        \n        print(f\"✅ 模型初始化完成，使用设备: {self.device}\")\n    \n    def create_synthetic_dataset(self, num_samples=1000):\n        \"\"\"\n        创建合成数据集用于演示\n        在实际使用中，这里应该加载真实的汉字数据\n        \"\"\"\n        print(f\"📊 创建合成数据集 ({num_samples} 样本)...\")\n        \n        # 创建128x128的合成图像\n        images = []\n        for i in range(num_samples):\n            # 创建简单的几何图案作为训练数据\n            img = np.zeros((128, 128, 3), dtype=np.float32)\n            \n            # 添加一些随机几何形状\n            if i % 4 == 0:\n                # 圆形\n                y, x = np.ogrid[:128, :128]\n                mask = (x - 64)**2 + (y - 64)**2 <= 30**2\n                img[mask] = [0.8, 0.8, 0.8]\n            elif i % 4 == 1:\n                # 矩形\n                img[40:88, 40:88] = [0.7, 0.7, 0.7]\n            elif i % 4 == 2:\n                # 三角形\n                for y in range(128):\n                    for x in range(128):\n                        if y >= 64 and abs(x - 64) <= (y - 64):\n                            img[y, x] = [0.6, 0.6, 0.6]\n            else:\n                # 随机噪声\n                img = np.random.rand(128, 128, 3).astype(np.float32) * 0.5\n            \n            # 归一化到[-1, 1]\n            img = (img - 0.5) * 2\n            images.append(img)\n        \n        # 转换为tensor\n        images = torch.tensor(images, dtype=torch.float32).permute(0, 3, 1, 2)\n        print(f\"✅ 数据集创建完成: {images.shape}\")\n        \n        return images\n    \n    def train_epoch(self, dataloader, epoch):\n        \"\"\"\n        训练一个epoch\n        \"\"\"\n        def train_epoch(self, dataloader, epoch):\n            \"\"\"\n            训练一个epoch - 修复UNet调用\n            \"\"\"\n        self.vae.train()\n        self.unet.train()\n    \n        total_loss = 0\n        num_batches = len(dataloader)\n    \n        for batch_idx, images in enumerate(dataloader):\n            images = images.to(self.device)\n            \n            with autocast():\n                # VAE编码\n                latents, mu, logvar, kl_loss = self.vae.encode(images)\n                \n                # 添加噪声\n                noise = torch.randn_like(latents, device=self.device)\n                timesteps = torch.randint(\n                    0, self.scheduler.num_train_timesteps, \n                    (latents.shape[0],), \n                    device=self.device\n                )\n                noisy_latents = self.scheduler.add_noise(latents, noise, timesteps)\n                \n                # UNet预测噪声 - 修复：不传递context参数\n                noise_pred = self.unet(noisy_latents, timesteps)  # 移除context参数\n                \n                # 计算损失\n                noise_loss = self.mse_loss(noise_pred, noise)\n                reconstruction_loss = self.mse_loss(self.vae.decode(latents), images)\n                \n                loss = noise_loss + 0.1 * kl_loss + 0.1 * reconstruction_loss\n                loss = loss / self.gradient_accumulation_steps\n    \n              \n            # 反向传播\n            self.scaler.scale(loss).backward()\n            \n            if (batch_idx + 1) % self.gradient_accumulation_steps == 0:\n                # 梯度裁剪\n                self.scaler.unscale_(self.optimizer)\n                torch.nn.utils.clip_grad_norm_(\n                    list(self.vae.parameters()) + list(self.unet.parameters()), \n                    max_norm=1.0\n                )\n                \n                # 优化器步进\n                self.scaler.step(self.optimizer)\n                self.scaler.update()\n                self.optimizer.zero_grad()\n            \n            total_loss += loss.item() * self.gradient_accumulation_steps\n            \n            # 进度显示\n            if (batch_idx + 1) % 10 == 0:\n                print(f\"   Epoch {epoch+1}/{self.num_epochs}, \"\n                      f\"Batch {batch_idx+1}/{num_batches}, \"\n                      f\"Loss: {loss.item():.6f}\")\n        \n        # 学习率调度\n        self.scheduler_lr.step()\n        \n        return total_loss / num_batches\n    \n    def save_checkpoint(self, epoch, loss, save_dir=\"colab_checkpoints\"):\n        \"\"\"\n        保存检查点\n        \"\"\"\n        os.makedirs(save_dir, exist_ok=True)\n        \n        checkpoint = {\n            'epoch': epoch,\n            'vae_state_dict': self.vae.state_dict(),\n            'unet_state_dict': self.unet.state_dict(),\n            'optimizer_state_dict': self.optimizer.state_dict(),\n            'scheduler_state_dict': self.scheduler_lr.state_dict(),\n            'loss': loss,\n            'device': self.device\n        }\n        \n        checkpoint_path = os.path.join(save_dir, f'checkpoint_epoch_{epoch+1}.pth')\n        torch.save(checkpoint, checkpoint_path)\n        print(f\"💾 检查点已保存: {checkpoint_path}\")\n        \n        # 保存最佳模型\n        if epoch == 0 or loss < getattr(self, 'best_loss', float('inf')):\n            self.best_loss = loss\n            best_model_path = os.path.join(save_dir, 'best_model.pth')\n            torch.save(checkpoint, best_model_path)\n            print(f\"🏆 最佳模型已保存: {best_model_path}\")\n    \n    def train(self):\n        \"\"\"\n        主训练循环\n        \"\"\"\n        print(f\"\\n🎯 开始训练...\")\n        print(f\"   • 设备: {self.device}\")\n        print(f\"   • 批次大小: {self.batch_size}\")\n        print(f\"   • 梯度累积步数: {self.gradient_accumulation_steps}\")\n        print(f\"   • 总epochs: {self.num_epochs}\")\n        print(f\"   • 混合精度: {'启用' if self.device == 'cuda' else '禁用'}\")\n        \n        # 创建数据集\n        images = self.create_synthetic_dataset()\n        dataloader = DataLoader(images, batch_size=self.batch_size, shuffle=True)\n        \n        # 训练历史\n        train_losses = []\n        start_time = time.time()\n        \n        try:\n            for epoch in range(self.num_epochs):\n                epoch_start = time.time()\n                \n                print(f\"\\n🔄 Epoch {epoch+1}/{self.num_epochs}\")\n                print(\"-\" * 50)\n                \n                # 训练\n                loss = self.train_epoch(dataloader, epoch)\n                train_losses.append(loss)\n                \n                epoch_time = time.time() - epoch_start\n                print(f\"   ⏱️  Epoch耗时: {epoch_time:.2f}秒\")\n                print(f\"   📊 平均损失: {loss:.6f}\")\n                print(f\"   📈 学习率: {self.optimizer.param_groups[0]['lr']:.2e}\")\n                \n                # 保存检查点\n                if (epoch + 1) % self.save_every == 0:\n                    self.save_checkpoint(epoch, loss)\n                \n                # 内存清理 (Colab优化)\n                if self.device == 'cuda':\n                    torch.cuda.empty_cache()\n                    gc.collect()\n                \n                # 显示GPU内存使用情况\n                if self.device == 'cuda':\n                    memory_allocated = torch.cuda.memory_allocated() / 1e9\n                    memory_reserved = torch.cuda.memory_reserved() / 1e9\n                    print(f\"   🧠 GPU内存: {memory_allocated:.2f}GB / {memory_reserved:.2f}GB\")\n        \n        except KeyboardInterrupt:\n            print(f\"\\n⚠️  训练被用户中断\")\n        except Exception as e:\n            print(f\"\\n❌ 训练出错: {e}\")\n            import traceback\n            traceback.print_exc()\n        \n        finally:\n            # 保存最终模型\n            final_loss = train_losses[-1] if train_losses else float('inf')\n            self.save_checkpoint(len(train_losses) - 1, final_loss)\n            \n            # 训练总结\n            total_time = time.time() - start_time\n            print(f\"\\n🎉 训练完成!\")\n            print(f\"   ⏱️  总耗时: {total_time:.2f}秒\")\n            print(f\"   📊 最终损失: {final_loss:.6f}\")\n            print(f\"   📈 损失变化: {train_losses[0]:.6f} → {final_loss:.6f}\")\n            \n            # 绘制损失曲线\n            self.plot_training_curve(train_losses)\n    \n    def plot_training_curve(self, losses):\n        \"\"\"\n        绘制训练损失曲线\n        \"\"\"\n        plt.figure(figsize=(10, 6))\n        plt.plot(losses, 'b-', linewidth=2, label='训练损失')\n        plt.title('Colab训练损失曲线', fontsize=16)\n        plt.xlabel('Epoch', fontsize=14)\n        plt.ylabel('损失', fontsize=14)\n        plt.grid(True, alpha=0.3)\n        plt.legend(fontsize=12)\n        plt.tight_layout()\n        \n        # 保存图片\n        plot_path = 'colab_training_curve.png'\n        plt.savefig(plot_path, dpi=150, bbox_inches='tight')\n        print(f\"📊 训练曲线已保存: {plot_path}\")\n        plt.show()\n    \n    def test_generation(self, prompt=\"water\"):\n        \"\"\"\n        测试生成功能\n        \"\"\"\n        print(f\"\\n🧪 测试生成: {prompt}\")\n        \n        try:\n            # 创建pipeline\n            pipeline = ImprovedStableDiffusionPipeline(device=self.device)\n            \n            # 加载训练好的权重\n            if hasattr(self, 'best_loss'):\n                checkpoint_path = 'colab_checkpoints/best_model.pth'\n                if os.path.exists(checkpoint_path):\n                    checkpoint = torch.load(checkpoint_path, map_location=self.device)\n                    pipeline.vae.load_state_dict(checkpoint['vae_state_dict'])\n                    pipeline.unet.load_state_dict(checkpoint['unet_state_dict'])\n                    print(f\"✅ 已加载最佳模型权重\")\n            \n            # 生成图像\n            print(f\"🌊 生成中...\")\n            result = pipeline.generate(\n                prompt,\n                height=128,\n                width=128,\n                num_inference_steps=50,\n                guidance_scale=7.5,\n                seed=42\n            )\n            \n            # 保存结果\n            if isinstance(result, torch.Tensor):\n                result = (result + 1) / 2\n                result = torch.clamp(result, 0, 1)\n                img_array = result.squeeze(0).permute(1, 2, 0).cpu().numpy()\n                pil_image = Image.fromarray((img_array * 255).astype(np.uint8))\n            else:\n                pil_image = result\n            \n            output_path = f'colab_generated_{prompt}.png'\n            pil_image.save(output_path)\n            print(f\"✅ 生成完成，已保存: {output_path}\")\n            \n            # 显示图像\n            plt.figure(figsize=(6, 6))\n            plt.imshow(pil_image, cmap='gray')\n            plt.title(f'Colab生成: {prompt}', fontsize=14)\n            plt.axis('off')\n            plt.show()\n            \n        except Exception as e:\n            print(f\"❌ 生成测试失败: {e}\")\n            import traceback\n            traceback.print_exc()\n\ndef main():\n    \"\"\"\n    主函数\n    \"\"\"\n    print(\"🚀 Google Colab优化的Stable Diffusion训练器\")\n    print(\"=\" * 60)\n    \n    # 检查Colab环境\n    is_colab = 'COLAB_GPU' in os.environ\n    if is_colab:\n        print(\"✅ 检测到Google Colab环境\")\n        print(f\"   • GPU类型: {os.environ.get('COLAB_GPU', 'Unknown')}\")\n        print(f\"   • 运行时类型: {os.environ.get('COLAB_RUNTIME_TYPE', 'Unknown')}\")\n    else:\n        print(\"💻 本地环境运行\")\n    \n    # 创建训练器\n    trainer = ColabOptimizedTrainer(device='auto')\n    \n    # 开始训练\n    trainer.train()\n    \n    # 测试生成\n    trainer.test_generation(\"water\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-25T04:01:41.786040Z","iopub.execute_input":"2025-08-25T04:01:41.786666Z","iopub.status.idle":"2025-08-25T04:01:41.821129Z","shell.execute_reply.started":"2025-08-25T04:01:41.786641Z","shell.execute_reply":"2025-08-25T04:01:41.820544Z"}},"outputs":[],"execution_count":11},{"cell_type":"markdown","source":"## 🚀 Start Training","metadata":{}},{"cell_type":"code","source":"# Create trainer and start training\nif 'ColabOptimizedTrainer' in globals():\n    trainer = ColabOptimizedTrainer()\n    trainer.train()\nelse:\n    print(\"⚠️ Trainer class not found. Please run the model implementation cells first.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-25T04:01:50.046539Z","iopub.execute_input":"2025-08-25T04:01:50.046912Z","iopub.status.idle":"2025-08-25T04:02:18.289921Z","shell.execute_reply.started":"2025-08-25T04:01:50.046886Z","shell.execute_reply":"2025-08-25T04:02:18.288918Z"}},"outputs":[{"name":"stdout","text":"🚀 检测到CUDA设备: Tesla T4\n   • GPU内存: 15.8 GB\n   • CUDA版本: 11.8\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_36/4175265272.py:65: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n  self.scaler = GradScaler()\n","output_type":"stream"},{"name":"stdout","text":"✅ 模型初始化完成，使用设备: cuda\n\n🎯 开始训练...\n   • 设备: cuda\n   • 批次大小: 8\n   • 梯度累积步数: 4\n   • 总epochs: 50\n   • 混合精度: 启用\n📊 创建合成数据集 (1000 样本)...\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_36/4175265272.py:115: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /pytorch/torch/csrc/utils/tensor_new.cpp:254.)\n  images = torch.tensor(images, dtype=torch.float32).permute(0, 3, 1, 2)\n/tmp/ipykernel_36/4175265272.py:137: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with autocast():\n","output_type":"stream"},{"name":"stdout","text":"✅ 数据集创建完成: torch.Size([1000, 3, 128, 128])\n\n🔄 Epoch 1/50\n--------------------------------------------------\n\n❌ 训练出错: Expected weight to be a vector of size equal to the number of channels in input, but got weight of shape [512] and input of shape [8, 1024, 1, 1]\n","output_type":"stream"},{"name":"stderr","text":"Traceback (most recent call last):\n  File \"/tmp/ipykernel_36/4175265272.py\", line 244, in train\n    loss = self.train_epoch(dataloader, epoch)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/tmp/ipykernel_36/4175265272.py\", line 151, in train_epoch\n    noise_pred = self.unet(noisy_latents, timesteps)  # 移除context参数\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1751, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1762, in _call_impl\n    return forward_call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/tmp/ipykernel_36/439328512.py\", line 357, in forward\n    h = submodule(h, t)\n        ^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1751, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1762, in _call_impl\n    return forward_call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/tmp/ipykernel_36/439328512.py\", line 204, in forward\n    h = self.block1(x)\n        ^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1751, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1762, in _call_impl\n    return forward_call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/container.py\", line 240, in forward\n    input = module(input)\n            ^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1751, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1762, in _call_impl\n    return forward_call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/normalization.py\", line 313, in forward\n    return F.group_norm(input, self.num_groups, self.weight, self.bias, self.eps)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/functional.py\", line 2965, in group_norm\n    return torch.group_norm(\n           ^^^^^^^^^^^^^^^^^\nRuntimeError: Expected weight to be a vector of size equal to the number of channels in input, but got weight of shape [512] and input of shape [8, 1024, 1, 1]\n","output_type":"stream"},{"name":"stdout","text":"💾 检查点已保存: colab_checkpoints/checkpoint_epoch_0.pth\n\n🎉 训练完成!\n   ⏱️  总耗时: 0.81秒\n   📊 最终损失: inf\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_36/1663092989.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;34m'ColabOptimizedTrainer'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mglobals\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mtrainer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mColabOptimizedTrainer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"⚠️ Trainer class not found. Please run the model implementation cells first.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_36/4175265272.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    282\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"   ⏱️  总耗时: {total_time:.2f}秒\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    283\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"   📊 最终损失: {final_loss:.6f}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 284\u001b[0;31m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"   📈 损失变化: {train_losses[0]:.6f} → {final_loss:.6f}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    285\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    286\u001b[0m             \u001b[0;31m# 绘制损失曲线\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mIndexError\u001b[0m: list index out of range"],"ename":"IndexError","evalue":"list index out of range","output_type":"error"}],"execution_count":12},{"cell_type":"markdown","source":"## 📥 Download Results","metadata":{}},{"cell_type":"code","source":"# Download training results\nfrom google.colab import files\nimport zipfile\n\ndef download_results():\n    print(\"📥 Preparing results for download...\")\n    \n    # Create results zip\n    with zipfile.ZipFile('training_results.zip', 'w') as zipf:\n        # Add checkpoints\n        if os.path.exists('checkpoints'):\n            for root, dirs, files in os.walk('checkpoints'):\n                for file in files:\n                    file_path = os.path.join(root, file)\n                    zipf.write(file_path, os.path.relpath(file_path, '.'))\n        \n        # Add training curves\n        for img_file in ['training_curve.png', 'loss_curve.png']:\n            if os.path.exists(img_file):\n                zipf.write(img_file)\n        \n        # Add generated images\n        for i in range(10):\n            img_file = f'generated_{i}.png'\n            if os.path.exists(img_file):\n                zipf.write(img_file)\n    \n    print(\"✅ Results packaged: training_results.zip\")\n    \n    # Download\n    try:\n        files.download('training_results.zip')\n        print(\"📥 Results downloaded successfully!\")\n    except:\n        print(\"⚠️ Download failed (not in Colab)\")\n        print(\"📁 Files are saved in the current directory\")\n\n# Download results\ndownload_results()","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}