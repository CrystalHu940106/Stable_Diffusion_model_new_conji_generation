{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ğŸš€ Complete Stable Diffusion Kanji Generation - Colab/Kaggle\n",
    "\n",
    "**Single file training notebook** - Upload to Colab/Kaggle and start training immediately!\n",
    "\n",
    "## ğŸ¯ Features\n",
    "- âœ… **Complete Training Pipeline**: VAE + UNet + DDPM\n",
    "- ğŸš€ **GPU Optimized**: Auto CUDA/MPS detection\n",
    "- ğŸ’¾ **Auto-save**: Checkpoints every 5 epochs\n",
    "- ğŸ“Š **Real-time Monitoring**: Loss curves and GPU stats\n",
    "- ğŸ”„ **Resume Training**: Continue from any checkpoint\n",
    "- ğŸŒ **Kanji Generation**: Text-to-Kanji capabilities\n",
    "\n",
    "## ğŸš€ Quick Start\n",
    "1. Upload this notebook to Colab/Kaggle\n",
    "2. Select GPU runtime\n",
    "3. Run all cells\n",
    "4. Start training!\n",
    "\n",
    "**Expected Training Time**:\n",
    "- Colab Free (T4): 50 epochs in 2-3 hours\n",
    "- Colab Pro (V100/P100): 50 epochs in 1-1.5 hours\n",
    "- Kaggle (P100): 50 epochs in 1-2 hours"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ“¦ Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install transformers pillow matplotlib scikit-image opencv-python tqdm\n",
    "!pip install --upgrade torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
    "\n",
    "print(\"âœ… Dependencies installed successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ”§ Check GPU and Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "\n",
    "# Check environment\n",
    "is_colab = 'COLAB_GPU' in os.environ\n",
    "is_kaggle = 'KAGGLE_KERNEL_RUN_TYPE' in os.environ\n",
    "\n",
    "print(f\"ğŸŒ Environment: {'Colab' if is_colab else 'Kaggle' if is_kaggle else 'Local'}\")\n",
    "print(f\"PyTorch: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "elif hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():\n",
    "    print(\"ğŸ Apple Silicon (MPS) available\")\n",
    "else:\n",
    "    print(\"âš ï¸ Using CPU (will be slow!)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ—ï¸ improved_stable_diffusion.py Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "æ”¹è¿›çš„Stable Diffusionå®ç°\n",
    "å€Ÿé‰´å®˜æ–¹CompVis/stable-diffusionçš„æœ€ä½³å®è·µ\n",
    "\"\"\"\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from transformers import CLIPTokenizer, CLIPTextModel\n",
    "import math\n",
    "from typing import Optional, Union, Tuple\n",
    "import numpy as np\n",
    "\n",
    "class ImprovedVAE(nn.Module):\n",
    "    \"\"\"\n",
    "    æ”¹è¿›çš„VAEå®ç°ï¼Œå€Ÿé‰´å®˜æ–¹æ¶æ„\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels=3, latent_channels=4, hidden_dims=[128, 256, 512, 1024]):\n",
    "        super().__init__()\n",
    "        self.latent_channels = latent_channels\n",
    "        \n",
    "        # Encoder - ä½¿ç”¨æ›´æ·±çš„ç½‘ç»œ\n",
    "        encoder_layers = []\n",
    "        in_ch = in_channels\n",
    "        for h_dim in hidden_dims:\n",
    "            # è®¡ç®—åˆé€‚çš„GroupNormç»„æ•°\n",
    "            num_groups = min(32, h_dim)\n",
    "            while h_dim % num_groups != 0 and num_groups > 1:\n",
    "                num_groups -= 1\n",
    "            \n",
    "            encoder_layers.extend([\n",
    "                nn.Conv2d(in_ch, h_dim, kernel_size=3, stride=2, padding=1),\n",
    "                nn.GroupNorm(num_groups, h_dim),  # ä½¿ç”¨GroupNormæ›¿ä»£BatchNorm\n",
    "                nn.SiLU()  # ä½¿ç”¨SiLUæ›¿ä»£LeakyReLU\n",
    "            ])\n",
    "            in_ch = h_dim\n",
    "        \n",
    "        # Final encoding layer\n",
    "        final_channels = latent_channels * 2\n",
    "        num_groups = min(32, final_channels)\n",
    "        while final_channels % num_groups != 0 and num_groups > 1:\n",
    "            num_groups -= 1\n",
    "        \n",
    "        encoder_layers.extend([\n",
    "            nn.Conv2d(hidden_dims[-1], final_channels, kernel_size=3, padding=1),\n",
    "            nn.GroupNorm(num_groups, final_channels)\n",
    "        ])\n",
    "        \n",
    "        self.encoder = nn.Sequential(*encoder_layers)\n",
    "        \n",
    "        # Decoder - ç¡®ä¿ç²¾ç¡®çš„128x128è¾“å‡º\n",
    "        decoder_layers = []\n",
    "        in_ch = latent_channels\n",
    "        \n",
    "        # ä½¿ç”¨hidden_dimsçš„ååºè¿›è¡Œä¸Šé‡‡æ ·\n",
    "        hidden_dims_rev = hidden_dims[::-1]\n",
    "        \n",
    "        for i, h_dim in enumerate(hidden_dims_rev):\n",
    "            # è®¡ç®—åˆé€‚çš„GroupNormç»„æ•°\n",
    "            num_groups = min(32, h_dim)\n",
    "            while h_dim % num_groups != 0 and num_groups > 1:\n",
    "                num_groups -= 1\n",
    "            \n",
    "            decoder_layers.extend([\n",
    "                nn.ConvTranspose2d(in_ch, h_dim, kernel_size=4, stride=2, padding=1),\n",
    "                nn.GroupNorm(num_groups, h_dim),\n",
    "                nn.SiLU()\n",
    "            ])\n",
    "            in_ch = h_dim\n",
    "        \n",
    "        # æœ€ç»ˆè¾“å‡ºå±‚\n",
    "        decoder_layers.extend([\n",
    "            nn.Conv2d(in_ch, in_channels, kernel_size=3, stride=1, padding=1),\n",
    "            nn.Tanh()\n",
    "        ])\n",
    "        \n",
    "        self.decoder = nn.Sequential(*decoder_layers)\n",
    "        \n",
    "    def encode(self, x):\n",
    "        # ç¡®ä¿è¾“å…¥æ˜¯128x128\n",
    "        if x.shape[-1] != 128:\n",
    "            x = F.interpolate(x, size=(128, 128), mode='bilinear', align_corners=False)\n",
    "        \n",
    "        # ç¼–ç åˆ°æ½œåœ¨ç©ºé—´\n",
    "        encoded = self.encoder(x)\n",
    "        mu, logvar = torch.chunk(encoded, 2, dim=1)\n",
    "        \n",
    "        # KLæ•£åº¦æŸå¤±\n",
    "        kl_loss = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "        \n",
    "        # é‡å‚æ•°åŒ–æŠ€å·§\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        z = mu + eps * std\n",
    "        \n",
    "        return z, mu, logvar, kl_loss\n",
    "    \n",
    "    def decode(self, z):\n",
    "        return self.decoder(z)\n",
    "\n",
    "class ImprovedCrossAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    æ”¹è¿›çš„äº¤å‰æ³¨æ„åŠ›å®ç°ï¼Œå€Ÿé‰´å®˜æ–¹ç‰ˆæœ¬\n",
    "    \"\"\"\n",
    "    def __init__(self, query_dim, context_dim=None, heads=8, dim_head=64, dropout=0.0):\n",
    "        super().__init__()\n",
    "        inner_dim = dim_head * heads\n",
    "        context_dim = context_dim if context_dim is not None else query_dim\n",
    "        \n",
    "        self.scale = dim_head ** -0.5\n",
    "        self.heads = heads\n",
    "        \n",
    "        self.to_q = nn.Linear(query_dim, inner_dim, bias=False)\n",
    "        self.to_k = nn.Linear(context_dim, inner_dim, bias=False)\n",
    "        self.to_v = nn.Linear(context_dim, inner_dim, bias=False)\n",
    "        \n",
    "        self.to_out = nn.Sequential(\n",
    "            nn.Linear(inner_dim, query_dim),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x, context=None):\n",
    "        context = context if context is not None else x\n",
    "        \n",
    "        q = self.to_q(x)\n",
    "        k = self.to_k(context)\n",
    "        v = self.to_v(context)\n",
    "        \n",
    "        # é‡å¡‘ä¸ºå¤šå¤´æ³¨æ„åŠ›\n",
    "        q = q.view(q.shape[0], -1, self.heads, q.shape[-1] // self.heads).transpose(1, 2)\n",
    "        k = k.view(k.shape[0], -1, self.heads, k.shape[-1] // self.heads).transpose(1, 2)\n",
    "        v = v.view(v.shape[0], -1, self.heads, v.shape[-1] // self.heads).transpose(1, 2)\n",
    "        \n",
    "        # è®¡ç®—æ³¨æ„åŠ›\n",
    "        scores = torch.matmul(q, k.transpose(-2, -1)) * self.scale\n",
    "        attn = F.softmax(scores, dim=-1)\n",
    "        \n",
    "        # åº”ç”¨æ³¨æ„åŠ›\n",
    "        out = torch.matmul(attn, v)\n",
    "        out = out.transpose(1, 2).contiguous().view(out.shape[0], -1, out.shape[-1] * self.heads)\n",
    "        \n",
    "        return self.to_out(out)\n",
    "\n",
    "class ImprovedResBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    æ”¹è¿›çš„æ®‹å·®å—ï¼Œå€Ÿé‰´å®˜æ–¹å®ç°\n",
    "    \"\"\"\n",
    "    def __init__(self, channels, time_dim, dropout=0.0):\n",
    "        super().__init__()\n",
    "        \n",
    "        # åŠ¨æ€è®¡ç®—GroupNormçš„ç»„æ•°ï¼Œç¡®ä¿channelsèƒ½è¢«num_groupsæ•´é™¤\n",
    "        if channels >= 32:\n",
    "            num_groups = min(32, channels // (channels // 32))\n",
    "        elif channels >= 16:\n",
    "            num_groups = min(16, channels // (channels // 16))\n",
    "        elif channels >= 8:\n",
    "            num_groups = min(8, channels // (channels // 8))\n",
    "        elif channels >= 4:\n",
    "            num_groups = min(4, channels // (channels // 4))\n",
    "        else:\n",
    "            num_groups = 1\n",
    "        \n",
    "        # ç¡®ä¿num_groupsèƒ½æ•´é™¤channels\n",
    "        while channels % num_groups != 0 and num_groups > 1:\n",
    "            num_groups -= 1\n",
    "        \n",
    "        self.time_mlp = nn.Sequential(\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(time_dim, channels)\n",
    "        )\n",
    "        \n",
    "        self.block1 = nn.Sequential(\n",
    "            nn.GroupNorm(num_groups, channels),\n",
    "            nn.SiLU(),\n",
    "            nn.Conv2d(channels, channels, 3, padding=1)\n",
    "        )\n",
    "        \n",
    "        self.block2 = nn.Sequential(\n",
    "            nn.GroupNorm(num_groups, channels),\n",
    "            nn.SiLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Conv2d(channels, channels, 3, padding=1)\n",
    "        )\n",
    "        \n",
    "        # æ—¶é—´åµŒå…¥æŠ•å½±\n",
    "        self.time_proj = nn.Linear(time_dim, channels)\n",
    "        \n",
    "    def forward(self, x, time_emb):\n",
    "        h = self.block1(x)\n",
    "        \n",
    "        # æ—¶é—´åµŒå…¥å¤„ç†\n",
    "        time_emb = self.time_proj(time_emb)\n",
    "        time_emb = time_emb.view(x.shape[0], -1, 1, 1)\n",
    "        h = h + time_emb\n",
    "        \n",
    "        h = self.block2(h)\n",
    "        return h + x\n",
    "\n",
    "class ImprovedUNet2DConditionModel(nn.Module):\n",
    "    \"\"\"\n",
    "    æ”¹è¿›çš„UNetå®ç°ï¼Œå€Ÿé‰´å®˜æ–¹æ¶æ„\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels=4, out_channels=4, model_channels=128, num_res_blocks=2, \n",
    "                 attention_resolutions=(8, 16), dropout=0.1, channel_mult=(1, 2, 4, 8), \n",
    "                 conv_resample=True, num_heads=8, context_dim=512):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.in_channels = in_channels\n",
    "        self.model_channels = model_channels\n",
    "        self.num_res_blocks = num_res_blocks\n",
    "        self.attention_resolutions = attention_resolutions\n",
    "        self.dropout = dropout\n",
    "        self.channel_mult = channel_mult\n",
    "        self.conv_resample = conv_resample\n",
    "        self.num_heads = num_heads\n",
    "        self.context_dim = context_dim\n",
    "        \n",
    "        # æ—¶é—´åµŒå…¥ - ä½¿ç”¨æ›´æ·±çš„ç½‘ç»œ\n",
    "        time_embed_dim = model_channels * 4\n",
    "        self.time_embedding = nn.Sequential(\n",
    "            nn.Linear(1, time_embed_dim),\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(time_embed_dim, time_embed_dim),\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(time_embed_dim, time_embed_dim)\n",
    "        )\n",
    "        \n",
    "        # è¾“å…¥æŠ•å½±\n",
    "        self.input_blocks = nn.ModuleList([\n",
    "            nn.Conv2d(in_channels, model_channels, kernel_size=3, padding=1)\n",
    "        ])\n",
    "        \n",
    "        # ä¸‹é‡‡æ ·å—\n",
    "        input_block_chans = [model_channels]\n",
    "        ch = model_channels\n",
    "        \n",
    "        for level, mult in enumerate(channel_mult):\n",
    "            # æ·»åŠ ResBlock\n",
    "            for _ in range(num_res_blocks):\n",
    "                self.input_blocks.append(\n",
    "                    nn.ModuleList([ImprovedResBlock(ch, time_embed_dim, dropout)])\n",
    "                )\n",
    "                input_block_chans.append(ch)\n",
    "            \n",
    "            # æ·»åŠ CrossAttention\n",
    "            if level in attention_resolutions:\n",
    "                self.input_blocks.append(\n",
    "                    nn.ModuleList([ImprovedCrossAttention(ch, context_dim, num_heads, dropout=dropout)])\n",
    "                )\n",
    "                input_block_chans.append(ch)\n",
    "            \n",
    "            # ä¸‹é‡‡æ ·\n",
    "            if level < len(channel_mult) - 1:\n",
    "                ch = mult * model_channels\n",
    "                self.input_blocks.append(\n",
    "                    nn.ModuleList([nn.Conv2d(input_block_chans[-1], ch, 3, stride=2, padding=1)])\n",
    "                )\n",
    "                input_block_chans.append(ch)\n",
    "        \n",
    "        # ä¸­é—´å—\n",
    "        self.middle_block = nn.ModuleList([\n",
    "            ImprovedResBlock(ch, time_embed_dim, dropout),\n",
    "            ImprovedCrossAttention(ch, context_dim, num_heads, dropout=dropout),\n",
    "            ImprovedResBlock(ch, time_embed_dim, dropout)\n",
    "        ])\n",
    "        \n",
    "        # è¾“å‡ºå—\n",
    "        self.output_blocks = nn.ModuleList([])\n",
    "        for level, mult in list(enumerate(channel_mult))[::-1]:\n",
    "            # ä¸Šé‡‡æ ·\n",
    "            if level < len(channel_mult) - 1:\n",
    "                self.output_blocks.append(\n",
    "                    nn.ModuleList([nn.ConvTranspose2d(ch, ch//2, 4, stride=2, padding=1)])\n",
    "                )\n",
    "                ch = ch // 2\n",
    "            \n",
    "            # æ·»åŠ ResBlock\n",
    "            for _ in range(num_res_blocks + 1):\n",
    "                self.output_blocks.append(\n",
    "                    nn.ModuleList([ImprovedResBlock(ch, time_embed_dim, dropout)])\n",
    "                )\n",
    "            \n",
    "            # æ·»åŠ CrossAttention\n",
    "            if level in attention_resolutions:\n",
    "                self.output_blocks.append(\n",
    "                    nn.ModuleList([ImprovedCrossAttention(ch, context_dim, num_heads, dropout=dropout)])\n",
    "                )\n",
    "        \n",
    "        # è¾“å‡ºæŠ•å½±\n",
    "        if ch >= 32:\n",
    "            num_groups = 32\n",
    "        elif ch >= 16:\n",
    "            num_groups = 16\n",
    "        elif ch >= 8:\n",
    "            num_groups = 8\n",
    "        elif ch >= 4:\n",
    "            num_groups = 4\n",
    "        else:\n",
    "            num_groups = 1\n",
    "        \n",
    "        self.out = nn.Sequential(\n",
    "            nn.GroupNorm(num_groups, ch),\n",
    "            nn.SiLU(),\n",
    "            nn.Conv2d(ch, out_channels, kernel_size=3, padding=1)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x, timesteps, context=None):\n",
    "        # æ—¶é—´åµŒå…¥\n",
    "        t = self.time_embedding(timesteps.unsqueeze(-1).float())\n",
    "        if t.dim() == 1:\n",
    "            t = t.unsqueeze(0)\n",
    "        \n",
    "        # è¾“å…¥å—\n",
    "        h = x\n",
    "        hs = []\n",
    "        for module in self.input_blocks:\n",
    "            if isinstance(module, nn.ModuleList):\n",
    "                # å¤„ç†ModuleListä¸­çš„æ¨¡å—\n",
    "                for submodule in module:\n",
    "                    if isinstance(submodule, ImprovedCrossAttention):\n",
    "                        h = submodule(h, context)\n",
    "                    elif isinstance(submodule, ImprovedResBlock):\n",
    "                        h = submodule(h, t)\n",
    "                    else:\n",
    "                        h = submodule(h)\n",
    "            else:\n",
    "                # ç›´æ¥å¤„ç†å•ä¸ªæ¨¡å—\n",
    "                if isinstance(module, ImprovedCrossAttention):\n",
    "                    h = module(h, context)\n",
    "                elif isinstance(module, ImprovedResBlock):\n",
    "                    h = module(h, t)\n",
    "                else:\n",
    "                    h = module(h)\n",
    "            hs.append(h)\n",
    "        \n",
    "        # ä¸­é—´å—\n",
    "        for module in self.middle_block:\n",
    "            if isinstance(module, ImprovedCrossAttention):\n",
    "                h = module(h, context)\n",
    "            else:\n",
    "                h = module(h, t)\n",
    "        \n",
    "        # è¾“å‡ºå—\n",
    "        for module in self.output_blocks:\n",
    "            if isinstance(module, nn.ModuleList):\n",
    "                # å¤„ç†ModuleListä¸­çš„æ¨¡å—\n",
    "                for submodule in module:\n",
    "                    if isinstance(submodule, ImprovedCrossAttention):\n",
    "                        h = submodule(h, context)\n",
    "                    elif isinstance(submodule, ImprovedResBlock):\n",
    "                        h = submodule(h, t)\n",
    "                    else:\n",
    "                        h = submodule(h)\n",
    "            else:\n",
    "                # ç›´æ¥å¤„ç†å•ä¸ªæ¨¡å—\n",
    "                if isinstance(module, ImprovedCrossAttention):\n",
    "                    h = module(h, context)\n",
    "                elif isinstance(module, ImprovedResBlock):\n",
    "                    h = module(h, t)\n",
    "                else:\n",
    "                    h = module(h)\n",
    "            \n",
    "            # è·³è·ƒè¿æ¥\n",
    "            if hs:\n",
    "                h = torch.cat([h, hs.pop()], dim=1)\n",
    "        \n",
    "        return self.out(h)\n",
    "\n",
    "class ImprovedDDPMScheduler:\n",
    "    \"\"\"\n",
    "    æ”¹è¿›çš„DDPMè°ƒåº¦å™¨ï¼Œå€Ÿé‰´å®˜æ–¹å®ç°\n",
    "    \"\"\"\n",
    "    def __init__(self, num_train_timesteps=1000, beta_start=0.0001, beta_end=0.02):\n",
    "        self.num_train_timesteps = num_train_timesteps\n",
    "        \n",
    "        # çº¿æ€§å™ªå£°è°ƒåº¦\n",
    "        self.betas = torch.linspace(beta_start, beta_end, num_train_timesteps)\n",
    "        self.alphas = 1.0 - self.betas\n",
    "        self.alphas_cumprod = torch.cumprod(self.alphas, dim=0)\n",
    "        self.alphas_cumprod_prev = torch.cat([torch.tensor([1.0]), self.alphas_cumprod[:-1]])\n",
    "        \n",
    "        # è®¡ç®—å™ªå£°é¢„æµ‹çš„ç³»æ•°\n",
    "        self.sqrt_alphas_cumprod = torch.sqrt(self.alphas_cumprod)\n",
    "        self.sqrt_one_minus_alphas_cumprod = torch.sqrt(1.0 - self.alphas_cumprod)\n",
    "        \n",
    "    def add_noise(self, original_samples, noise, timesteps):\n",
    "        \"\"\"æ·»åŠ å™ªå£°åˆ°åŸå§‹æ ·æœ¬\"\"\"\n",
    "        sqrt_alpha = self.sqrt_alphas_cumprod[timesteps].view(-1, 1, 1, 1)\n",
    "        sqrt_one_minus_alpha = self.sqrt_one_minus_alphas_cumprod[timesteps].view(-1, 1, 1, 1)\n",
    "        \n",
    "        return sqrt_alpha * original_samples + sqrt_one_minus_alpha * noise\n",
    "    \n",
    "    def step(self, model_output, timestep, sample):\n",
    "        \"\"\"å»å™ªæ­¥éª¤\"\"\"\n",
    "        alpha = self.alphas_cumprod[timestep].view(-1, 1, 1, 1)\n",
    "        alpha_prev = self.alphas_cumprod_prev[timestep].view(-1, 1, 1, 1)\n",
    "        \n",
    "        # é¢„æµ‹x0\n",
    "        pred_original_sample = (sample - torch.sqrt(1 - alpha) * model_output) / torch.sqrt(alpha)\n",
    "        \n",
    "        # é¢„æµ‹å‰ä¸€ä¸ªæ ·æœ¬\n",
    "        pred_sample_direction = torch.sqrt(1 - alpha_prev) * model_output\n",
    "        pred_prev_sample = torch.sqrt(alpha_prev) * pred_original_sample + pred_sample_direction\n",
    "        \n",
    "        return pred_prev_sample\n",
    "    \n",
    "    def set_timesteps(self, num_inference_steps):\n",
    "        \"\"\"è®¾ç½®æ¨ç†æ—¶é—´æ­¥\"\"\"\n",
    "        self.num_inference_steps = num_inference_steps\n",
    "        step_ratio = self.num_train_timesteps // num_inference_steps\n",
    "        timesteps = (torch.arange(0, num_inference_steps) * step_ratio).flip(0)\n",
    "        return timesteps\n",
    "\n",
    "class ImprovedStableDiffusionPipeline:\n",
    "    \"\"\"\n",
    "    æ”¹è¿›çš„Stable Diffusion Pipelineï¼Œå€Ÿé‰´å®˜æ–¹æœ€ä½³å®è·µ\n",
    "    \"\"\"\n",
    "    def __init__(self, device='cuda' if torch.cuda.is_available() else 'cpu'):\n",
    "        self.device = device\n",
    "        \n",
    "        # åˆå§‹åŒ–ç»„ä»¶\n",
    "        self.vae = ImprovedVAE().to(device)\n",
    "        self.unet = ImprovedUNet2DConditionModel(\n",
    "            in_channels=4,\n",
    "            out_channels=4,\n",
    "            model_channels=128,\n",
    "            channel_mult=(1, 2, 4, 8),\n",
    "            attention_resolutions=(8, 16),\n",
    "            context_dim=512\n",
    "        ).to(device)\n",
    "        self.scheduler = ImprovedDDPMScheduler()\n",
    "        \n",
    "        # CLIPæ–‡æœ¬ç¼–ç å™¨\n",
    "        self.tokenizer = CLIPTokenizer.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "        self.text_encoder = CLIPTextModel.from_pretrained(\"openai/clip-vit-base-patch32\").to(device)\n",
    "        \n",
    "        # è®¾ç½®ä¸ºè¯„ä¼°æ¨¡å¼\n",
    "        self.text_encoder.eval()\n",
    "        self.vae.eval()\n",
    "        \n",
    "    def _encode_prompt(self, prompt):\n",
    "        \"\"\"ç¼–ç æ–‡æœ¬æç¤º\"\"\"\n",
    "        tokens = self.tokenizer(prompt, padding=True, return_tensors=\"pt\").to(self.device)\n",
    "        with torch.no_grad():\n",
    "            text_embeddings = self.text_encoder(**tokens).last_hidden_state\n",
    "        return text_embeddings\n",
    "    \n",
    "    def _parse_kanji_prompt(self, prompt):\n",
    "        \"\"\"è§£ææ±‰å­—æç¤ºï¼Œä½¿ç”¨æ›´è¯¦ç»†çš„æè¿°\"\"\"\n",
    "        base_prompt = f\"kanji character representing {prompt}, traditional calligraphy style, black ink on white paper, high contrast, detailed strokes, clear lines, professional quality, artistic interpretation\"\n",
    "        return base_prompt\n",
    "    \n",
    "    def generate(self, prompt, height=128, width=128, num_inference_steps=50, \n",
    "                guidance_scale=7.5, seed=None):\n",
    "        \"\"\"ç”Ÿæˆå›¾åƒï¼Œä½¿ç”¨å®˜æ–¹æ¨èçš„å‚æ•°\"\"\"\n",
    "        \n",
    "        # è®¾ç½®éšæœºç§å­\n",
    "        if seed is not None:\n",
    "            torch.manual_seed(seed)\n",
    "            torch.cuda.manual_seed(seed)\n",
    "        \n",
    "        # ç¼–ç æç¤º\n",
    "        text_embeddings = self._encode_prompt(self._parse_kanji_prompt(prompt))\n",
    "        \n",
    "        # åˆå§‹åŒ–æ½œåœ¨å˜é‡\n",
    "        latent_height = height // 8\n",
    "        latent_width = width // 8\n",
    "        latents = torch.randn(1, 4, latent_height, latent_width, device=self.device)\n",
    "        \n",
    "        # è®¾ç½®æ—¶é—´æ­¥\n",
    "        timesteps = self.scheduler.set_timesteps(num_inference_steps)\n",
    "        timesteps = timesteps.to(self.device)\n",
    "        \n",
    "        # æ”¹è¿›çš„å»å™ªå¾ªç¯\n",
    "        for i, t in enumerate(timesteps):\n",
    "            # æ‰©å±•æ½œåœ¨å˜é‡ç”¨äºæ‰¹å¤„ç†\n",
    "            latent_model_input = torch.cat([latents] * 2)\n",
    "            t_expanded = t.expand(2)\n",
    "            \n",
    "            # é¢„æµ‹å™ªå£°\n",
    "            with torch.no_grad():\n",
    "                noise_pred = self.unet(latent_model_input, t_expanded, text_embeddings)\n",
    "            \n",
    "            # æ‰§è¡Œclassifier-free guidance\n",
    "            noise_pred_uncond, noise_pred_text = noise_pred.chunk(2)\n",
    "            \n",
    "            # ä½¿ç”¨å®˜æ–¹æ¨èçš„guidance scale\n",
    "            guidance_scale = torch.clamp(torch.tensor(guidance_scale), min=1.0, max=20.0)\n",
    "            noise_pred = noise_pred_uncond + guidance_scale * (noise_pred_text - noise_pred_uncond)\n",
    "            \n",
    "            # è®¡ç®—å‰ä¸€ä¸ªæ ·æœ¬\n",
    "            latents = self.scheduler.step(noise_pred, t, latents)\n",
    "        \n",
    "        # è§£ç æ½œåœ¨å˜é‡\n",
    "        with torch.no_grad():\n",
    "            image = self.vae.decode(latents)\n",
    "        \n",
    "        return image\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ—ï¸ colab_training.py Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Google Colabä¼˜åŒ–çš„Stable Diffusionè®­ç»ƒè„šæœ¬\n",
    "ä¸“é—¨ä¸ºColab GPUç¯å¢ƒä¼˜åŒ–ï¼ŒåŒ…å«è‡ªåŠ¨æ£€æµ‹å’Œæ€§èƒ½ä¼˜åŒ–\n",
    "\"\"\"\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import gc\n",
    "\n",
    "# æ·»åŠ å½“å‰ç›®å½•åˆ°è·¯å¾„\n",
    "sys.path.append(os.path.dirname(os.path.abspath(__file__)))\n",
    "\n",
    "from improved_stable_diffusion import (\n",
    "    ImprovedStableDiffusionPipeline,\n",
    "    ImprovedVAE,\n",
    "    ImprovedUNet2DConditionModel,\n",
    "    ImprovedDDPMScheduler\n",
    ")\n",
    "\n",
    "class ColabOptimizedTrainer:\n",
    "    \"\"\"\n",
    "    Colabä¼˜åŒ–çš„è®­ç»ƒå™¨\n",
    "    \"\"\"\n",
    "    def __init__(self, device='auto'):\n",
    "        # è‡ªåŠ¨æ£€æµ‹è®¾å¤‡\n",
    "        if device == 'auto':\n",
    "            if torch.cuda.is_available():\n",
    "                self.device = 'cuda'\n",
    "                print(f\"ğŸš€ æ£€æµ‹åˆ°CUDAè®¾å¤‡: {torch.cuda.get_device_name()}\")\n",
    "                print(f\"   â€¢ GPUå†…å­˜: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "                print(f\"   â€¢ CUDAç‰ˆæœ¬: {torch.version.cuda}\")\n",
    "            elif hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():\n",
    "                self.device = 'mps'\n",
    "                print(\"ğŸ æ£€æµ‹åˆ°Apple Silicon (MPS)\")\n",
    "            else:\n",
    "                self.device = 'cpu'\n",
    "                print(\"ğŸ’» ä½¿ç”¨CPUè®­ç»ƒ\")\n",
    "        else:\n",
    "            self.device = device\n",
    "        \n",
    "        # åˆå§‹åŒ–æ¨¡å‹\n",
    "        self.vae = ImprovedVAE().to(self.device)\n",
    "        self.unet = ImprovedUNet2DConditionModel(\n",
    "            in_channels=4,\n",
    "            out_channels=4,\n",
    "            model_channels=128,\n",
    "            channel_mult=(1, 2, 4, 8),\n",
    "            attention_resolutions=(8, 16),\n",
    "            context_dim=512\n",
    "        ).to(self.device)\n",
    "        self.scheduler = ImprovedDDPMScheduler()\n",
    "        \n",
    "        # ä¼˜åŒ–å™¨è®¾ç½®\n",
    "        self.optimizer = optim.AdamW([\n",
    "            {'params': self.vae.parameters(), 'lr': 1e-4},\n",
    "            {'params': self.unet.parameters(), 'lr': 1e-4}\n",
    "        ], weight_decay=0.01)\n",
    "        \n",
    "        # å­¦ä¹ ç‡è°ƒåº¦å™¨\n",
    "        self.scheduler_lr = optim.lr_scheduler.CosineAnnealingLR(\n",
    "            self.optimizer, T_max=100, eta_min=1e-6\n",
    "        )\n",
    "        \n",
    "        # æ··åˆç²¾åº¦è®­ç»ƒ\n",
    "        self.scaler = GradScaler()\n",
    "        \n",
    "        # è®­ç»ƒå‚æ•°\n",
    "        self.num_epochs = 50\n",
    "        self.batch_size = 8  # Colab GPUå†…å­˜ä¼˜åŒ–\n",
    "        self.gradient_accumulation_steps = 4\n",
    "        self.save_every = 5\n",
    "        \n",
    "        # æŸå¤±å‡½æ•°\n",
    "        self.mse_loss = nn.MSELoss()\n",
    "        \n",
    "        print(f\"âœ… æ¨¡å‹åˆå§‹åŒ–å®Œæˆï¼Œä½¿ç”¨è®¾å¤‡: {self.device}\")\n",
    "    \n",
    "    def create_synthetic_dataset(self, num_samples=1000):\n",
    "        \"\"\"\n",
    "        åˆ›å»ºåˆæˆæ•°æ®é›†ç”¨äºæ¼”ç¤º\n",
    "        åœ¨å®é™…ä½¿ç”¨ä¸­ï¼Œè¿™é‡Œåº”è¯¥åŠ è½½çœŸå®çš„æ±‰å­—æ•°æ®\n",
    "        \"\"\"\n",
    "        print(f\"ğŸ“Š åˆ›å»ºåˆæˆæ•°æ®é›† ({num_samples} æ ·æœ¬)...\")\n",
    "        \n",
    "        # åˆ›å»º128x128çš„åˆæˆå›¾åƒ\n",
    "        images = []\n",
    "        for i in range(num_samples):\n",
    "            # åˆ›å»ºç®€å•çš„å‡ ä½•å›¾æ¡ˆä½œä¸ºè®­ç»ƒæ•°æ®\n",
    "            img = np.zeros((128, 128, 3), dtype=np.float32)\n",
    "            \n",
    "            # æ·»åŠ ä¸€äº›éšæœºå‡ ä½•å½¢çŠ¶\n",
    "            if i % 4 == 0:\n",
    "                # åœ†å½¢\n",
    "                y, x = np.ogrid[:128, :128]\n",
    "                mask = (x - 64)**2 + (y - 64)**2 <= 30**2\n",
    "                img[mask] = [0.8, 0.8, 0.8]\n",
    "            elif i % 4 == 1:\n",
    "                # çŸ©å½¢\n",
    "                img[40:88, 40:88] = [0.7, 0.7, 0.7]\n",
    "            elif i % 4 == 2:\n",
    "                # ä¸‰è§’å½¢\n",
    "                for y in range(128):\n",
    "                    for x in range(128):\n",
    "                        if y >= 64 and abs(x - 64) <= (y - 64):\n",
    "                            img[y, x] = [0.6, 0.6, 0.6]\n",
    "            else:\n",
    "                # éšæœºå™ªå£°\n",
    "                img = np.random.rand(128, 128, 3).astype(np.float32) * 0.5\n",
    "            \n",
    "            # å½’ä¸€åŒ–åˆ°[-1, 1]\n",
    "            img = (img - 0.5) * 2\n",
    "            images.append(img)\n",
    "        \n",
    "        # è½¬æ¢ä¸ºtensor\n",
    "        images = torch.tensor(images, dtype=torch.float32).permute(0, 3, 1, 2)\n",
    "        print(f\"âœ… æ•°æ®é›†åˆ›å»ºå®Œæˆ: {images.shape}\")\n",
    "        \n",
    "        return images\n",
    "    \n",
    "    def train_epoch(self, dataloader, epoch):\n",
    "        \"\"\"\n",
    "        è®­ç»ƒä¸€ä¸ªepoch\n",
    "        \"\"\"\n",
    "        self.vae.train()\n",
    "        self.unet.train()\n",
    "        \n",
    "        total_loss = 0\n",
    "        num_batches = len(dataloader)\n",
    "        \n",
    "        for batch_idx, images in enumerate(dataloader):\n",
    "            images = images.to(self.device)\n",
    "            \n",
    "            # æ¢¯åº¦ç´¯ç§¯\n",
    "            with autocast():\n",
    "                # VAEç¼–ç \n",
    "                latents, mu, logvar, kl_loss = self.vae.encode(images)\n",
    "                \n",
    "                # æ·»åŠ å™ªå£°\n",
    "                noise = torch.randn_like(latents)\n",
    "                timesteps = torch.randint(0, self.scheduler.num_train_timesteps, \n",
    "                                       (latents.shape[0],), device=self.device)\n",
    "                noisy_latents = self.scheduler.add_noise(latents, noise, timesteps)\n",
    "                \n",
    "                # UNeté¢„æµ‹å™ªå£°\n",
    "                noise_pred = self.unet(noisy_latents, timesteps)\n",
    "                \n",
    "                # è®¡ç®—æŸå¤±\n",
    "                noise_loss = self.mse_loss(noise_pred, noise)\n",
    "                reconstruction_loss = self.mse_loss(self.vae.decode(latents), images)\n",
    "                \n",
    "                # æ€»æŸå¤±\n",
    "                loss = noise_loss + 0.1 * kl_loss + 0.1 * reconstruction_loss\n",
    "                loss = loss / self.gradient_accumulation_steps\n",
    "            \n",
    "            # åå‘ä¼ æ’­\n",
    "            self.scaler.scale(loss).backward()\n",
    "            \n",
    "            if (batch_idx + 1) % self.gradient_accumulation_steps == 0:\n",
    "                # æ¢¯åº¦è£å‰ª\n",
    "                self.scaler.unscale_(self.optimizer)\n",
    "                torch.nn.utils.clip_grad_norm_(\n",
    "                    list(self.vae.parameters()) + list(self.unet.parameters()), \n",
    "                    max_norm=1.0\n",
    "                )\n",
    "                \n",
    "                # ä¼˜åŒ–å™¨æ­¥è¿›\n",
    "                self.scaler.step(self.optimizer)\n",
    "                self.scaler.update()\n",
    "                self.optimizer.zero_grad()\n",
    "            \n",
    "            total_loss += loss.item() * self.gradient_accumulation_steps\n",
    "            \n",
    "            # è¿›åº¦æ˜¾ç¤º\n",
    "            if (batch_idx + 1) % 10 == 0:\n",
    "                print(f\"   Epoch {epoch+1}/{self.num_epochs}, \"\n",
    "                      f\"Batch {batch_idx+1}/{num_batches}, \"\n",
    "                      f\"Loss: {loss.item():.6f}\")\n",
    "        \n",
    "        # å­¦ä¹ ç‡è°ƒåº¦\n",
    "        self.scheduler_lr.step()\n",
    "        \n",
    "        return total_loss / num_batches\n",
    "    \n",
    "    def save_checkpoint(self, epoch, loss, save_dir=\"colab_checkpoints\"):\n",
    "        \"\"\"\n",
    "        ä¿å­˜æ£€æŸ¥ç‚¹\n",
    "        \"\"\"\n",
    "        os.makedirs(save_dir, exist_ok=True)\n",
    "        \n",
    "        checkpoint = {\n",
    "            'epoch': epoch,\n",
    "            'vae_state_dict': self.vae.state_dict(),\n",
    "            'unet_state_dict': self.unet.state_dict(),\n",
    "            'optimizer_state_dict': self.optimizer.state_dict(),\n",
    "            'scheduler_state_dict': self.scheduler_lr.state_dict(),\n",
    "            'loss': loss,\n",
    "            'device': self.device\n",
    "        }\n",
    "        \n",
    "        checkpoint_path = os.path.join(save_dir, f'checkpoint_epoch_{epoch+1}.pth')\n",
    "        torch.save(checkpoint, checkpoint_path)\n",
    "        print(f\"ğŸ’¾ æ£€æŸ¥ç‚¹å·²ä¿å­˜: {checkpoint_path}\")\n",
    "        \n",
    "        # ä¿å­˜æœ€ä½³æ¨¡å‹\n",
    "        if epoch == 0 or loss < getattr(self, 'best_loss', float('inf')):\n",
    "            self.best_loss = loss\n",
    "            best_model_path = os.path.join(save_dir, 'best_model.pth')\n",
    "            torch.save(checkpoint, best_model_path)\n",
    "            print(f\"ğŸ† æœ€ä½³æ¨¡å‹å·²ä¿å­˜: {best_model_path}\")\n",
    "    \n",
    "    def train(self):\n",
    "        \"\"\"\n",
    "        ä¸»è®­ç»ƒå¾ªç¯\n",
    "        \"\"\"\n",
    "        print(f\"\\nğŸ¯ å¼€å§‹è®­ç»ƒ...\")\n",
    "        print(f\"   â€¢ è®¾å¤‡: {self.device}\")\n",
    "        print(f\"   â€¢ æ‰¹æ¬¡å¤§å°: {self.batch_size}\")\n",
    "        print(f\"   â€¢ æ¢¯åº¦ç´¯ç§¯æ­¥æ•°: {self.gradient_accumulation_steps}\")\n",
    "        print(f\"   â€¢ æ€»epochs: {self.num_epochs}\")\n",
    "        print(f\"   â€¢ æ··åˆç²¾åº¦: {'å¯ç”¨' if self.device == 'cuda' else 'ç¦ç”¨'}\")\n",
    "        \n",
    "        # åˆ›å»ºæ•°æ®é›†\n",
    "        images = self.create_synthetic_dataset()\n",
    "        dataloader = DataLoader(images, batch_size=self.batch_size, shuffle=True)\n",
    "        \n",
    "        # è®­ç»ƒå†å²\n",
    "        train_losses = []\n",
    "        start_time = time.time()\n",
    "        \n",
    "        try:\n",
    "            for epoch in range(self.num_epochs):\n",
    "                epoch_start = time.time()\n",
    "                \n",
    "                print(f\"\\nğŸ”„ Epoch {epoch+1}/{self.num_epochs}\")\n",
    "                print(\"-\" * 50)\n",
    "                \n",
    "                # è®­ç»ƒ\n",
    "                loss = self.train_epoch(dataloader, epoch)\n",
    "                train_losses.append(loss)\n",
    "                \n",
    "                epoch_time = time.time() - epoch_start\n",
    "                print(f\"   â±ï¸  Epochè€—æ—¶: {epoch_time:.2f}ç§’\")\n",
    "                print(f\"   ğŸ“Š å¹³å‡æŸå¤±: {loss:.6f}\")\n",
    "                print(f\"   ğŸ“ˆ å­¦ä¹ ç‡: {self.optimizer.param_groups[0]['lr']:.2e}\")\n",
    "                \n",
    "                # ä¿å­˜æ£€æŸ¥ç‚¹\n",
    "                if (epoch + 1) % self.save_every == 0:\n",
    "                    self.save_checkpoint(epoch, loss)\n",
    "                \n",
    "                # å†…å­˜æ¸…ç† (Colabä¼˜åŒ–)\n",
    "                if self.device == 'cuda':\n",
    "                    torch.cuda.empty_cache()\n",
    "                    gc.collect()\n",
    "                \n",
    "                # æ˜¾ç¤ºGPUå†…å­˜ä½¿ç”¨æƒ…å†µ\n",
    "                if self.device == 'cuda':\n",
    "                    memory_allocated = torch.cuda.memory_allocated() / 1e9\n",
    "                    memory_reserved = torch.cuda.memory_reserved() / 1e9\n",
    "                    print(f\"   ğŸ§  GPUå†…å­˜: {memory_allocated:.2f}GB / {memory_reserved:.2f}GB\")\n",
    "        \n",
    "        except KeyboardInterrupt:\n",
    "            print(f\"\\nâš ï¸  è®­ç»ƒè¢«ç”¨æˆ·ä¸­æ–­\")\n",
    "        except Exception as e:\n",
    "            print(f\"\\nâŒ è®­ç»ƒå‡ºé”™: {e}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "        \n",
    "        finally:\n",
    "            # ä¿å­˜æœ€ç»ˆæ¨¡å‹\n",
    "            final_loss = train_losses[-1] if train_losses else float('inf')\n",
    "            self.save_checkpoint(len(train_losses) - 1, final_loss)\n",
    "            \n",
    "            # è®­ç»ƒæ€»ç»“\n",
    "            total_time = time.time() - start_time\n",
    "            print(f\"\\nğŸ‰ è®­ç»ƒå®Œæˆ!\")\n",
    "            print(f\"   â±ï¸  æ€»è€—æ—¶: {total_time:.2f}ç§’\")\n",
    "            print(f\"   ğŸ“Š æœ€ç»ˆæŸå¤±: {final_loss:.6f}\")\n",
    "            print(f\"   ğŸ“ˆ æŸå¤±å˜åŒ–: {train_losses[0]:.6f} â†’ {final_loss:.6f}\")\n",
    "            \n",
    "            # ç»˜åˆ¶æŸå¤±æ›²çº¿\n",
    "            self.plot_training_curve(train_losses)\n",
    "    \n",
    "    def plot_training_curve(self, losses):\n",
    "        \"\"\"\n",
    "        ç»˜åˆ¶è®­ç»ƒæŸå¤±æ›²çº¿\n",
    "        \"\"\"\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.plot(losses, 'b-', linewidth=2, label='è®­ç»ƒæŸå¤±')\n",
    "        plt.title('Colabè®­ç»ƒæŸå¤±æ›²çº¿', fontsize=16)\n",
    "        plt.xlabel('Epoch', fontsize=14)\n",
    "        plt.ylabel('æŸå¤±', fontsize=14)\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        plt.legend(fontsize=12)\n",
    "        plt.tight_layout()\n",
    "        \n",
    "        # ä¿å­˜å›¾ç‰‡\n",
    "        plot_path = 'colab_training_curve.png'\n",
    "        plt.savefig(plot_path, dpi=150, bbox_inches='tight')\n",
    "        print(f\"ğŸ“Š è®­ç»ƒæ›²çº¿å·²ä¿å­˜: {plot_path}\")\n",
    "        plt.show()\n",
    "    \n",
    "    def test_generation(self, prompt=\"water\"):\n",
    "        \"\"\"\n",
    "        æµ‹è¯•ç”ŸæˆåŠŸèƒ½\n",
    "        \"\"\"\n",
    "        print(f\"\\nğŸ§ª æµ‹è¯•ç”Ÿæˆ: {prompt}\")\n",
    "        \n",
    "        try:\n",
    "            # åˆ›å»ºpipeline\n",
    "            pipeline = ImprovedStableDiffusionPipeline(device=self.device)\n",
    "            \n",
    "            # åŠ è½½è®­ç»ƒå¥½çš„æƒé‡\n",
    "            if hasattr(self, 'best_loss'):\n",
    "                checkpoint_path = 'colab_checkpoints/best_model.pth'\n",
    "                if os.path.exists(checkpoint_path):\n",
    "                    checkpoint = torch.load(checkpoint_path, map_location=self.device)\n",
    "                    pipeline.vae.load_state_dict(checkpoint['vae_state_dict'])\n",
    "                    pipeline.unet.load_state_dict(checkpoint['unet_state_dict'])\n",
    "                    print(f\"âœ… å·²åŠ è½½æœ€ä½³æ¨¡å‹æƒé‡\")\n",
    "            \n",
    "            # ç”Ÿæˆå›¾åƒ\n",
    "            print(f\"ğŸŒŠ ç”Ÿæˆä¸­...\")\n",
    "            result = pipeline.generate(\n",
    "                prompt,\n",
    "                height=128,\n",
    "                width=128,\n",
    "                num_inference_steps=50,\n",
    "                guidance_scale=7.5,\n",
    "                seed=42\n",
    "            )\n",
    "            \n",
    "            # ä¿å­˜ç»“æœ\n",
    "            if isinstance(result, torch.Tensor):\n",
    "                result = (result + 1) / 2\n",
    "                result = torch.clamp(result, 0, 1)\n",
    "                img_array = result.squeeze(0).permute(1, 2, 0).cpu().numpy()\n",
    "                pil_image = Image.fromarray((img_array * 255).astype(np.uint8))\n",
    "            else:\n",
    "                pil_image = result\n",
    "            \n",
    "            output_path = f'colab_generated_{prompt}.png'\n",
    "            pil_image.save(output_path)\n",
    "            print(f\"âœ… ç”Ÿæˆå®Œæˆï¼Œå·²ä¿å­˜: {output_path}\")\n",
    "            \n",
    "            # æ˜¾ç¤ºå›¾åƒ\n",
    "            plt.figure(figsize=(6, 6))\n",
    "            plt.imshow(pil_image, cmap='gray')\n",
    "            plt.title(f'Colabç”Ÿæˆ: {prompt}', fontsize=14)\n",
    "            plt.axis('off')\n",
    "            plt.show()\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"âŒ ç”Ÿæˆæµ‹è¯•å¤±è´¥: {e}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    ä¸»å‡½æ•°\n",
    "    \"\"\"\n",
    "    print(\"ğŸš€ Google Colabä¼˜åŒ–çš„Stable Diffusionè®­ç»ƒå™¨\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # æ£€æŸ¥Colabç¯å¢ƒ\n",
    "    is_colab = 'COLAB_GPU' in os.environ\n",
    "    if is_colab:\n",
    "        print(\"âœ… æ£€æµ‹åˆ°Google Colabç¯å¢ƒ\")\n",
    "        print(f\"   â€¢ GPUç±»å‹: {os.environ.get('COLAB_GPU', 'Unknown')}\")\n",
    "        print(f\"   â€¢ è¿è¡Œæ—¶ç±»å‹: {os.environ.get('COLAB_RUNTIME_TYPE', 'Unknown')}\")\n",
    "    else:\n",
    "        print(\"ğŸ’» æœ¬åœ°ç¯å¢ƒè¿è¡Œ\")\n",
    "    \n",
    "    # åˆ›å»ºè®­ç»ƒå™¨\n",
    "    trainer = ColabOptimizedTrainer(device='auto')\n",
    "    \n",
    "    # å¼€å§‹è®­ç»ƒ\n",
    "    trainer.train()\n",
    "    \n",
    "    # æµ‹è¯•ç”Ÿæˆ\n",
    "    trainer.test_generation(\"water\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸš€ Start Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create trainer and start training\n",
    "if 'ColabOptimizedTrainer' in globals():\n",
    "    trainer = ColabOptimizedTrainer()\n",
    "    trainer.train()\n",
    "else:\n",
    "    print(\"âš ï¸ Trainer class not found. Please run the model implementation cells first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ“¥ Download Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download training results\n",
    "from google.colab import files\n",
    "import zipfile\n",
    "\n",
    "def download_results():\n",
    "    print(\"ğŸ“¥ Preparing results for download...\")\n",
    "    \n",
    "    # Create results zip\n",
    "    with zipfile.ZipFile('training_results.zip', 'w') as zipf:\n",
    "        # Add checkpoints\n",
    "        if os.path.exists('checkpoints'):\n",
    "            for root, dirs, files in os.walk('checkpoints'):\n",
    "                for file in files:\n",
    "                    file_path = os.path.join(root, file)\n",
    "                    zipf.write(file_path, os.path.relpath(file_path, '.'))\n",
    "        \n",
    "        # Add training curves\n",
    "        for img_file in ['training_curve.png', 'loss_curve.png']:\n",
    "            if os.path.exists(img_file):\n",
    "                zipf.write(img_file)\n",
    "        \n",
    "        # Add generated images\n",
    "        for i in range(10):\n",
    "            img_file = f'generated_{i}.png'\n",
    "            if os.path.exists(img_file):\n",
    "                zipf.write(img_file)\n",
    "    \n",
    "    print(\"âœ… Results packaged: training_results.zip\")\n",
    "    \n",
    "    # Download\n",
    "    try:\n",
    "        files.download('training_results.zip')\n",
    "        print(\"ğŸ“¥ Results downloaded successfully!\")\n",
    "    except:\n",
    "        print(\"âš ï¸ Download failed (not in Colab)\")\n",
    "        print(\"ğŸ“ Files are saved in the current directory\")\n",
    "\n",
    "# Download results\n",
    "download_results()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
