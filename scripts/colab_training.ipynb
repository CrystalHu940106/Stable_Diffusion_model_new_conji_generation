{
 "cells": [
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "Google Colabä¼˜åŒ–çš„Stable Diffusionè®­ç»ƒè„šæœ¬\n",
    "ä¸“é—¨ä¸ºColab GPUç¯å¢ƒä¼˜åŒ–ï¼ŒåŒ…å«è‡ªåŠ¨æ£€æµ‹å’Œæ€§èƒ½ä¼˜åŒ–\n",
    "\"\"\"\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import gc\n",
    "\n",
    "# addwhenå‰ç›®å½•toè·¯å¾„\n",
    "sys.path.append(os.path.dirname(os.path.abspath(__file__)))\n",
    "\n",
    "from improved_stable_diffusion import (\n",
    "    ImprovedStableDiffusionPipeline,\n",
    "    ImprovedVAE,\n",
    "    ImprovedUNet2DConditionModel,\n",
    "    ImprovedDDPMScheduler\n",
    ")\n",
    "\n",
    "class ColabOptimizedTrainer:\n",
    "    \"\"\"\n",
    "    Colabä¼˜åŒ–çš„è®­ç»ƒå™¨\n",
    "    \"\"\"\n",
    "    def __init__(self, device='auto'):\n",
    "        # è‡ªåŠ¨æ£€æµ‹è®¾å¤‡\n",
    "        if device == 'auto':\n",
    "            if torch.cuda.is_available():\n",
    "                self.device = 'cuda'\n",
    "                print(f\"ğŸš€ æ£€æµ‹toCUDAè®¾å¤‡: {torch.cuda.get_device_name()}\")\n",
    "                print(f\"   â€¢ GPUå†…å­˜: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "                print(f\"   â€¢ CUDAversion: {torch.version.cuda}\")\n",
    "            elif hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():\n",
    "                self.device = 'mps'\n",
    "                print(\"ğŸ æ£€æµ‹toApple Silicon (MPS)\")\n",
    "            else:\n",
    "                self.device = 'cpu'\n",
    "                print(\"ğŸ’» usingCPUtraining\")\n",
    "        else:\n",
    "            self.device = device\n",
    "        \n",
    "        # initializationmodel\n",
    "        self.vae = ImprovedVAE().to(self.device)\n",
    "        self.unet = ImprovedUNet2DConditionModel(\n",
    "            in_channels=4,\n",
    "            out_channels=4,\n",
    "            model_channels=128,\n",
    "            channel_mult=(1, 2, 4, 8),\n",
    "            attention_resolutions=(8, 16),\n",
    "            context_dim=512\n",
    "        ).to(self.device)\n",
    "        self.scheduler = ImprovedDDPMScheduler()\n",
    "        \n",
    "        # ä¼˜åŒ–å™¨set\n",
    "        self.optimizer = optim.AdamW([\n",
    "            {'params': self.vae.parameters(), 'lr': 1e-4},\n",
    "            {'params': self.unet.parameters(), 'lr': 1e-4}\n",
    "        ], weight_decay=0.01)\n",
    "        \n",
    "        # å­¦ä¹ ç‡schedulingå™¨\n",
    "        self.scheduler_lr = optim.lr_scheduler.CosineAnnealingLR(\n",
    "            self.optimizer, T_max=100, eta_min=1e-6\n",
    "        )\n",
    "        \n",
    "        # æ··åˆç²¾åº¦training\n",
    "        self.scaler = GradScaler()\n",
    "        \n",
    "        # trainingå‚æ•°\n",
    "        self.num_epochs = 50\n",
    "        self.batch_size = 8  # Colab GPUå†…å­˜ä¼˜åŒ–\n",
    "        self.gradient_accumulation_steps = 4\n",
    "        self.save_every = 5\n",
    "        \n",
    "        # æŸå¤±å‡½æ•°\n",
    "        self.mse_loss = nn.MSELoss()\n",
    "        \n",
    "        print(f\"âœ… modelinitializationcompleteï¼Œusingè®¾å¤‡: {self.device}\")\n",
    "    \n",
    "    def create_synthetic_dataset(self, num_samples=1000):\n",
    "        \"\"\"\n",
    "        åˆ›å»ºåˆæˆæ•°æ®é›†ç”¨äºæ¼”ç¤º\n",
    "        åœ¨å®é™…ä½¿ç”¨ä¸­ï¼Œè¿™é‡Œåº”è¯¥åŠ è½½çœŸå®çš„æ±‰å­—æ•°æ®\n",
    "        \"\"\"\n",
    "        print(f\"ğŸ“Š createåˆæˆæ•°æ®é›† ({num_samples} æ ·æœ¬)...\")\n",
    "        \n",
    "        # create128x128ofåˆæˆimage\n",
    "        images = []\n",
    "        for i in range(num_samples):\n",
    "            # createç®€å•ofå‡ ä½•å›¾æ¡ˆä½œä¸ºtrainingæ•°æ®\n",
    "            img = np.zeros((128, 128, 3), dtype=np.float32)\n",
    "            \n",
    "            # addä¸€äº›éšæœºå‡ ä½•å½¢çŠ¶\n",
    "            if i % 4 == 0:\n",
    "                # åœ†å½¢\n",
    "                y, x = np.ogrid[:128, :128]\n",
    "                mask = (x - 64)**2 + (y - 64)**2 <= 30**2\n",
    "                img[mask] = [0.8, 0.8, 0.8]\n",
    "            elif i % 4 == 1:\n",
    "                # çŸ©å½¢\n",
    "                img[40:88, 40:88] = [0.7, 0.7, 0.7]\n",
    "            elif i % 4 == 2:\n",
    "                # ä¸‰è§’å½¢\n",
    "                for y in range(128):\n",
    "                    for x in range(128):\n",
    "                        if y >= 64 and abs(x - 64) <= (y - 64):\n",
    "                            img[y, x] = [0.6, 0.6, 0.6]\n",
    "            else:\n",
    "                # éšæœºnoise\n",
    "                img = np.random.rand(128, 128, 3).astype(np.float32) * 0.5\n",
    "            \n",
    "            # å½’ä¸€åŒ–to[-1, 1]\n",
    "            img = (img - 0.5) * 2\n",
    "            images.append(img)\n",
    "        \n",
    "        # convertä¸ºtensor\n",
    "        images = torch.tensor(images, dtype=torch.float32).permute(0, 3, 1, 2)\n",
    "        print(f\"âœ… æ•°æ®é›†createcomplete: {images.shape}\")\n",
    "        \n",
    "        return images\n",
    "    \n",
    "    def train_epoch(self, dataloader, epoch):\n",
    "        \"\"\"\n",
    "        è®­ç»ƒä¸€ä¸ªepoch\n",
    "        \"\"\"\n",
    "        self.vae.train()\n",
    "        self.unet.train()\n",
    "        \n",
    "        total_loss = 0\n",
    "        num_batches = len(dataloader)\n",
    "        \n",
    "        for batch_idx, images in enumerate(dataloader):\n",
    "            images = images.to(self.device)\n",
    "            \n",
    "            # æ¢¯åº¦ç´¯ç§¯\n",
    "            with autocast():\n",
    "                # VAEç¼–ç \n",
    "                latents, mu, logvar, kl_loss = self.vae.encode(images)\n",
    "                \n",
    "                # addnoise\n",
    "                noise = torch.randn_like(latents)\n",
    "                timesteps = torch.randint(0, self.scheduler.num_train_timesteps, \n",
    "                                       (latents.shape[0],), device=self.device)\n",
    "                noisy_latents = self.scheduler.add_noise(latents, noise, timesteps)\n",
    "                \n",
    "                # UNetpredictionnoise\n",
    "                noise_pred = self.unet(noisy_latents, timesteps)\n",
    "                \n",
    "                # è®¡ç®—æŸå¤±\n",
    "                noise_loss = self.mse_loss(noise_pred, noise)\n",
    "                reconstruction_loss = self.mse_loss(self.vae.decode(latents), images)\n",
    "                \n",
    "                # æ€»æŸå¤±\n",
    "                loss = noise_loss + 0.1 * kl_loss + 0.1 * reconstruction_loss\n",
    "                loss = loss / self.gradient_accumulation_steps\n",
    "            \n",
    "            # åå‘ä¼ æ’­\n",
    "            self.scaler.scale(loss).backward()\n",
    "            \n",
    "            if (batch_idx + 1) % self.gradient_accumulation_steps == 0:\n",
    "                # æ¢¯åº¦è£å‰ª\n",
    "                self.scaler.unscale_(self.optimizer)\n",
    "                torch.nn.utils.clip_grad_norm_(\n",
    "                    list(self.vae.parameters()) + list(self.unet.parameters()), \n",
    "                    max_norm=1.0\n",
    "                )\n",
    "                \n",
    "                # ä¼˜åŒ–å™¨æ­¥è¿›\n",
    "                self.scaler.step(self.optimizer)\n",
    "                self.scaler.update()\n",
    "                self.optimizer.zero_grad()\n",
    "            \n",
    "            total_loss += loss.item() * self.gradient_accumulation_steps\n",
    "            \n",
    "            # è¿›åº¦æ˜¾ç¤º\n",
    "            if (batch_idx + 1) % 10 == 0:\n",
    "                print(f\"   Epoch {epoch+1}/{self.num_epochs}, \"\n",
    "                      f\"Batch {batch_idx+1}/{num_batches}, \"\n",
    "                      f\"Loss: {loss.item():.6f}\")\n",
    "        \n",
    "        # å­¦ä¹ ç‡scheduling\n",
    "        self.scheduler_lr.step()\n",
    "        \n",
    "        return total_loss / num_batches\n",
    "    \n",
    "    def save_checkpoint(self, epoch, loss, save_dir=\"colab_checkpoints\"):\n",
    "        \"\"\"\n",
    "        ä¿å­˜æ£€æŸ¥ç‚¹\n",
    "        \"\"\"\n",
    "        os.makedirs(save_dir, exist_ok=True)\n",
    "        \n",
    "        checkpoint = {\n",
    "            'epoch': epoch,\n",
    "            'vae_state_dict': self.vae.state_dict(),\n",
    "            'unet_state_dict': self.unet.state_dict(),\n",
    "            'optimizer_state_dict': self.optimizer.state_dict(),\n",
    "            'scheduler_state_dict': self.scheduler_lr.state_dict(),\n",
    "            'loss': loss,\n",
    "            'device': self.device\n",
    "        }\n",
    "        \n",
    "        checkpoint_path = os.path.join(save_dir, f'checkpoint_epoch_{epoch+1}.pth')\n",
    "        torch.save(checkpoint, checkpoint_path)\n",
    "        print(f\"ğŸ’¾ checkç‚¹å·²ä¿å­˜: {checkpoint_path}\")\n",
    "        \n",
    "        # ä¿å­˜æœ€ä½³model\n",
    "        if epoch == 0 or loss < getattr(self, 'best_loss', float('inf')):\n",
    "            self.best_loss = loss\n",
    "            best_model_path = os.path.join(save_dir, 'best_model.pth')\n",
    "            torch.save(checkpoint, best_model_path)\n",
    "            print(f\"ğŸ† æœ€ä½³modelå·²ä¿å­˜: {best_model_path}\")\n",
    "    \n",
    "    def train(self):\n",
    "        \"\"\"\n",
    "        ä¸»è®­ç»ƒå¾ªç¯\n",
    "        \"\"\"\n",
    "        print(f\"\\nğŸ¯ start training...\")\n",
    "        print(f\"   â€¢ è®¾å¤‡: {self.device}\")\n",
    "        print(f\"   â€¢ æ‰¹æ¬¡å¤§å°: {self.batch_size}\")\n",
    "        print(f\"   â€¢ æ¢¯åº¦ç´¯ç§¯æ­¥æ•°: {self.gradient_accumulation_steps}\")\n",
    "        print(f\"   â€¢ æ€»epochs: {self.num_epochs}\")\n",
    "        print(f\"   â€¢ æ··åˆç²¾åº¦: {'å¯ç”¨' if self.device == 'cuda' else 'ç¦ç”¨'}\")\n",
    "        \n",
    "        # createæ•°æ®é›†\n",
    "        images = self.create_synthetic_dataset()\n",
    "        dataloader = DataLoader(images, batch_size=self.batch_size, shuffle=True)\n",
    "        \n",
    "        # trainingå†å²\n",
    "        train_losses = []\n",
    "        start_time = time.time()\n",
    "        \n",
    "        try:\n",
    "            for epoch in range(self.num_epochs):\n",
    "                epoch_start = time.time()\n",
    "                \n",
    "                print(f\"\\nğŸ”„ Epoch {epoch+1}/{self.num_epochs}\")\n",
    "                print(\"-\" * 50)\n",
    "                \n",
    "                # training\n",
    "                loss = self.train_epoch(dataloader, epoch)\n",
    "                train_losses.append(loss)\n",
    "                \n",
    "                epoch_time = time.time() - epoch_start\n",
    "                print(f\"   â±ï¸  Epochè€—æ—¶: {epoch_time:.2f}ç§’\")\n",
    "                print(f\"   ğŸ“Š å¹³å‡æŸå¤±: {loss:.6f}\")\n",
    "                print(f\"   ğŸ“ˆ å­¦ä¹ ç‡: {self.optimizer.param_groups[0]['lr']:.2e}\")\n",
    "                \n",
    "                # ä¿å­˜checkç‚¹\n",
    "                if (epoch + 1) % self.save_every == 0:\n",
    "                    self.save_checkpoint(epoch, loss)\n",
    "                \n",
    "                # å†…å­˜æ¸…ç† (Colabä¼˜åŒ–)\n",
    "                if self.device == 'cuda':\n",
    "                    torch.cuda.empty_cache()\n",
    "                    gc.collect()\n",
    "                \n",
    "                # æ˜¾ç¤ºGPUå†…å­˜usingæƒ…å†µ\n",
    "                if self.device == 'cuda':\n",
    "                    memory_allocated = torch.cuda.memory_allocated() / 1e9\n",
    "                    memory_reserved = torch.cuda.memory_reserved() / 1e9\n",
    "                    print(f\"   ğŸ§  GPUå†…å­˜: {memory_allocated:.2f}GB / {memory_reserved:.2f}GB\")\n",
    "        \n",
    "        except KeyboardInterrupt:\n",
    "            print(f\"\\nâš ï¸  trainingè¢«ç”¨æˆ·ä¸­æ–­\")\n",
    "        except Exception as e:\n",
    "            print(f\"\\nâŒ trainingå‡ºé”™: {e}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "        \n",
    "        finally:\n",
    "            # ä¿å­˜æœ€ç»ˆmodel\n",
    "            final_loss = train_losses[-1] if train_losses else float('inf')\n",
    "            self.save_checkpoint(len(train_losses) - 1, final_loss)\n",
    "            \n",
    "            # trainingæ€»ç»“\n",
    "            total_time = time.time() - start_time\n",
    "            print(f\"\\nğŸ‰ trainingcomplete!\")\n",
    "            print(f\"   â±ï¸  æ€»è€—æ—¶: {total_time:.2f}ç§’\")\n",
    "            print(f\"   ğŸ“Š æœ€ç»ˆæŸå¤±: {final_loss:.6f}\")\n",
    "            print(f\"   ğŸ“ˆ æŸå¤±å˜åŒ–: {train_losses[0]:.6f} â†’ {final_loss:.6f}\")\n",
    "            \n",
    "            # ç»˜åˆ¶æŸå¤±æ›²çº¿\n",
    "            self.plot_training_curve(train_losses)\n",
    "    \n",
    "    def plot_training_curve(self, losses):\n",
    "        \"\"\"\n",
    "        ç»˜åˆ¶è®­ç»ƒæŸå¤±æ›²çº¿\n",
    "        \"\"\"\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.plot(losses, 'b-', linewidth=2, label='è®­ç»ƒæŸå¤±')\n",
    "        plt.title('Colabè®­ç»ƒæŸå¤±æ›²çº¿', fontsize=16)\n",
    "        plt.xlabel('Epoch', fontsize=14)\n",
    "        plt.ylabel('æŸå¤±', fontsize=14)\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        plt.legend(fontsize=12)\n",
    "        plt.tight_layout()\n",
    "        \n",
    "        # ä¿å­˜å›¾ç‰‡\n",
    "        plot_path = 'colab_training_curve.png'\n",
    "        plt.savefig(plot_path, dpi=150, bbox_inches='tight')\n",
    "        print(f\"ğŸ“Š trainingæ›²çº¿å·²ä¿å­˜: {plot_path}\")\n",
    "        plt.show()\n",
    "    \n",
    "    def test_generation(self, prompt=\"water\"):\n",
    "        \"\"\"\n",
    "        æµ‹è¯•ç”ŸæˆåŠŸèƒ½\n",
    "        \"\"\"\n",
    "        print(f\"\\nğŸ§ª testgeneration: {prompt}\")\n",
    "        \n",
    "        try:\n",
    "            # createpipeline\n",
    "            pipeline = ImprovedStableDiffusionPipeline(device=self.device)\n",
    "            \n",
    "            # åŠ è½½trainingå¥½ofweights\n",
    "            if hasattr(self, 'best_loss'):\n",
    "                checkpoint_path = 'colab_checkpoints/best_model.pth'\n",
    "                if os.path.exists(checkpoint_path):\n",
    "                    checkpoint = torch.load(checkpoint_path, map_location=self.device)\n",
    "                    pipeline.vae.load_state_dict(checkpoint['vae_state_dict'])\n",
    "                    pipeline.unet.load_state_dict(checkpoint['unet_state_dict'])\n",
    "                    print(f\"âœ… å·²åŠ è½½æœ€ä½³modelweights\")\n",
    "            \n",
    "            # generationimage\n",
    "            print(f\"ğŸŒŠ generationä¸­...\")\n",
    "            result = pipeline.generate(\n",
    "                prompt,\n",
    "                height=128,\n",
    "                width=128,\n",
    "                num_inference_steps=50,\n",
    "                guidance_scale=7.5,\n",
    "                seed=42\n",
    "            )\n",
    "            \n",
    "            # ä¿å­˜ç»“æœ\n",
    "            if isinstance(result, torch.Tensor):\n",
    "                result = (result + 1) / 2\n",
    "                result = torch.clamp(result, 0, 1)\n",
    "                img_array = result.squeeze(0).permute(1, 2, 0).cpu().numpy()\n",
    "                pil_image = Image.fromarray((img_array * 255).astype(np.uint8))\n",
    "            else:\n",
    "                pil_image = result\n",
    "            \n",
    "            output_path = f'colab_generated_{prompt}.png'\n",
    "            pil_image.save(output_path)\n",
    "            print(f\"âœ… generationcompleteï¼Œå·²ä¿å­˜: {output_path}\")\n",
    "            \n",
    "            # æ˜¾ç¤ºimage\n",
    "            plt.figure(figsize=(6, 6))\n",
    "            plt.imshow(pil_image, cmap='gray')\n",
    "            plt.title(f'Colabç”Ÿæˆ: {prompt}', fontsize=14)\n",
    "            plt.axis('off')\n",
    "            plt.show()\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"âŒ generationtestå¤±è´¥: {e}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    ä¸»å‡½æ•°\n",
    "    \"\"\"\n",
    "    print(\"ğŸš€ Google Colabä¼˜åŒ–ofStable Diffusiontrainingå™¨\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # checkColabç¯å¢ƒ\n",
    "    is_colab = 'COLAB_GPU' in os.environ\n",
    "    if is_colab:\n",
    "        print(\"âœ… æ£€æµ‹toGoogle Colabç¯å¢ƒ\")\n",
    "        print(f\"   â€¢ GPUç±»å‹: {os.environ.get('COLAB_GPU', 'Unknown')}\")\n",
    "        print(f\"   â€¢ runæ—¶ç±»å‹: {os.environ.get('COLAB_RUNTIME_TYPE', 'Unknown')}\")\n",
    "    else:\n",
    "        print(\"ğŸ’» æœ¬åœ°ç¯å¢ƒrun\")\n",
    "    \n",
    "    # createtrainingå™¨\n",
    "    trainer = ColabOptimizedTrainer(device='auto')\n",
    "    \n",
    "    # start training\n",
    "    trainer.train()\n",
    "    \n",
    "    # testgeneration\n",
    "    trainer.test_generation(\"water\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}