{
  "cells": [
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#!/usr/bin/env python3\n",
        "\"\"\"\n",
        "Improved test script for generating high-quality Kanji\n",
        "\"\"\"\n",
        "\n",
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image, ImageEnhance\n",
        "import numpy as np\n",
        "from stable_diffusion_kanji import StableDiffusionPipeline\n",
        "import cv2\n",
        "\n",
        "def enhance_image_quality(image_pil):\n",
        "    \"\"\"Enhance image quality for better Kanji visibility\"\"\"\n",
        "    # Convert to numpy for OpenCV processing\n",
        "    img_np = np.array(image_pil)\n",
        "    \n",
        "    # Convert to grayscale\n",
        "    gray = cv2.cvtColor(img_np, cv2.COLOR_RGB2GRAY)\n",
        "    \n",
        "    # Apply histogram equalization for better contrast\n",
        "    clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8,8))\n",
        "    enhanced = clahe.apply(gray)\n",
        "    \n",
        "    # Apply Gaussian blur to reduce noise\n",
        "    blurred = cv2.GaussianBlur(enhanced, (3, 3), 0)\n",
        "    \n",
        "    # Apply adaptive thresholding for better stroke definition\n",
        "    thresh = cv2.adaptiveThreshold(blurred, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C, cv2.THRESH_BINARY, 11, 2)\n",
        "    \n",
        "    # Convert back to PIL\n",
        "    enhanced_pil = Image.fromarray(thresh)\n",
        "    \n",
        "    return enhanced_pil\n",
        "\n",
        "def test_improved_generation():\n",
        "    \"\"\"Test improved generation with better parameters\"\"\"\n",
        "    print(\"\ud83c\udf8c Improved Kanji Generation Test\")\n",
        "    print(\"=\" * 50)\n",
        "    \n",
        "    # Initialize pipeline with correct configuration\n",
        "    device = 'mps' if torch.backends.mps.is_available() else 'cpu'\n",
        "    print(f\"\ud83d\udd27 Using device: {device}\")\n",
        "    \n",
        "    from stable_diffusion_kanji import VAE, UNet2DConditionModel, DDPMScheduler\n",
        "    \n",
        "    vae = VAE(hidden_dims=[128, 256, 512, 1024]).to(device)\n",
        "    unet = UNet2DConditionModel(\n",
        "        model_channels=256,\n",
        "        num_res_blocks=3,\n",
        "        channel_mult=(1, 2, 4, 8),\n",
        "        attention_resolutions=(8,),\n",
        "        num_heads=16\n",
        "    ).to(device)\n",
        "    \n",
        "    pipeline = StableDiffusionPipeline(device=device)\n",
        "    pipeline.vae = vae\n",
        "    pipeline.unet = unet\n",
        "    \n",
        "    # Load trained model\n",
        "    print(\"\ud83d\udcc2 Loading trained model...\")\n",
        "    try:\n",
        "        checkpoint = torch.load('best_model.pth', map_location=device)\n",
        "        pipeline.vae.load_state_dict(checkpoint['vae_state_dict'])\n",
        "        pipeline.unet.load_state_dict(checkpoint['unet_state_dict'])\n",
        "        print(\"\u2705 Model loaded successfully!\")\n",
        "    except Exception as e:\n",
        "        print(f\"\u274c Failed to load model: {e}\")\n",
        "        return\n",
        "    \n",
        "    # Test concepts with improved parameters\n",
        "    concepts = [\"water\", \"future\"]\n",
        "    \n",
        "    for concept in concepts:\n",
        "        print(f\"\\n\ud83c\udf0a Generating Kanji for: {concept.upper()}\")\n",
        "        print(\"-\" * 30)\n",
        "        \n",
        "        try:\n",
        "            # Generate with different parameters\n",
        "            for guidance_scale in [7.0, 9.0, 11.0]:\n",
        "                for num_steps in [50, 100]:\n",
        "                    print(f\"   Guidance: {guidance_scale}, Steps: {num_steps}\")\n",
        "                    \n",
        "                    # Generate image with more steps\n",
        "                    image = pipeline.generate(\n",
        "                        prompt=f\"kanji character for {concept}, traditional calligraphy, black ink on white paper, high contrast, detailed strokes\",\n",
        "                        height=128,\n",
        "                        width=128,\n",
        "                        num_inference_steps=num_steps,\n",
        "                        guidance_scale=guidance_scale,\n",
        "                        seed=42  # Fixed seed for reproducibility\n",
        "                    )\n",
        "                    \n",
        "                    # Convert to PIL image\n",
        "                    if isinstance(image, torch.Tensor):\n",
        "                        # Denormalize from [-1, 1] to [0, 1]\n",
        "                        image = (image + 1) / 2\n",
        "                        image = torch.clamp(image, 0, 1)\n",
        "                        \n",
        "                        # Convert to PIL\n",
        "                        image_np = image.squeeze(0).permute(1, 2, 0).cpu().numpy()\n",
        "                        image_pil = Image.fromarray((image_np * 255).astype(np.uint8))\n",
        "                    else:\n",
        "                        image_pil = image\n",
        "                    \n",
        "                    # Enhance image quality\n",
        "                    enhanced_pil = enhance_image_quality(image_pil)\n",
        "                    \n",
        "                    # Save both original and enhanced\n",
        "                    orig_filename = f\"kanji_{concept}_g{guidance_scale}_s{num_steps}_orig.png\"\n",
        "                    enhanced_filename = f\"kanji_{concept}_g{guidance_scale}_s{num_steps}_enhanced.png\"\n",
        "                    \n",
        "                    image_pil.save(orig_filename)\n",
        "                    enhanced_pil.save(enhanced_filename)\n",
        "                    \n",
        "                    print(f\"   \ud83d\udcbe Saved: {orig_filename}, {enhanced_filename}\")\n",
        "                    \n",
        "                    # Display comparison\n",
        "                    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 5))\n",
        "                    \n",
        "                    ax1.imshow(image_pil)\n",
        "                    ax1.set_title(f'Original (G:{guidance_scale}, S:{num_steps})')\n",
        "                    ax1.axis('off')\n",
        "                    \n",
        "                    ax2.imshow(enhanced_pil, cmap='gray')\n",
        "                    ax2.set_title(f'Enhanced (G:{guidance_scale}, S:{num_steps})')\n",
        "                    ax2.axis('off')\n",
        "                    \n",
        "                    plt.suptitle(f'Kanji for \"{concept}\" - Quality Comparison')\n",
        "                    plt.tight_layout()\n",
        "                    plt.show()\n",
        "                    \n",
        "        except Exception as e:\n",
        "            print(f\"   \u274c Generation failed: {e}\")\n",
        "            import traceback\n",
        "            traceback.print_exc()\n",
        "    \n",
        "    print(f\"\\n\ud83c\udf89 Improved generation test completed!\")\n",
        "    print(f\"\ud83d\udcc1 Generated images saved in current directory\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    test_improved_generation()\n"
      ],
      "outputs": [],
      "execution_count": null
    }
  ],
  "metadata": {
    "anaconda-cloud": {},
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}