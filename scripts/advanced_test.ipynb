{
  "cells": [
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#!/usr/bin/env python3\n",
        "\"\"\"\n",
        "Advanced Testing Script for Kanji Diffusion Model\n",
        "Test different generation strategies and parameters\n",
        "\"\"\"\n",
        "\n",
        "import json\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from pathlib import Path\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "from torchvision import transforms\n",
        "\n",
        "class SimpleUNet(nn.Module):\n",
        "    \"\"\"Simple UNet for testing generation\"\"\"\n",
        "    \n",
        "    def __init__(self, in_channels=3, out_channels=3, image_size=64):\n",
        "        super().__init__()\n",
        "        \n",
        "        # Simple encoder-decoder structure\n",
        "        self.encoder = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, 32, 3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(32, 64, 3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2),\n",
        "            \n",
        "            nn.Conv2d(64, 64, 3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(64, 128, 3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2),\n",
        "        )\n",
        "        \n",
        "        self.decoder = nn.Sequential(\n",
        "            nn.ConvTranspose2d(128, 64, 2, stride=2),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(64, 64, 3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            \n",
        "            nn.ConvTranspose2d(64, 32, 2, stride=2),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(32, out_channels, 3, padding=1),\n",
        "            nn.Tanh(),\n",
        "        )\n",
        "        \n",
        "    def forward(self, x):\n",
        "        encoded = self.encoder(x)\n",
        "        decoded = self.decoder(encoded)\n",
        "        return decoded\n",
        "\n",
        "def load_model(model_path):\n",
        "    \"\"\"Load trained model\"\"\"\n",
        "    model = SimpleUNet()\n",
        "    checkpoint = torch.load(model_path, map_location='cpu')\n",
        "    model.load_state_dict(checkpoint['model_state_dict'])\n",
        "    model.eval()\n",
        "    return model\n",
        "\n",
        "def create_different_noise_patterns(image_size=64, num_samples=4):\n",
        "    \"\"\"Create different types of noise for generation\"\"\"\n",
        "    \n",
        "    patterns = {}\n",
        "    \n",
        "    # 1. Random noise\n",
        "    patterns['random'] = torch.randn(num_samples, 3, image_size, image_size)\n",
        "    \n",
        "    # 2. Structured noise (more organized)\n",
        "    x = torch.linspace(-1, 1, image_size)\n",
        "    y = torch.linspace(-1, 1, image_size)\n",
        "    X, Y = torch.meshgrid(x, y, indexing='ij')\n",
        "    structured = torch.stack([X, Y, torch.zeros_like(X)], dim=0)\n",
        "    structured = structured.unsqueeze(0).repeat(num_samples, 1, 1, 1)\n",
        "    patterns['structured'] = structured\n",
        "    \n",
        "    # 3. Low-frequency noise\n",
        "    low_freq = torch.randn(num_samples, 3, image_size//4, image_size//4)\n",
        "    low_freq = torch.nn.functional.interpolate(low_freq, size=(image_size, image_size), mode='bilinear')\n",
        "    patterns['low_frequency'] = low_freq\n",
        "    \n",
        "    # 4. High-frequency noise\n",
        "    high_freq = torch.randn(num_samples, 3, image_size, image_size) * 0.5\n",
        "    patterns['high_frequency'] = high_freq\n",
        "    \n",
        "    return patterns\n",
        "\n",
        "def generate_with_different_strategies(model, save_dir=\"advanced_results\"):\n",
        "    \"\"\"Generate using different strategies\"\"\"\n",
        "    \n",
        "    save_path = Path(save_dir)\n",
        "    save_path.mkdir(exist_ok=True)\n",
        "    \n",
        "    print(f\"\\n\ud83c\udfa8 Testing Different Generation Strategies...\")\n",
        "    \n",
        "    # Create different noise patterns\n",
        "    noise_patterns = create_different_noise_patterns()\n",
        "    \n",
        "    for pattern_name, noise_input in noise_patterns.items():\n",
        "        print(f\"\\n   Testing {pattern_name} noise pattern...\")\n",
        "        \n",
        "        # Generate\n",
        "        with torch.no_grad():\n",
        "            generated = model(noise_input)\n",
        "        \n",
        "        # Save results\n",
        "        generated = (generated + 1) / 2\n",
        "        generated = torch.clamp(generated, 0, 1)\n",
        "        \n",
        "        for i in range(generated.shape[0]):\n",
        "            img_array = generated[i].permute(1, 2, 0).numpy()\n",
        "            img = Image.fromarray((img_array * 255).astype(np.uint8))\n",
        "            \n",
        "            img_path = save_path / f\"success_{pattern_name}_{i+1}.png\"\n",
        "            img.save(img_path)\n",
        "            print(f\"     \u2022 Saved: {img_path}\")\n",
        "\n",
        "def test_interpolation_between_kanji(model, dataset_path=\"kanji_dataset\"):\n",
        "    \"\"\"Test interpolation between existing Kanji\"\"\"\n",
        "    \n",
        "    print(f\"\\n\ud83d\udd04 Testing Interpolation Between Kanji...\")\n",
        "    \n",
        "    # Load some existing Kanji\n",
        "    with open(f\"{dataset_path}/metadata/dataset.json\", 'r') as f:\n",
        "        dataset = json.load(f)\n",
        "    \n",
        "    # Find some success-related Kanji\n",
        "    success_kanji = []\n",
        "    for entry in dataset:\n",
        "        meanings = [m.lower() for m in entry['meanings']]\n",
        "        if any(word in meanings for word in ['success', 'achieve', 'complete', 'finish']):\n",
        "            success_kanji.append(entry)\n",
        "            if len(success_kanji) >= 4:\n",
        "                break\n",
        "    \n",
        "    if len(success_kanji) < 2:\n",
        "        print(\"   \u274c Not enough success-related Kanji found for interpolation\")\n",
        "        return\n",
        "    \n",
        "    print(f\"   \u2022 Found {len(success_kanji)} success-related Kanji for interpolation\")\n",
        "    \n",
        "    # Load and process Kanji images\n",
        "    transform = transforms.Compose([\n",
        "        transforms.Resize((64, 64)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
        "    ])\n",
        "    \n",
        "    kanji_images = []\n",
        "    for kanji_info in success_kanji[:2]:  # Use first 2\n",
        "        img_path = f\"{dataset_path}/images/{kanji_info['image_file']}\"\n",
        "        img = Image.open(img_path).convert('RGB')\n",
        "        img_tensor = transform(img).unsqueeze(0)\n",
        "        kanji_images.append(img_tensor)\n",
        "        print(f\"     \u2022 Loaded: {kanji_info['kanji']} ({', '.join(kanji_info['meanings'][:2])})\")\n",
        "    \n",
        "    # Interpolate between them\n",
        "    save_path = Path(\"advanced_results\")\n",
        "    save_path.mkdir(exist_ok=True)\n",
        "    \n",
        "    for i in range(5):  # 5 interpolation steps\n",
        "        alpha = i / 4.0\n",
        "        interpolated = (1 - alpha) * kanji_images[0] + alpha * kanji_images[1]\n",
        "        \n",
        "        # Generate from interpolated\n",
        "        with torch.no_grad():\n",
        "            generated = model(interpolated)\n",
        "        \n",
        "        # Save\n",
        "        generated = (generated + 1) / 2\n",
        "        generated = torch.clamp(generated, 0, 1)\n",
        "        \n",
        "        img_array = generated[0].permute(1, 2, 0).numpy()\n",
        "        img = Image.fromarray((img_array * 255).astype(np.uint8))\n",
        "        \n",
        "        img_path = save_path / f\"interpolation_step_{i+1}.png\"\n",
        "        img.save(img_path)\n",
        "        print(f\"     \u2022 Interpolation {i+1}/5: {img_path}\")\n",
        "\n",
        "def test_model_analysis():\n",
        "    \"\"\"Analyze model behavior and capabilities\"\"\"\n",
        "    \n",
        "    print(f\"\\n\ud83d\udd0d Model Analysis...\")\n",
        "    \n",
        "    # Load model\n",
        "    model_path = \"quick_test_results/quick_test_epoch_2.pth\"\n",
        "    model = load_model(model_path)\n",
        "    \n",
        "    # Analyze model parameters\n",
        "    total_params = sum(p.numel() for p in model.parameters())\n",
        "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "    \n",
        "    print(f\"   \u2022 Total parameters: {total_params:,}\")\n",
        "    print(f\"   \u2022 Trainable parameters: {trainable_params:,}\")\n",
        "    \n",
        "    # Test with different input sizes\n",
        "    print(f\"   \u2022 Testing model with different inputs...\")\n",
        "    \n",
        "    test_inputs = [\n",
        "        torch.randn(1, 3, 64, 64),\n",
        "        torch.randn(1, 3, 32, 32),  # Smaller\n",
        "        torch.randn(1, 3, 128, 128),  # Larger\n",
        "    ]\n",
        "    \n",
        "    for i, test_input in enumerate(test_inputs):\n",
        "        try:\n",
        "            with torch.no_grad():\n",
        "                output = model(test_input)\n",
        "            print(f\"     \u2022 Input {test_input.shape} -> Output {output.shape} \u2705\")\n",
        "        except Exception as e:\n",
        "            print(f\"     \u2022 Input {test_input.shape} -> Error: {e} \u274c\")\n",
        "\n",
        "def main():\n",
        "    \"\"\"Main function\"\"\"\n",
        "    \n",
        "    print(\"\ud83c\udf8c Advanced Kanji Generation Testing\")\n",
        "    print(\"=\" * 50)\n",
        "    \n",
        "    # Check if model exists\n",
        "    model_path = Path(\"quick_test_results/quick_test_epoch_2.pth\")\n",
        "    if not model_path.exists():\n",
        "        print(\"\u274c Trained model not found! Please run quick_train_test.py first.\")\n",
        "        return\n",
        "    \n",
        "    # Load model\n",
        "    model = load_model(model_path)\n",
        "    print(f\"\u2705 Model loaded successfully\")\n",
        "    \n",
        "    # Run different tests\n",
        "    generate_with_different_strategies(model)\n",
        "    test_interpolation_between_kanji(model)\n",
        "    test_model_analysis()\n",
        "    \n",
        "    print(f\"\\n\ud83c\udf89 Advanced testing completed!\")\n",
        "    print(f\"   \u2022 Check advanced_results/ directory for generated images\")\n",
        "    print(f\"   \u2022 Different noise patterns tested\")\n",
        "    print(f\"   \u2022 Interpolation between Kanji tested\")\n",
        "    print(f\"   \u2022 Model analysis completed\")\n",
        "    \n",
        "    print(f\"\\n\ud83d\udcca Test Results Summary:\")\n",
        "    print(f\"   \u2022 Generated images with 4 different noise patterns\")\n",
        "    print(f\"   \u2022 Tested interpolation between existing Kanji\")\n",
        "    print(f\"   \u2022 Analyzed model capabilities and parameters\")\n",
        "    print(f\"   \u2022 All results saved in advanced_results/ directory\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "outputs": [],
      "execution_count": null
    }
  ],
  "metadata": {
    "anaconda-cloud": {},
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}