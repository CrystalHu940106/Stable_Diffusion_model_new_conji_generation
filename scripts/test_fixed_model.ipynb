{
  "cells": [
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#!/usr/bin/env python3\n",
        "\"\"\"\n",
        "Test script using the fixed VAE model\n",
        "\"\"\"\n",
        "\n",
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "from stable_diffusion_kanji import StableDiffusionPipeline\n",
        "\n",
        "def test_fixed_model():\n",
        "    \"\"\"Test generation with fixed VAE model\"\"\"\n",
        "    print(\"\ud83c\udf8c Testing Fixed VAE Model for Kanji Generation\")\n",
        "    print(\"=\" * 50)\n",
        "    \n",
        "    # Initialize pipeline with correct configuration\n",
        "    device = 'mps' if torch.backends.mps.is_available() else 'cpu'\n",
        "    print(f\"\ud83d\udd27 Using device: {device}\")\n",
        "    \n",
        "    from stable_diffusion_kanji import VAE, UNet2DConditionModel, DDPMScheduler\n",
        "    \n",
        "    vae = VAE(hidden_dims=[128, 256, 512, 1024]).to(device)\n",
        "    unet = UNet2DConditionModel(\n",
        "        model_channels=256,\n",
        "        num_res_blocks=3,\n",
        "        channel_mult=(1, 2, 4, 8),\n",
        "        attention_resolutions=(8,),\n",
        "        num_heads=16\n",
        "    ).to(device)\n",
        "    \n",
        "    pipeline = StableDiffusionPipeline(device=device)\n",
        "    pipeline.vae = vae\n",
        "    pipeline.unet = unet\n",
        "    \n",
        "    # Load fixed model\n",
        "    print(\"\ud83d\udcc2 Loading fixed VAE model...\")\n",
        "    try:\n",
        "        checkpoint = torch.load('fixed_vae_checkpoint_epoch_5.pth', map_location=device)\n",
        "        pipeline.vae.load_state_dict(checkpoint['vae_state_dict'])\n",
        "        pipeline.unet.load_state_dict(checkpoint['unet_state_dict'])\n",
        "        print(\"\u2705 Fixed model loaded successfully!\")\n",
        "    except Exception as e:\n",
        "        print(f\"\u274c Failed to load fixed model: {e}\")\n",
        "        return\n",
        "    \n",
        "    # Test concepts\n",
        "    concepts = [\"water\", \"future\"]\n",
        "    \n",
        "    for concept in concepts:\n",
        "        print(f\"\\n\ud83c\udf0a Generating Kanji for: {concept.upper()}\")\n",
        "        print(\"-\" * 30)\n",
        "        \n",
        "        try:\n",
        "            # Generate with different parameters\n",
        "            for guidance_scale in [7.0, 9.0, 11.0]:\n",
        "                print(f\"   Guidance Scale: {guidance_scale}\")\n",
        "                \n",
        "                # Generate image\n",
        "                image = pipeline.generate(\n",
        "                    prompt=f\"kanji character for {concept}, traditional calligraphy, black ink on white paper, high contrast, detailed strokes\",\n",
        "                    height=128,\n",
        "                    width=128,\n",
        "                    num_inference_steps=50,\n",
        "                    guidance_scale=guidance_scale,\n",
        "                    seed=42  # Fixed seed for reproducibility\n",
        "                )\n",
        "                \n",
        "                # Convert to PIL image\n",
        "                if isinstance(image, torch.Tensor):\n",
        "                    # Denormalize from [-1, 1] to [0, 1]\n",
        "                    image = (image + 1) / 2\n",
        "                    image = torch.clamp(image, 0, 1)\n",
        "                    \n",
        "                    # Convert to PIL\n",
        "                    image_np = image.squeeze(0).permute(1, 2, 0).cpu().numpy()\n",
        "                    image_pil = Image.fromarray((image_np * 255).astype(np.uint8))\n",
        "                else:\n",
        "                    image_pil = image\n",
        "                \n",
        "                # Save image\n",
        "                filename = f\"fixed_kanji_{concept}_{guidance_scale}.png\"\n",
        "                image_pil.save(filename)\n",
        "                print(f\"   \ud83d\udcbe Saved: {filename}\")\n",
        "                \n",
        "                # Display image\n",
        "                plt.figure(figsize=(6, 6))\n",
        "                plt.imshow(image_pil)\n",
        "                plt.title(f'Fixed Model: Kanji for \"{concept}\" (Guidance: {guidance_scale})')\n",
        "                plt.axis('off')\n",
        "                plt.show()\n",
        "                \n",
        "                # Print image statistics\n",
        "                img_array = np.array(image_pil)\n",
        "                if len(img_array.shape) == 3:\n",
        "                    gray = np.mean(img_array, axis=2)\n",
        "                else:\n",
        "                    gray = img_array\n",
        "                \n",
        "                print(f\"   \ud83d\udcca Image stats: size={image_pil.size}, mean={gray.mean():.2f}, std={gray.std():.2f}\")\n",
        "                \n",
        "        except Exception as e:\n",
        "            print(f\"   \u274c Generation failed: {e}\")\n",
        "            import traceback\n",
        "            traceback.print_exc()\n",
        "    \n",
        "    print(f\"\\n\ud83c\udf89 Fixed model test completed!\")\n",
        "    print(f\"\ud83d\udcc1 Generated images saved in current directory\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    test_fixed_model()\n"
      ],
      "outputs": [],
      "execution_count": null
    }
  ],
  "metadata": {
    "anaconda-cloud": {},
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}