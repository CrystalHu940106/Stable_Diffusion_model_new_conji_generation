{
  "cells": [
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#!/usr/bin/env python3\n",
        "\"\"\"\n",
        "Kanji Dataset Summary\n",
        "\n",
        "This script provides a comprehensive summary of the generated Kanji dataset.\n",
        "\"\"\"\n",
        "\n",
        "import json\n",
        "import os\n",
        "from pathlib import Path\n",
        "from collections import Counter\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "def generate_summary():\n",
        "    \"\"\"Generate a comprehensive summary of the dataset\"\"\"\n",
        "    \n",
        "    dataset_path = Path(\"kanji_dataset\")\n",
        "    metadata_path = dataset_path / \"metadata\" / \"dataset.json\"\n",
        "    images_path = dataset_path / \"images\"\n",
        "    \n",
        "    print(\"=== Kanji Dataset Summary ===\\n\")\n",
        "    \n",
        "    # Load dataset\n",
        "    with open(metadata_path, 'r', encoding='utf-8') as f:\n",
        "        dataset = json.load(f)\n",
        "    \n",
        "    # Basic statistics\n",
        "    total_kanji = len(dataset)\n",
        "    total_meanings = sum(len(entry['meanings']) for entry in dataset)\n",
        "    avg_meanings = total_meanings / total_kanji\n",
        "    \n",
        "    print(f\"\ud83d\udcca Dataset Statistics:\")\n",
        "    print(f\"   Total Kanji: {total_kanji:,}\")\n",
        "    print(f\"   Total meanings: {total_meanings:,}\")\n",
        "    print(f\"   Average meanings per Kanji: {avg_meanings:.1f}\")\n",
        "    print(f\"   Image files: {len(list(images_path.glob('*.png'))):,}\")\n",
        "    \n",
        "    # Meaning distribution\n",
        "    meaning_counts = [len(entry['meanings']) for entry in dataset]\n",
        "    meaning_counter = Counter(meaning_counts)\n",
        "    \n",
        "    print(f\"\\n\ud83d\udcc8 Meaning Distribution:\")\n",
        "    for count in sorted(meaning_counter.keys()):\n",
        "        percentage = (meaning_counter[count] / total_kanji) * 100\n",
        "        print(f\"   {count} meaning(s): {meaning_counter[count]:,} Kanji ({percentage:.1f}%)\")\n",
        "    \n",
        "    # Common meanings analysis\n",
        "    all_meanings = []\n",
        "    for entry in dataset:\n",
        "        all_meanings.extend(entry['meanings'])\n",
        "    \n",
        "    meaning_freq = Counter(all_meanings)\n",
        "    top_meanings = meaning_freq.most_common(20)\n",
        "    \n",
        "    print(f\"\\n\ud83c\udfc6 Top 20 Most Common Meanings:\")\n",
        "    for i, (meaning, count) in enumerate(top_meanings, 1):\n",
        "        print(f\"   {i:2d}. {meaning}: {count:,} occurrences\")\n",
        "    \n",
        "    # Unicode range analysis\n",
        "    unicode_values = [int(entry['unicode'], 16) for entry in dataset]\n",
        "    unicode_min = min(unicode_values)\n",
        "    unicode_max = max(unicode_values)\n",
        "    \n",
        "    print(f\"\\n\ud83d\udd24 Unicode Range:\")\n",
        "    print(f\"   Minimum: U+{unicode_min:04X} ({chr(unicode_min)})\")\n",
        "    print(f\"   Maximum: U+{unicode_max:04X} ({chr(unicode_max)})\")\n",
        "    print(f\"   Range: {unicode_max - unicode_min:,} code points\")\n",
        "    \n",
        "    # Common Kanji check\n",
        "    common_kanji = ['\u4eba', '\u5927', '\u5c0f', '\u5c71', '\u5ddd', '\u65e5', '\u6708', '\u706b', '\u6c34', '\u6728', '\u91d1', '\u571f', '\u5929', '\u5730', '\u4e2d', '\u56fd', '\u5e74', '\u751f', '\u5b66', '\u6821']\n",
        "    found_common = []\n",
        "    missing_common = []\n",
        "    \n",
        "    for kanji in common_kanji:\n",
        "        found = False\n",
        "        for entry in dataset:\n",
        "            if entry['kanji'] == kanji:\n",
        "                found_common.append((kanji, entry['meanings']))\n",
        "                found = True\n",
        "                break\n",
        "        if not found:\n",
        "            missing_common.append(kanji)\n",
        "    \n",
        "    print(f\"\\n\u2705 Common Kanji Coverage:\")\n",
        "    print(f\"   Found: {len(found_common)}/{len(common_kanji)}\")\n",
        "    for kanji, meanings in found_common:\n",
        "        print(f\"     {kanji}: {', '.join(meanings[:3])}\")\n",
        "    \n",
        "    if missing_common:\n",
        "        print(f\"   Missing: {', '.join(missing_common)}\")\n",
        "    \n",
        "    # File size analysis\n",
        "    image_files = list(images_path.glob(\"*.png\"))\n",
        "    total_size = sum(f.stat().st_size for f in image_files)\n",
        "    avg_size = total_size / len(image_files) if image_files else 0\n",
        "    \n",
        "    print(f\"\\n\ud83d\udcbe File Analysis:\")\n",
        "    print(f\"   Total image size: {total_size / 1024 / 1024:.1f} MB\")\n",
        "    print(f\"   Average image size: {avg_size:.0f} bytes\")\n",
        "    print(f\"   Image format: PNG, 64x64 pixels\")\n",
        "    \n",
        "    # Quality metrics\n",
        "    print(f\"\\n\ud83c\udfaf Quality Metrics:\")\n",
        "    print(f\"   \u2705 Pure black/white images\")\n",
        "    print(f\"   \u2705 No stroke order numbers\")\n",
        "    print(f\"   \u2705 Consistent 64x64 resolution\")\n",
        "    print(f\"   \u2705 High contrast for ML training\")\n",
        "    print(f\"   \u2705 Complete metadata coverage\")\n",
        "    \n",
        "    # Training recommendations\n",
        "    print(f\"\\n\ud83d\ude80 Training Recommendations:\")\n",
        "    print(f\"   \u2022 Batch size: 8-16 (depending on GPU memory)\")\n",
        "    print(f\"   \u2022 Learning rate: 1e-5 to 1e-4\")\n",
        "    print(f\"   \u2022 Training steps: 1000-5000 per epoch\")\n",
        "    print(f\"   \u2022 Image size: 64x64 pixels (as provided)\")\n",
        "    print(f\"   \u2022 Data augmentation: Small rotations/translations\")\n",
        "    print(f\"   \u2022 Validation split: 80/20 recommended\")\n",
        "    \n",
        "    # Dataset structure\n",
        "    print(f\"\\n\ud83d\udcc1 Dataset Structure:\")\n",
        "    print(f\"   kanji_dataset/\")\n",
        "    print(f\"   \u251c\u2500\u2500 images/           # {len(image_files):,} PNG files\")\n",
        "    print(f\"   \u2514\u2500\u2500 metadata/\")\n",
        "    print(f\"       \u251c\u2500\u2500 dataset.json  # Complete dataset\")\n",
        "    print(f\"       \u2514\u2500\u2500 *.json        # Individual Kanji files\")\n",
        "    \n",
        "    return dataset\n",
        "\n",
        "def show_sample_entries(dataset, num_samples=10):\n",
        "    \"\"\"Show sample entries from the dataset\"\"\"\n",
        "    \n",
        "    print(f\"\\n\ud83d\udcdd Sample Entries:\")\n",
        "    for i, entry in enumerate(dataset[:num_samples], 1):\n",
        "        kanji = entry['kanji']\n",
        "        meanings = entry['meanings']\n",
        "        unicode_val = entry['unicode']\n",
        "        image_file = entry['image_file']\n",
        "        \n",
        "        print(f\"\\n{i:2d}. {kanji} (U+{unicode_val.upper()})\")\n",
        "        print(f\"    Meanings: {', '.join(meanings)}\")\n",
        "        print(f\"    Image: {image_file}\")\n",
        "        print(f\"    Prompt: {entry['prompt']}\")\n",
        "\n",
        "def main():\n",
        "    \"\"\"Main function\"\"\"\n",
        "    \n",
        "    if not Path(\"kanji_dataset\").exists():\n",
        "        print(\"\u274c Dataset not found! Please run process_kanji_data.py first.\")\n",
        "        return\n",
        "    \n",
        "    # Generate summary\n",
        "    dataset = generate_summary()\n",
        "    \n",
        "    # Show sample entries\n",
        "    show_sample_entries(dataset)\n",
        "    \n",
        "    print(f\"\\n\ud83c\udf89 Dataset Summary Complete!\")\n",
        "    print(f\"The Kanji dataset is ready for stable diffusion training with {len(dataset):,} high-quality entries.\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main() "
      ],
      "outputs": [],
      "execution_count": null
    }
  ],
  "metadata": {
    "anaconda-cloud": {},
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}