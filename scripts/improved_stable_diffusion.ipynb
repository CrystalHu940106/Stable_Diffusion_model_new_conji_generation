{
 "cells": [
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "æ”¹è¿›çš„Stable Diffusionå®ç°\n",
    "å€Ÿé‰´å®˜æ–¹CompVis/stable-diffusionçš„æœ€ä½³å®è·µ\n",
    "\"\"\"\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from transformers import CLIPTokenizer, CLIPTextModel\n",
    "import math\n",
    "from typing import Optional, Union, Tuple\n",
    "import numpy as np\n",
    "\n",
    "class ImprovedVAE(nn.Module):\n",
    "    \"\"\"\n",
    "    æ”¹è¿›çš„VAEå®ç°ï¼Œå€Ÿé‰´å®˜æ–¹æ¶æ„\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels=3, latent_channels=4, hidden_dims=[128, 256, 512, 1024]):\n",
    "        super().__init__()\n",
    "        self.latent_channels = latent_channels\n",
    "        \n",
    "        # Encoder - usingmoreæ·±ofç½‘ç»œ\n",
    "        encoder_layers = []\n",
    "        in_ch = in_channels\n",
    "        for h_dim in hidden_dims:\n",
    "            # è®¡ç®—åˆé€‚ofGroupNormç»„æ•°\n",
    "            num_groups = min(32, h_dim)\n",
    "            while h_dim % num_groups != 0 and num_groups > 1:\n",
    "                num_groups -= 1\n",
    "            \n",
    "            encoder_layers.extend([\n",
    "                nn.Conv2d(in_ch, h_dim, kernel_size=3, stride=2, padding=1),\n",
    "                nn.GroupNorm(num_groups, h_dim),  # usingGroupNormreplaceBatchNorm\n",
    "                nn.SiLU()  # usingSiLUreplaceLeakyReLU\n",
    "            ])\n",
    "            in_ch = h_dim\n",
    "        \n",
    "        # Final encoding layer\n",
    "        final_channels = latent_channels * 2\n",
    "        num_groups = min(32, final_channels)\n",
    "        while final_channels % num_groups != 0 and num_groups > 1:\n",
    "            num_groups -= 1\n",
    "        \n",
    "        encoder_layers.extend([\n",
    "            nn.Conv2d(hidden_dims[-1], final_channels, kernel_size=3, padding=1),\n",
    "            nn.GroupNorm(num_groups, final_channels)\n",
    "        ])\n",
    "        \n",
    "        self.encoder = nn.Sequential(*encoder_layers)\n",
    "        \n",
    "        # Decoder - ensureç²¾ç¡®of128x128output\n",
    "        decoder_layers = []\n",
    "        in_ch = latent_channels\n",
    "        \n",
    "        # usinghidden_dimsofååºè¿›è¡Œoné‡‡æ ·\n",
    "        hidden_dims_rev = hidden_dims[::-1]\n",
    "        \n",
    "        for i, h_dim in enumerate(hidden_dims_rev):\n",
    "            # è®¡ç®—åˆé€‚ofGroupNormç»„æ•°\n",
    "            num_groups = min(32, h_dim)\n",
    "            while h_dim % num_groups != 0 and num_groups > 1:\n",
    "                num_groups -= 1\n",
    "            \n",
    "            decoder_layers.extend([\n",
    "                nn.ConvTranspose2d(in_ch, h_dim, kernel_size=4, stride=2, padding=1),\n",
    "                nn.GroupNorm(num_groups, h_dim),\n",
    "                nn.SiLU()\n",
    "            ])\n",
    "            in_ch = h_dim\n",
    "        \n",
    "        # æœ€ç»ˆoutputå±‚\n",
    "        decoder_layers.extend([\n",
    "            nn.Conv2d(in_ch, in_channels, kernel_size=3, stride=1, padding=1),\n",
    "            nn.Tanh()\n",
    "        ])\n",
    "        \n",
    "        self.decoder = nn.Sequential(*decoder_layers)\n",
    "        \n",
    "    def encode(self, x):\n",
    "        # ensureè¾“å…¥æ˜¯128x128\n",
    "        if x.shape[-1] != 128:\n",
    "            x = F.interpolate(x, size=(128, 128), mode='bilinear', align_corners=False)\n",
    "        \n",
    "        # ç¼–ç toæ½œinç©ºé—´\n",
    "        encoded = self.encoder(x)\n",
    "        mu, logvar = torch.chunk(encoded, 2, dim=1)\n",
    "        \n",
    "        # KLæ•£åº¦æŸå¤±\n",
    "        kl_loss = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "        \n",
    "        # é‡å‚æ•°åŒ–æŠ€å·§\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        z = mu + eps * std\n",
    "        \n",
    "        return z, mu, logvar, kl_loss\n",
    "    \n",
    "    def decode(self, z):\n",
    "        return self.decoder(z)\n",
    "\n",
    "class ImprovedCrossAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    æ”¹è¿›çš„äº¤å‰æ³¨æ„åŠ›å®ç°ï¼Œå€Ÿé‰´å®˜æ–¹ç‰ˆæœ¬\n",
    "    \"\"\"\n",
    "    def __init__(self, query_dim, context_dim=None, heads=8, dim_head=64, dropout=0.0):\n",
    "        super().__init__()\n",
    "        inner_dim = dim_head * heads\n",
    "        context_dim = context_dim if context_dim is not None else query_dim\n",
    "        \n",
    "        self.scale = dim_head ** -0.5\n",
    "        self.heads = heads\n",
    "        \n",
    "        self.to_q = nn.Linear(query_dim, inner_dim, bias=False)\n",
    "        self.to_k = nn.Linear(context_dim, inner_dim, bias=False)\n",
    "        self.to_v = nn.Linear(context_dim, inner_dim, bias=False)\n",
    "        \n",
    "        self.to_out = nn.Sequential(\n",
    "            nn.Linear(inner_dim, query_dim),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x, context=None):\n",
    "        context = context if context is not None else x\n",
    "        \n",
    "        q = self.to_q(x)\n",
    "        k = self.to_k(context)\n",
    "        v = self.to_v(context)\n",
    "        \n",
    "        # é‡å¡‘ä¸ºå¤šå¤´æ³¨æ„åŠ›\n",
    "        q = q.view(q.shape[0], -1, self.heads, q.shape[-1] // self.heads).transpose(1, 2)\n",
    "        k = k.view(k.shape[0], -1, self.heads, k.shape[-1] // self.heads).transpose(1, 2)\n",
    "        v = v.view(v.shape[0], -1, self.heads, v.shape[-1] // self.heads).transpose(1, 2)\n",
    "        \n",
    "        # è®¡ç®—æ³¨æ„åŠ›\n",
    "        scores = torch.matmul(q, k.transpose(-2, -1)) * self.scale\n",
    "        attn = F.softmax(scores, dim=-1)\n",
    "        \n",
    "        # åº”ç”¨æ³¨æ„åŠ›\n",
    "        out = torch.matmul(attn, v)\n",
    "        out = out.transpose(1, 2).contiguous().view(out.shape[0], -1, out.shape[-1] * self.heads)\n",
    "        \n",
    "        return self.to_out(out)\n",
    "\n",
    "class ImprovedResBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    æ”¹è¿›çš„æ®‹å·®å—ï¼Œå€Ÿé‰´å®˜æ–¹å®ç°\n",
    "    \"\"\"\n",
    "    def __init__(self, channels, time_dim, dropout=0.0):\n",
    "        super().__init__()\n",
    "        \n",
    "        # åŠ¨æ€è®¡ç®—GroupNormofç»„æ•°ï¼Œensurechannelscanè¢«num_groupsæ•´é™¤\n",
    "        if channels >= 32:\n",
    "            num_groups = min(32, channels // (channels // 32))\n",
    "        elif channels >= 16:\n",
    "            num_groups = min(16, channels // (channels // 16))\n",
    "        elif channels >= 8:\n",
    "            num_groups = min(8, channels // (channels // 8))\n",
    "        elif channels >= 4:\n",
    "            num_groups = min(4, channels // (channels // 4))\n",
    "        else:\n",
    "            num_groups = 1\n",
    "        \n",
    "        # ensurenum_groupscanæ•´é™¤channels\n",
    "        while channels % num_groups != 0 and num_groups > 1:\n",
    "            num_groups -= 1\n",
    "        \n",
    "        self.time_mlp = nn.Sequential(\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(time_dim, channels)\n",
    "        )\n",
    "        \n",
    "        self.block1 = nn.Sequential(\n",
    "            nn.GroupNorm(num_groups, channels),\n",
    "            nn.SiLU(),\n",
    "            nn.Conv2d(channels, channels, 3, padding=1)\n",
    "        )\n",
    "        \n",
    "        self.block2 = nn.Sequential(\n",
    "            nn.GroupNorm(num_groups, channels),\n",
    "            nn.SiLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Conv2d(channels, channels, 3, padding=1)\n",
    "        )\n",
    "        \n",
    "        # æ—¶é—´åµŒå…¥æŠ•å½±\n",
    "        self.time_proj = nn.Linear(time_dim, channels)\n",
    "        \n",
    "    def forward(self, x, time_emb):\n",
    "        h = self.block1(x)\n",
    "        \n",
    "        # æ—¶é—´åµŒå…¥å¤„ç†\n",
    "        time_emb = self.time_proj(time_emb)\n",
    "        time_emb = time_emb.view(x.shape[0], -1, 1, 1)\n",
    "        h = h + time_emb\n",
    "        \n",
    "        h = self.block2(h)\n",
    "        return h + x\n",
    "\n",
    "class ImprovedUNet2DConditionModel(nn.Module):\n",
    "    \"\"\"\n",
    "    æ”¹è¿›çš„UNetå®ç°ï¼Œå€Ÿé‰´å®˜æ–¹æ¶æ„\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels=4, out_channels=4, model_channels=128, num_res_blocks=2, \n",
    "                 attention_resolutions=(8, 16), dropout=0.1, channel_mult=(1, 2, 4, 8), \n",
    "                 conv_resample=True, num_heads=8, context_dim=512):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.in_channels = in_channels\n",
    "        self.model_channels = model_channels\n",
    "        self.num_res_blocks = num_res_blocks\n",
    "        self.attention_resolutions = attention_resolutions\n",
    "        self.dropout = dropout\n",
    "        self.channel_mult = channel_mult\n",
    "        self.conv_resample = conv_resample\n",
    "        self.num_heads = num_heads\n",
    "        self.context_dim = context_dim\n",
    "        \n",
    "        # æ—¶é—´åµŒå…¥ - usingmoreæ·±ofç½‘ç»œ\n",
    "        time_embed_dim = model_channels * 4\n",
    "        self.time_embedding = nn.Sequential(\n",
    "            nn.Linear(1, time_embed_dim),\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(time_embed_dim, time_embed_dim),\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(time_embed_dim, time_embed_dim)\n",
    "        )\n",
    "        \n",
    "        # è¾“å…¥æŠ•å½±\n",
    "        self.input_blocks = nn.ModuleList([\n",
    "            nn.Conv2d(in_channels, model_channels, kernel_size=3, padding=1)\n",
    "        ])\n",
    "        \n",
    "        # ä¸‹é‡‡æ ·å—\n",
    "        input_block_chans = [model_channels]\n",
    "        ch = model_channels\n",
    "        \n",
    "        for level, mult in enumerate(channel_mult):\n",
    "            # addResBlock\n",
    "            for _ in range(num_res_blocks):\n",
    "                self.input_blocks.append(\n",
    "                    nn.ModuleList([ImprovedResBlock(ch, time_embed_dim, dropout)])\n",
    "                )\n",
    "                input_block_chans.append(ch)\n",
    "            \n",
    "            # addCrossAttention\n",
    "            if level in attention_resolutions:\n",
    "                self.input_blocks.append(\n",
    "                    nn.ModuleList([ImprovedCrossAttention(ch, context_dim, num_heads, dropout=dropout)])\n",
    "                )\n",
    "                input_block_chans.append(ch)\n",
    "            \n",
    "            # ä¸‹é‡‡æ ·\n",
    "            if level < len(channel_mult) - 1:\n",
    "                ch = mult * model_channels\n",
    "                self.input_blocks.append(\n",
    "                    nn.ModuleList([nn.Conv2d(input_block_chans[-1], ch, 3, stride=2, padding=1)])\n",
    "                )\n",
    "                input_block_chans.append(ch)\n",
    "        \n",
    "        # ä¸­é—´å—\n",
    "        self.middle_block = nn.ModuleList([\n",
    "            ImprovedResBlock(ch, time_embed_dim, dropout),\n",
    "            ImprovedCrossAttention(ch, context_dim, num_heads, dropout=dropout),\n",
    "            ImprovedResBlock(ch, time_embed_dim, dropout)\n",
    "        ])\n",
    "        \n",
    "        # outputå—\n",
    "        self.output_blocks = nn.ModuleList([])\n",
    "        for level, mult in list(enumerate(channel_mult))[::-1]:\n",
    "            # oné‡‡æ ·\n",
    "            if level < len(channel_mult) - 1:\n",
    "                self.output_blocks.append(\n",
    "                    nn.ModuleList([nn.ConvTranspose2d(ch, ch//2, 4, stride=2, padding=1)])\n",
    "                )\n",
    "                ch = ch // 2\n",
    "            \n",
    "            # addResBlock\n",
    "            for _ in range(num_res_blocks + 1):\n",
    "                self.output_blocks.append(\n",
    "                    nn.ModuleList([ImprovedResBlock(ch, time_embed_dim, dropout)])\n",
    "                )\n",
    "            \n",
    "            # addCrossAttention\n",
    "            if level in attention_resolutions:\n",
    "                self.output_blocks.append(\n",
    "                    nn.ModuleList([ImprovedCrossAttention(ch, context_dim, num_heads, dropout=dropout)])\n",
    "                )\n",
    "        \n",
    "        # outputæŠ•å½±\n",
    "        if ch >= 32:\n",
    "            num_groups = 32\n",
    "        elif ch >= 16:\n",
    "            num_groups = 16\n",
    "        elif ch >= 8:\n",
    "            num_groups = 8\n",
    "        elif ch >= 4:\n",
    "            num_groups = 4\n",
    "        else:\n",
    "            num_groups = 1\n",
    "        \n",
    "        self.out = nn.Sequential(\n",
    "            nn.GroupNorm(num_groups, ch),\n",
    "            nn.SiLU(),\n",
    "            nn.Conv2d(ch, out_channels, kernel_size=3, padding=1)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x, timesteps, context=None):\n",
    "        # æ—¶é—´åµŒå…¥\n",
    "        t = self.time_embedding(timesteps.unsqueeze(-1).float())\n",
    "        if t.dim() == 1:\n",
    "            t = t.unsqueeze(0)\n",
    "        \n",
    "        # è¾“å…¥å—\n",
    "        h = x\n",
    "        hs = []\n",
    "        for module in self.input_blocks:\n",
    "            if isinstance(module, nn.ModuleList):\n",
    "                # å¤„ç†ModuleListä¸­ofæ¨¡å—\n",
    "                for submodule in module:\n",
    "                    if isinstance(submodule, ImprovedCrossAttention):\n",
    "                        h = submodule(h, context)\n",
    "                    elif isinstance(submodule, ImprovedResBlock):\n",
    "                        h = submodule(h, t)\n",
    "                    else:\n",
    "                        h = submodule(h)\n",
    "            else:\n",
    "                # ç›´æ¥å¤„ç†å•ä¸ªæ¨¡å—\n",
    "                if isinstance(module, ImprovedCrossAttention):\n",
    "                    h = module(h, context)\n",
    "                elif isinstance(module, ImprovedResBlock):\n",
    "                    h = module(h, t)\n",
    "                else:\n",
    "                    h = module(h)\n",
    "            hs.append(h)\n",
    "        \n",
    "        # ä¸­é—´å—\n",
    "        for module in self.middle_block:\n",
    "            if isinstance(module, ImprovedCrossAttention):\n",
    "                h = module(h, context)\n",
    "            else:\n",
    "                h = module(h, t)\n",
    "        \n",
    "        # outputå—\n",
    "        for module in self.output_blocks:\n",
    "            if isinstance(module, nn.ModuleList):\n",
    "                # å¤„ç†ModuleListä¸­ofæ¨¡å—\n",
    "                for submodule in module:\n",
    "                    if isinstance(submodule, ImprovedCrossAttention):\n",
    "                        h = submodule(h, context)\n",
    "                    elif isinstance(submodule, ImprovedResBlock):\n",
    "                        h = submodule(h, t)\n",
    "                    else:\n",
    "                        h = submodule(h)\n",
    "            else:\n",
    "                # ç›´æ¥å¤„ç†å•ä¸ªæ¨¡å—\n",
    "                if isinstance(module, ImprovedCrossAttention):\n",
    "                    h = module(h, context)\n",
    "                elif isinstance(module, ImprovedResBlock):\n",
    "                    h = module(h, t)\n",
    "                else:\n",
    "                    h = module(h)\n",
    "            \n",
    "            # è·³è·ƒè¿æ¥\n",
    "            if hs:\n",
    "                h = torch.cat([h, hs.pop()], dim=1)\n",
    "        \n",
    "        return self.out(h)\n",
    "\n",
    "class ImprovedDDPMScheduler:\n",
    "    \"\"\"\n",
    "    æ”¹è¿›çš„DDPMè°ƒåº¦å™¨ï¼Œå€Ÿé‰´å®˜æ–¹å®ç°\n",
    "    \"\"\"\n",
    "    def __init__(self, num_train_timesteps=1000, beta_start=0.0001, beta_end=0.02):\n",
    "        self.num_train_timesteps = num_train_timesteps\n",
    "        \n",
    "        # çº¿æ€§noisescheduling\n",
    "        self.betas = torch.linspace(beta_start, beta_end, num_train_timesteps)\n",
    "        self.alphas = 1.0 - self.betas\n",
    "        self.alphas_cumprod = torch.cumprod(self.alphas, dim=0)\n",
    "        self.alphas_cumprod_prev = torch.cat([torch.tensor([1.0]), self.alphas_cumprod[:-1]])\n",
    "        \n",
    "        # è®¡ç®—noisepredictionofcoefficient\n",
    "        self.sqrt_alphas_cumprod = torch.sqrt(self.alphas_cumprod)\n",
    "        self.sqrt_one_minus_alphas_cumprod = torch.sqrt(1.0 - self.alphas_cumprod)\n",
    "        \n",
    "    def add_noise(self, original_samples, noise, timesteps):\n",
    "        \"\"\"addnoisetoåŸå§‹æ ·æœ¬\"\"\"\n",
    "        sqrt_alpha = self.sqrt_alphas_cumprod[timesteps].view(-1, 1, 1, 1)\n",
    "        sqrt_one_minus_alpha = self.sqrt_one_minus_alphas_cumprod[timesteps].view(-1, 1, 1, 1)\n",
    "        \n",
    "        return sqrt_alpha * original_samples + sqrt_one_minus_alpha * noise\n",
    "    \n",
    "    def step(self, model_output, timestep, sample):\n",
    "        \"\"\"å»å™ªæ­¥éª¤\"\"\"\n",
    "        alpha = self.alphas_cumprod[timestep].view(-1, 1, 1, 1)\n",
    "        alpha_prev = self.alphas_cumprod_prev[timestep].view(-1, 1, 1, 1)\n",
    "        \n",
    "        # predictionx0\n",
    "        pred_original_sample = (sample - torch.sqrt(1 - alpha) * model_output) / torch.sqrt(alpha)\n",
    "        \n",
    "        # predictionå‰aæ ·æœ¬\n",
    "        pred_sample_direction = torch.sqrt(1 - alpha_prev) * model_output\n",
    "        pred_prev_sample = torch.sqrt(alpha_prev) * pred_original_sample + pred_sample_direction\n",
    "        \n",
    "        return pred_prev_sample\n",
    "    \n",
    "    def set_timesteps(self, num_inference_steps):\n",
    "        \"\"\"setæ¨ç†æ—¶é—´æ­¥\"\"\"\n",
    "        self.num_inference_steps = num_inference_steps\n",
    "        step_ratio = self.num_train_timesteps // num_inference_steps\n",
    "        timesteps = (torch.arange(0, num_inference_steps) * step_ratio).flip(0)\n",
    "        return timesteps\n",
    "\n",
    "class ImprovedStableDiffusionPipeline:\n",
    "    \"\"\"\n",
    "    æ”¹è¿›çš„Stable Diffusion Pipelineï¼Œå€Ÿé‰´å®˜æ–¹æœ€ä½³å®è·µ\n",
    "    \"\"\"\n",
    "    def __init__(self, device='cuda' if torch.cuda.is_available() else 'cpu'):\n",
    "        self.device = device\n",
    "        \n",
    "        # initializationç»„ä»¶\n",
    "        self.vae = ImprovedVAE().to(device)\n",
    "        self.unet = ImprovedUNet2DConditionModel(\n",
    "            in_channels=4,\n",
    "            out_channels=4,\n",
    "            model_channels=128,\n",
    "            channel_mult=(1, 2, 4, 8),\n",
    "            attention_resolutions=(8, 16),\n",
    "            context_dim=512\n",
    "        ).to(device)\n",
    "        self.scheduler = ImprovedDDPMScheduler()\n",
    "        \n",
    "        # CLIPæ–‡æœ¬ç¼–ç å™¨\n",
    "        self.tokenizer = CLIPTokenizer.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "        self.text_encoder = CLIPTextModel.from_pretrained(\"openai/clip-vit-base-patch32\").to(device)\n",
    "        \n",
    "        # setä¸ºè¯„ä¼°æ¨¡å¼\n",
    "        self.text_encoder.eval()\n",
    "        self.vae.eval()\n",
    "        \n",
    "    def _encode_prompt(self, prompt):\n",
    "        \"\"\"ç¼–ç æ–‡æœ¬æç¤º\"\"\"\n",
    "        tokens = self.tokenizer(prompt, padding=True, return_tensors=\"pt\").to(self.device)\n",
    "        with torch.no_grad():\n",
    "            text_embeddings = self.text_encoder(**tokens).last_hidden_state\n",
    "        return text_embeddings\n",
    "    \n",
    "    def _parse_kanji_prompt(self, prompt):\n",
    "        \"\"\"è§£ææ±‰å­—æç¤ºï¼Œusingmoreè¯¦ç»†ofæè¿°\"\"\"\n",
    "        base_prompt = f\"kanji character representing {prompt}, traditional calligraphy style, black ink on white paper, high contrast, detailed strokes, clear lines, professional quality, artistic interpretation\"\n",
    "        return base_prompt\n",
    "    \n",
    "    def generate(self, prompt, height=128, width=128, num_inference_steps=50, \n",
    "                guidance_scale=7.5, seed=None):\n",
    "        \"\"\"generationimageï¼Œusingå®˜æ–¹æ¨èofå‚æ•°\"\"\"\n",
    "        \n",
    "        # setrandom seed\n",
    "        if seed is not None:\n",
    "            torch.manual_seed(seed)\n",
    "            torch.cuda.manual_seed(seed)\n",
    "        \n",
    "        # ç¼–ç æç¤º\n",
    "        text_embeddings = self._encode_prompt(self._parse_kanji_prompt(prompt))\n",
    "        \n",
    "        # initializationæ½œinå˜é‡\n",
    "        latent_height = height // 8\n",
    "        latent_width = width // 8\n",
    "        latents = torch.randn(1, 4, latent_height, latent_width, device=self.device)\n",
    "        \n",
    "        # setæ—¶é—´æ­¥\n",
    "        timesteps = self.scheduler.set_timesteps(num_inference_steps)\n",
    "        timesteps = timesteps.to(self.device)\n",
    "        \n",
    "        # æ”¹è¿›ofå»å™ªå¾ªç¯\n",
    "        for i, t in enumerate(timesteps):\n",
    "            # æ‰©å±•æ½œinå˜é‡ç”¨äºæ‰¹å¤„ç†\n",
    "            latent_model_input = torch.cat([latents] * 2)\n",
    "            t_expanded = t.expand(2)\n",
    "            \n",
    "            # predictionnoise\n",
    "            with torch.no_grad():\n",
    "                noise_pred = self.unet(latent_model_input, t_expanded, text_embeddings)\n",
    "            \n",
    "            # executedclassifier-free guidance\n",
    "            noise_pred_uncond, noise_pred_text = noise_pred.chunk(2)\n",
    "            \n",
    "            # usingå®˜æ–¹æ¨èofguidance scale\n",
    "            guidance_scale = torch.clamp(torch.tensor(guidance_scale), min=1.0, max=20.0)\n",
    "            noise_pred = noise_pred_uncond + guidance_scale * (noise_pred_text - noise_pred_uncond)\n",
    "            \n",
    "            # è®¡ç®—å‰aæ ·æœ¬\n",
    "            latents = self.scheduler.step(noise_pred, t, latents)\n",
    "        \n",
    "        # decodeæ½œinå˜é‡\n",
    "        with torch.no_grad():\n",
    "            image = self.vae.decode(latents)\n",
    "        \n",
    "        return image\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"ğŸŒ æ”¹è¿›ofStable Diffusionimplementation\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # testmodel\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    print(f\"ğŸ”§ usingè®¾å¤‡: {device}\")\n",
    "    \n",
    "    try:\n",
    "        pipeline = ImprovedStableDiffusionPipeline(device=device)\n",
    "        print(\"âœ… modelinitializationæˆåŠŸ\")\n",
    "        \n",
    "        # testgeneration\n",
    "        print(\"ğŸŒŠ testgeneration...\")\n",
    "        result = pipeline.generate(\n",
    "            \"water\",\n",
    "            height=128,\n",
    "            width=128,\n",
    "            num_inference_steps=50,\n",
    "            guidance_scale=7.5,\n",
    "            seed=42\n",
    "        )\n",
    "        print(\"âœ… generationcomplete\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ é”™è¯¯: {e}\")\n"
   ],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}