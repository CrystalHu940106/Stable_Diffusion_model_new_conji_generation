{
  "cells": [
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#!/usr/bin/env python3\n",
        "\"\"\"\n",
        "\u6539\u8fdb\u7684Stable Diffusion\u5b9e\u73b0\n",
        "\u501f\u9274\u5b98\u65b9CompVis/stable-diffusion\u7684\u6700\u4f73\u5b9e\u8df5\n",
        "\"\"\"\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from transformers import CLIPTokenizer, CLIPTextModel\n",
        "import math\n",
        "from typing import Optional, Union, Tuple\n",
        "import numpy as np\n",
        "\n",
        "class ImprovedVAE(nn.Module):\n",
        "    \"\"\"\n",
        "    \u6539\u8fdb\u7684VAE\u5b9e\u73b0\uff0c\u501f\u9274\u5b98\u65b9\u67b6\u6784\n",
        "    \"\"\"\n",
        "    def __init__(self, in_channels=3, latent_channels=4, hidden_dims=[128, 256, 512, 1024]):\n",
        "        super().__init__()\n",
        "        self.latent_channels = latent_channels\n",
        "        \n",
        "        # Encoder - \u4f7f\u7528\u66f4\u6df1\u7684\u7f51\u7edc\n",
        "        encoder_layers = []\n",
        "        in_ch = in_channels\n",
        "        for h_dim in hidden_dims:\n",
        "            # \u8ba1\u7b97\u5408\u9002\u7684GroupNorm\u7ec4\u6570\n",
        "            num_groups = min(32, h_dim)\n",
        "            while h_dim % num_groups != 0 and num_groups > 1:\n",
        "                num_groups -= 1\n",
        "            \n",
        "            encoder_layers.extend([\n",
        "                nn.Conv2d(in_ch, h_dim, kernel_size=3, stride=2, padding=1),\n",
        "                nn.GroupNorm(num_groups, h_dim),  # \u4f7f\u7528GroupNorm\u66ff\u4ee3BatchNorm\n",
        "                nn.SiLU()  # \u4f7f\u7528SiLU\u66ff\u4ee3LeakyReLU\n",
        "            ])\n",
        "            in_ch = h_dim\n",
        "        \n",
        "        # Final encoding layer\n",
        "        final_channels = latent_channels * 2\n",
        "        num_groups = min(32, final_channels)\n",
        "        while final_channels % num_groups != 0 and num_groups > 1:\n",
        "            num_groups -= 1\n",
        "        \n",
        "        encoder_layers.extend([\n",
        "            nn.Conv2d(hidden_dims[-1], final_channels, kernel_size=3, padding=1),\n",
        "            nn.GroupNorm(num_groups, final_channels)\n",
        "        ])\n",
        "        \n",
        "        self.encoder = nn.Sequential(*encoder_layers)\n",
        "        \n",
        "        # Decoder - \u786e\u4fdd\u7cbe\u786e\u7684128x128\u8f93\u51fa\n",
        "        decoder_layers = []\n",
        "        in_ch = latent_channels\n",
        "        \n",
        "        # \u4f7f\u7528hidden_dims\u7684\u53cd\u5e8f\u8fdb\u884c\u4e0a\u91c7\u6837\n",
        "        hidden_dims_rev = hidden_dims[::-1]\n",
        "        \n",
        "        for i, h_dim in enumerate(hidden_dims_rev):\n",
        "            # \u8ba1\u7b97\u5408\u9002\u7684GroupNorm\u7ec4\u6570\n",
        "            num_groups = min(32, h_dim)\n",
        "            while h_dim % num_groups != 0 and num_groups > 1:\n",
        "                num_groups -= 1\n",
        "            \n",
        "            decoder_layers.extend([\n",
        "                nn.ConvTranspose2d(in_ch, h_dim, kernel_size=4, stride=2, padding=1),\n",
        "                nn.GroupNorm(num_groups, h_dim),\n",
        "                nn.SiLU()\n",
        "            ])\n",
        "            in_ch = h_dim\n",
        "        \n",
        "        # \u6700\u7ec8\u8f93\u51fa\u5c42\n",
        "        decoder_layers.extend([\n",
        "            nn.Conv2d(in_ch, in_channels, kernel_size=3, stride=1, padding=1),\n",
        "            nn.Tanh()\n",
        "        ])\n",
        "        \n",
        "        self.decoder = nn.Sequential(*decoder_layers)\n",
        "        \n",
        "    def encode(self, x):\n",
        "        # \u786e\u4fdd\u8f93\u5165\u662f128x128\n",
        "        if x.shape[-1] != 128:\n",
        "            x = F.interpolate(x, size=(128, 128), mode='bilinear', align_corners=False)\n",
        "        \n",
        "        # \u7f16\u7801\u5230\u6f5c\u5728\u7a7a\u95f4\n",
        "        encoded = self.encoder(x)\n",
        "        mu, logvar = torch.chunk(encoded, 2, dim=1)\n",
        "        \n",
        "        # KL\u6563\u5ea6\u635f\u5931\n",
        "        kl_loss = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
        "        \n",
        "        # \u91cd\u53c2\u6570\u5316\u6280\u5de7\n",
        "        std = torch.exp(0.5 * logvar)\n",
        "        eps = torch.randn_like(std)\n",
        "        z = mu + eps * std\n",
        "        \n",
        "        return z, mu, logvar, kl_loss\n",
        "    \n",
        "    def decode(self, z):\n",
        "        return self.decoder(z)\n",
        "\n",
        "class ImprovedCrossAttention(nn.Module):\n",
        "    \"\"\"\n",
        "    \u6539\u8fdb\u7684\u4ea4\u53c9\u6ce8\u610f\u529b\u5b9e\u73b0\uff0c\u501f\u9274\u5b98\u65b9\u7248\u672c\n",
        "    \"\"\"\n",
        "    def __init__(self, query_dim, context_dim=None, heads=8, dim_head=64, dropout=0.0):\n",
        "        super().__init__()\n",
        "        inner_dim = dim_head * heads\n",
        "        context_dim = context_dim if context_dim is not None else query_dim\n",
        "        \n",
        "        self.scale = dim_head ** -0.5\n",
        "        self.heads = heads\n",
        "        \n",
        "        self.to_q = nn.Linear(query_dim, inner_dim, bias=False)\n",
        "        self.to_k = nn.Linear(context_dim, inner_dim, bias=False)\n",
        "        self.to_v = nn.Linear(context_dim, inner_dim, bias=False)\n",
        "        \n",
        "        self.to_out = nn.Sequential(\n",
        "            nn.Linear(inner_dim, query_dim),\n",
        "            nn.Dropout(dropout)\n",
        "        )\n",
        "        \n",
        "    def forward(self, x, context=None):\n",
        "        context = context if context is not None else x\n",
        "        \n",
        "        q = self.to_q(x)\n",
        "        k = self.to_k(context)\n",
        "        v = self.to_v(context)\n",
        "        \n",
        "        # \u91cd\u5851\u4e3a\u591a\u5934\u6ce8\u610f\u529b\n",
        "        q = q.view(q.shape[0], -1, self.heads, q.shape[-1] // self.heads).transpose(1, 2)\n",
        "        k = k.view(k.shape[0], -1, self.heads, k.shape[-1] // self.heads).transpose(1, 2)\n",
        "        v = v.view(v.shape[0], -1, self.heads, v.shape[-1] // self.heads).transpose(1, 2)\n",
        "        \n",
        "        # \u8ba1\u7b97\u6ce8\u610f\u529b\n",
        "        scores = torch.matmul(q, k.transpose(-2, -1)) * self.scale\n",
        "        attn = F.softmax(scores, dim=-1)\n",
        "        \n",
        "        # \u5e94\u7528\u6ce8\u610f\u529b\n",
        "        out = torch.matmul(attn, v)\n",
        "        out = out.transpose(1, 2).contiguous().view(out.shape[0], -1, out.shape[-1] * self.heads)\n",
        "        \n",
        "        return self.to_out(out)\n",
        "\n",
        "class ImprovedResBlock(nn.Module):\n",
        "    \"\"\"\n",
        "    \u6539\u8fdb\u7684\u6b8b\u5dee\u5757\uff0c\u501f\u9274\u5b98\u65b9\u5b9e\u73b0\n",
        "    \"\"\"\n",
        "    def __init__(self, channels, time_dim, dropout=0.0):\n",
        "        super().__init__()\n",
        "        \n",
        "        # \u52a8\u6001\u8ba1\u7b97GroupNorm\u7684\u7ec4\u6570\uff0c\u786e\u4fddchannels\u80fd\u88abnum_groups\u6574\u9664\n",
        "        if channels >= 32:\n",
        "            num_groups = min(32, channels // (channels // 32))\n",
        "        elif channels >= 16:\n",
        "            num_groups = min(16, channels // (channels // 16))\n",
        "        elif channels >= 8:\n",
        "            num_groups = min(8, channels // (channels // 8))\n",
        "        elif channels >= 4:\n",
        "            num_groups = min(4, channels // (channels // 4))\n",
        "        else:\n",
        "            num_groups = 1\n",
        "        \n",
        "        # \u786e\u4fddnum_groups\u80fd\u6574\u9664channels\n",
        "        while channels % num_groups != 0 and num_groups > 1:\n",
        "            num_groups -= 1\n",
        "        \n",
        "        self.time_mlp = nn.Sequential(\n",
        "            nn.SiLU(),\n",
        "            nn.Linear(time_dim, channels)\n",
        "        )\n",
        "        \n",
        "        self.block1 = nn.Sequential(\n",
        "            nn.GroupNorm(num_groups, channels),\n",
        "            nn.SiLU(),\n",
        "            nn.Conv2d(channels, channels, 3, padding=1)\n",
        "        )\n",
        "        \n",
        "        self.block2 = nn.Sequential(\n",
        "            nn.GroupNorm(num_groups, channels),\n",
        "            nn.SiLU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Conv2d(channels, channels, 3, padding=1)\n",
        "        )\n",
        "        \n",
        "        # \u65f6\u95f4\u5d4c\u5165\u6295\u5f71\n",
        "        self.time_proj = nn.Linear(time_dim, channels)\n",
        "        \n",
        "    def forward(self, x, time_emb):\n",
        "        h = self.block1(x)\n",
        "        \n",
        "        # \u65f6\u95f4\u5d4c\u5165\u5904\u7406\n",
        "        time_emb = self.time_proj(time_emb)\n",
        "        time_emb = time_emb.view(x.shape[0], -1, 1, 1)\n",
        "        h = h + time_emb\n",
        "        \n",
        "        h = self.block2(h)\n",
        "        return h + x\n",
        "\n",
        "class ImprovedUNet2DConditionModel(nn.Module):\n",
        "    \"\"\"\n",
        "    \u6539\u8fdb\u7684UNet\u5b9e\u73b0\uff0c\u501f\u9274\u5b98\u65b9\u67b6\u6784\n",
        "    \"\"\"\n",
        "    def __init__(self, in_channels=4, out_channels=4, model_channels=128, num_res_blocks=2, \n",
        "                 attention_resolutions=(8, 16), dropout=0.1, channel_mult=(1, 2, 4, 8), \n",
        "                 conv_resample=True, num_heads=8, context_dim=512):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.in_channels = in_channels\n",
        "        self.model_channels = model_channels\n",
        "        self.num_res_blocks = num_res_blocks\n",
        "        self.attention_resolutions = attention_resolutions\n",
        "        self.dropout = dropout\n",
        "        self.channel_mult = channel_mult\n",
        "        self.conv_resample = conv_resample\n",
        "        self.num_heads = num_heads\n",
        "        self.context_dim = context_dim\n",
        "        \n",
        "        # \u65f6\u95f4\u5d4c\u5165 - \u4f7f\u7528\u66f4\u6df1\u7684\u7f51\u7edc\n",
        "        time_embed_dim = model_channels * 4\n",
        "        self.time_embedding = nn.Sequential(\n",
        "            nn.Linear(1, time_embed_dim),\n",
        "            nn.SiLU(),\n",
        "            nn.Linear(time_embed_dim, time_embed_dim),\n",
        "            nn.SiLU(),\n",
        "            nn.Linear(time_embed_dim, time_embed_dim)\n",
        "        )\n",
        "        \n",
        "        # \u8f93\u5165\u6295\u5f71\n",
        "        self.input_blocks = nn.ModuleList([\n",
        "            nn.Conv2d(in_channels, model_channels, kernel_size=3, padding=1)\n",
        "        ])\n",
        "        \n",
        "        # \u4e0b\u91c7\u6837\u5757\n",
        "        input_block_chans = [model_channels]\n",
        "        ch = model_channels\n",
        "        \n",
        "        for level, mult in enumerate(channel_mult):\n",
        "            # \u6dfb\u52a0ResBlock\n",
        "            for _ in range(num_res_blocks):\n",
        "                self.input_blocks.append(\n",
        "                    nn.ModuleList([ImprovedResBlock(ch, time_embed_dim, dropout)])\n",
        "                )\n",
        "                input_block_chans.append(ch)\n",
        "            \n",
        "            # \u6dfb\u52a0CrossAttention\n",
        "            if level in attention_resolutions:\n",
        "                self.input_blocks.append(\n",
        "                    nn.ModuleList([ImprovedCrossAttention(ch, context_dim, num_heads, dropout=dropout)])\n",
        "                )\n",
        "                input_block_chans.append(ch)\n",
        "            \n",
        "            # \u4e0b\u91c7\u6837\n",
        "            if level < len(channel_mult) - 1:\n",
        "                ch = mult * model_channels\n",
        "                self.input_blocks.append(\n",
        "                    nn.ModuleList([nn.Conv2d(input_block_chans[-1], ch, 3, stride=2, padding=1)])\n",
        "                )\n",
        "                input_block_chans.append(ch)\n",
        "        \n",
        "        # \u4e2d\u95f4\u5757\n",
        "        self.middle_block = nn.ModuleList([\n",
        "            ImprovedResBlock(ch, time_embed_dim, dropout),\n",
        "            ImprovedCrossAttention(ch, context_dim, num_heads, dropout=dropout),\n",
        "            ImprovedResBlock(ch, time_embed_dim, dropout)\n",
        "        ])\n",
        "        \n",
        "        # \u8f93\u51fa\u5757\n",
        "        self.output_blocks = nn.ModuleList([])\n",
        "        for level, mult in list(enumerate(channel_mult))[::-1]:\n",
        "            # \u4e0a\u91c7\u6837\n",
        "            if level < len(channel_mult) - 1:\n",
        "                self.output_blocks.append(\n",
        "                    nn.ModuleList([nn.ConvTranspose2d(ch, ch//2, 4, stride=2, padding=1)])\n",
        "                )\n",
        "                ch = ch // 2\n",
        "            \n",
        "            # \u6dfb\u52a0ResBlock\n",
        "            for _ in range(num_res_blocks + 1):\n",
        "                self.output_blocks.append(\n",
        "                    nn.ModuleList([ImprovedResBlock(ch, time_embed_dim, dropout)])\n",
        "                )\n",
        "            \n",
        "            # \u6dfb\u52a0CrossAttention\n",
        "            if level in attention_resolutions:\n",
        "                self.output_blocks.append(\n",
        "                    nn.ModuleList([ImprovedCrossAttention(ch, context_dim, num_heads, dropout=dropout)])\n",
        "                )\n",
        "        \n",
        "        # \u8f93\u51fa\u6295\u5f71\n",
        "        if ch >= 32:\n",
        "            num_groups = 32\n",
        "        elif ch >= 16:\n",
        "            num_groups = 16\n",
        "        elif ch >= 8:\n",
        "            num_groups = 8\n",
        "        elif ch >= 4:\n",
        "            num_groups = 4\n",
        "        else:\n",
        "            num_groups = 1\n",
        "        \n",
        "        self.out = nn.Sequential(\n",
        "            nn.GroupNorm(num_groups, ch),\n",
        "            nn.SiLU(),\n",
        "            nn.Conv2d(ch, out_channels, kernel_size=3, padding=1)\n",
        "        )\n",
        "        \n",
        "    def forward(self, x, timesteps, context=None):\n",
        "        # \u65f6\u95f4\u5d4c\u5165\n",
        "        t = self.time_embedding(timesteps.unsqueeze(-1).float())\n",
        "        if t.dim() == 1:\n",
        "            t = t.unsqueeze(0)\n",
        "        \n",
        "        # \u8f93\u5165\u5757\n",
        "        h = x\n",
        "        hs = []\n",
        "        for module in self.input_blocks:\n",
        "            if isinstance(module, nn.ModuleList):\n",
        "                # \u5904\u7406ModuleList\u4e2d\u7684\u6a21\u5757\n",
        "                for submodule in module:\n",
        "                    if isinstance(submodule, ImprovedCrossAttention):\n",
        "                        h = submodule(h, context)\n",
        "                    elif isinstance(submodule, ImprovedResBlock):\n",
        "                        h = submodule(h, t)\n",
        "                    else:\n",
        "                        h = submodule(h)\n",
        "            else:\n",
        "                # \u76f4\u63a5\u5904\u7406\u5355\u4e2a\u6a21\u5757\n",
        "                if isinstance(module, ImprovedCrossAttention):\n",
        "                    h = module(h, context)\n",
        "                elif isinstance(module, ImprovedResBlock):\n",
        "                    h = module(h, t)\n",
        "                else:\n",
        "                    h = module(h)\n",
        "            hs.append(h)\n",
        "        \n",
        "        # \u4e2d\u95f4\u5757\n",
        "        for module in self.middle_block:\n",
        "            if isinstance(module, ImprovedCrossAttention):\n",
        "                h = module(h, context)\n",
        "            else:\n",
        "                h = module(h, t)\n",
        "        \n",
        "        # \u8f93\u51fa\u5757\n",
        "        for module in self.output_blocks:\n",
        "            if isinstance(module, nn.ModuleList):\n",
        "                # \u5904\u7406ModuleList\u4e2d\u7684\u6a21\u5757\n",
        "                for submodule in module:\n",
        "                    if isinstance(submodule, ImprovedCrossAttention):\n",
        "                        h = submodule(h, context)\n",
        "                    elif isinstance(submodule, ImprovedResBlock):\n",
        "                        h = submodule(h, t)\n",
        "                    else:\n",
        "                        h = submodule(h)\n",
        "            else:\n",
        "                # \u76f4\u63a5\u5904\u7406\u5355\u4e2a\u6a21\u5757\n",
        "                if isinstance(module, ImprovedCrossAttention):\n",
        "                    h = module(h, context)\n",
        "                elif isinstance(module, ImprovedResBlock):\n",
        "                    h = module(h, t)\n",
        "                else:\n",
        "                    h = module(h)\n",
        "            \n",
        "            # \u8df3\u8dc3\u8fde\u63a5\n",
        "            if hs:\n",
        "                h = torch.cat([h, hs.pop()], dim=1)\n",
        "        \n",
        "        return self.out(h)\n",
        "\n",
        "class ImprovedDDPMScheduler:\n",
        "    \"\"\"\n",
        "    \u6539\u8fdb\u7684DDPM\u8c03\u5ea6\u5668\uff0c\u501f\u9274\u5b98\u65b9\u5b9e\u73b0\n",
        "    \"\"\"\n",
        "    def __init__(self, num_train_timesteps=1000, beta_start=0.0001, beta_end=0.02):\n",
        "        self.num_train_timesteps = num_train_timesteps\n",
        "        \n",
        "        # \u7ebf\u6027\u566a\u58f0\u8c03\u5ea6\n",
        "        self.betas = torch.linspace(beta_start, beta_end, num_train_timesteps)\n",
        "        self.alphas = 1.0 - self.betas\n",
        "        self.alphas_cumprod = torch.cumprod(self.alphas, dim=0)\n",
        "        self.alphas_cumprod_prev = torch.cat([torch.tensor([1.0]), self.alphas_cumprod[:-1]])\n",
        "        \n",
        "        # \u8ba1\u7b97\u566a\u58f0\u9884\u6d4b\u7684\u7cfb\u6570\n",
        "        self.sqrt_alphas_cumprod = torch.sqrt(self.alphas_cumprod)\n",
        "        self.sqrt_one_minus_alphas_cumprod = torch.sqrt(1.0 - self.alphas_cumprod)\n",
        "        \n",
        "    def add_noise(self, original_samples, noise, timesteps):\n",
        "        \"\"\"\u6dfb\u52a0\u566a\u58f0\u5230\u539f\u59cb\u6837\u672c\"\"\"\n",
        "        sqrt_alpha = self.sqrt_alphas_cumprod[timesteps].view(-1, 1, 1, 1)\n",
        "        sqrt_one_minus_alpha = self.sqrt_one_minus_alphas_cumprod[timesteps].view(-1, 1, 1, 1)\n",
        "        \n",
        "        return sqrt_alpha * original_samples + sqrt_one_minus_alpha * noise\n",
        "    \n",
        "    def step(self, model_output, timestep, sample):\n",
        "        \"\"\"\u53bb\u566a\u6b65\u9aa4\"\"\"\n",
        "        alpha = self.alphas_cumprod[timestep].view(-1, 1, 1, 1)\n",
        "        alpha_prev = self.alphas_cumprod_prev[timestep].view(-1, 1, 1, 1)\n",
        "        \n",
        "        # \u9884\u6d4bx0\n",
        "        pred_original_sample = (sample - torch.sqrt(1 - alpha) * model_output) / torch.sqrt(alpha)\n",
        "        \n",
        "        # \u9884\u6d4b\u524d\u4e00\u4e2a\u6837\u672c\n",
        "        pred_sample_direction = torch.sqrt(1 - alpha_prev) * model_output\n",
        "        pred_prev_sample = torch.sqrt(alpha_prev) * pred_original_sample + pred_sample_direction\n",
        "        \n",
        "        return pred_prev_sample\n",
        "    \n",
        "    def set_timesteps(self, num_inference_steps):\n",
        "        \"\"\"\u8bbe\u7f6e\u63a8\u7406\u65f6\u95f4\u6b65\"\"\"\n",
        "        self.num_inference_steps = num_inference_steps\n",
        "        step_ratio = self.num_train_timesteps // num_inference_steps\n",
        "        timesteps = (torch.arange(0, num_inference_steps) * step_ratio).flip(0)\n",
        "        return timesteps\n",
        "\n",
        "class ImprovedStableDiffusionPipeline:\n",
        "    \"\"\"\n",
        "    \u6539\u8fdb\u7684Stable Diffusion Pipeline\uff0c\u501f\u9274\u5b98\u65b9\u6700\u4f73\u5b9e\u8df5\n",
        "    \"\"\"\n",
        "    def __init__(self, device='cuda' if torch.cuda.is_available() else 'cpu'):\n",
        "        self.device = device\n",
        "        \n",
        "        # \u521d\u59cb\u5316\u7ec4\u4ef6\n",
        "        self.vae = ImprovedVAE().to(device)\n",
        "        self.unet = ImprovedUNet2DConditionModel(\n",
        "            in_channels=4,\n",
        "            out_channels=4,\n",
        "            model_channels=128,\n",
        "            channel_mult=(1, 2, 4, 8),\n",
        "            attention_resolutions=(8, 16),\n",
        "            context_dim=512\n",
        "        ).to(device)\n",
        "        self.scheduler = ImprovedDDPMScheduler()\n",
        "        \n",
        "        # CLIP\u6587\u672c\u7f16\u7801\u5668\n",
        "        self.tokenizer = CLIPTokenizer.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
        "        self.text_encoder = CLIPTextModel.from_pretrained(\"openai/clip-vit-base-patch32\").to(device)\n",
        "        \n",
        "        # \u8bbe\u7f6e\u4e3a\u8bc4\u4f30\u6a21\u5f0f\n",
        "        self.text_encoder.eval()\n",
        "        self.vae.eval()\n",
        "        \n",
        "    def _encode_prompt(self, prompt):\n",
        "        \"\"\"\u7f16\u7801\u6587\u672c\u63d0\u793a\"\"\"\n",
        "        tokens = self.tokenizer(prompt, padding=True, return_tensors=\"pt\").to(self.device)\n",
        "        with torch.no_grad():\n",
        "            text_embeddings = self.text_encoder(**tokens).last_hidden_state\n",
        "        return text_embeddings\n",
        "    \n",
        "    def _parse_kanji_prompt(self, prompt):\n",
        "        \"\"\"\u89e3\u6790\u6c49\u5b57\u63d0\u793a\uff0c\u4f7f\u7528\u66f4\u8be6\u7ec6\u7684\u63cf\u8ff0\"\"\"\n",
        "        base_prompt = f\"kanji character representing {prompt}, traditional calligraphy style, black ink on white paper, high contrast, detailed strokes, clear lines, professional quality, artistic interpretation\"\n",
        "        return base_prompt\n",
        "    \n",
        "    def generate(self, prompt, height=128, width=128, num_inference_steps=50, \n",
        "                guidance_scale=7.5, seed=None):\n",
        "        \"\"\"\u751f\u6210\u56fe\u50cf\uff0c\u4f7f\u7528\u5b98\u65b9\u63a8\u8350\u7684\u53c2\u6570\"\"\"\n",
        "        \n",
        "        # \u8bbe\u7f6e\u968f\u673a\u79cd\u5b50\n",
        "        if seed is not None:\n",
        "            torch.manual_seed(seed)\n",
        "            torch.cuda.manual_seed(seed)\n",
        "        \n",
        "        # \u7f16\u7801\u63d0\u793a\n",
        "        text_embeddings = self._encode_prompt(self._parse_kanji_prompt(prompt))\n",
        "        \n",
        "        # \u521d\u59cb\u5316\u6f5c\u5728\u53d8\u91cf\n",
        "        latent_height = height // 8\n",
        "        latent_width = width // 8\n",
        "        latents = torch.randn(1, 4, latent_height, latent_width, device=self.device)\n",
        "        \n",
        "        # \u8bbe\u7f6e\u65f6\u95f4\u6b65\n",
        "        timesteps = self.scheduler.set_timesteps(num_inference_steps)\n",
        "        timesteps = timesteps.to(self.device)\n",
        "        \n",
        "        # \u6539\u8fdb\u7684\u53bb\u566a\u5faa\u73af\n",
        "        for i, t in enumerate(timesteps):\n",
        "            # \u6269\u5c55\u6f5c\u5728\u53d8\u91cf\u7528\u4e8e\u6279\u5904\u7406\n",
        "            latent_model_input = torch.cat([latents] * 2)\n",
        "            t_expanded = t.expand(2)\n",
        "            \n",
        "            # \u9884\u6d4b\u566a\u58f0\n",
        "            with torch.no_grad():\n",
        "                noise_pred = self.unet(latent_model_input, t_expanded, text_embeddings)\n",
        "            \n",
        "            # \u6267\u884cclassifier-free guidance\n",
        "            noise_pred_uncond, noise_pred_text = noise_pred.chunk(2)\n",
        "            \n",
        "            # \u4f7f\u7528\u5b98\u65b9\u63a8\u8350\u7684guidance scale\n",
        "            guidance_scale = torch.clamp(torch.tensor(guidance_scale), min=1.0, max=20.0)\n",
        "            noise_pred = noise_pred_uncond + guidance_scale * (noise_pred_text - noise_pred_uncond)\n",
        "            \n",
        "            # \u8ba1\u7b97\u524d\u4e00\u4e2a\u6837\u672c\n",
        "            latents = self.scheduler.step(noise_pred, t, latents)\n",
        "        \n",
        "        # \u89e3\u7801\u6f5c\u5728\u53d8\u91cf\n",
        "        with torch.no_grad():\n",
        "            image = self.vae.decode(latents)\n",
        "        \n",
        "        return image\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    print(\"\ud83c\udf8c \u6539\u8fdb\u7684Stable Diffusion\u5b9e\u73b0\")\n",
        "    print(\"=\" * 50)\n",
        "    \n",
        "    # \u6d4b\u8bd5\u6a21\u578b\n",
        "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "    print(f\"\ud83d\udd27 \u4f7f\u7528\u8bbe\u5907: {device}\")\n",
        "    \n",
        "    try:\n",
        "        pipeline = ImprovedStableDiffusionPipeline(device=device)\n",
        "        print(\"\u2705 \u6a21\u578b\u521d\u59cb\u5316\u6210\u529f\")\n",
        "        \n",
        "        # \u6d4b\u8bd5\u751f\u6210\n",
        "        print(\"\ud83c\udf0a \u6d4b\u8bd5\u751f\u6210...\")\n",
        "        result = pipeline.generate(\n",
        "            \"water\",\n",
        "            height=128,\n",
        "            width=128,\n",
        "            num_inference_steps=50,\n",
        "            guidance_scale=7.5,\n",
        "            seed=42\n",
        "        )\n",
        "        print(\"\u2705 \u751f\u6210\u5b8c\u6210\")\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"\u274c \u9519\u8bef: {e}\")\n"
      ],
      "outputs": [],
      "execution_count": null
    }
  ],
  "metadata": {
    "anaconda-cloud": {},
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}