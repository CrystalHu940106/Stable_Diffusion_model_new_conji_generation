{
  "cells": [
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#!/usr/bin/env python3\n",
        "\"\"\"\n",
        "Complete Stable Diffusion Implementation for Kanji Generation\n",
        "\"\"\"\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from transformers import CLIPTokenizer, CLIPTextModel\n",
        "import math\n",
        "from typing import Optional, Union, Tuple\n",
        "import numpy as np\n",
        "\n",
        "class VAE(nn.Module):\n",
        "    def __init__(self, in_channels=3, latent_channels=4, hidden_dims=[128, 256, 512]):\n",
        "        super().__init__()\n",
        "        self.latent_channels = latent_channels\n",
        "        \n",
        "        # Encoder\n",
        "        encoder_layers = []\n",
        "        in_ch = in_channels\n",
        "        for h_dim in hidden_dims:\n",
        "            encoder_layers.extend([\n",
        "                nn.Conv2d(in_ch, h_dim, kernel_size=3, stride=2, padding=1),\n",
        "                nn.BatchNorm2d(h_dim),\n",
        "                nn.LeakyReLU()\n",
        "            ])\n",
        "            in_ch = h_dim\n",
        "        \n",
        "        # Final encoding layer\n",
        "        encoder_layers.extend([\n",
        "            nn.Conv2d(hidden_dims[-1], latent_channels * 2, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(latent_channels * 2)\n",
        "        ])\n",
        "        \n",
        "        self.encoder = nn.Sequential(*encoder_layers)\n",
        "        \n",
        "        # Decoder - ensure exact 128x128 output\n",
        "        decoder_layers = []\n",
        "        in_ch = latent_channels\n",
        "        \n",
        "        # We need exactly 4 upsampling layers: 8->16->32->64->128\n",
        "        # Use the hidden_dims in reverse order for channels\n",
        "        hidden_dims_rev = hidden_dims[::-1]\n",
        "        \n",
        "        for i, h_dim in enumerate(hidden_dims_rev):\n",
        "            decoder_layers.extend([\n",
        "                nn.ConvTranspose2d(in_ch, h_dim, kernel_size=4, stride=2, padding=1),\n",
        "                nn.BatchNorm2d(h_dim),\n",
        "                nn.LeakyReLU()\n",
        "            ])\n",
        "            in_ch = h_dim\n",
        "        \n",
        "        # Final layer to get to 3 channels\n",
        "        decoder_layers.extend([\n",
        "            nn.Conv2d(in_ch, in_channels, kernel_size=3, stride=1, padding=1),\n",
        "            nn.Tanh()\n",
        "        ])\n",
        "        \n",
        "        self.decoder = nn.Sequential(*decoder_layers)\n",
        "        \n",
        "        # Pre-define projection layers for different input sizes\n",
        "        self.input_projections = nn.ModuleDict({\n",
        "            '128': nn.Conv2d(3, 3, 1),\n",
        "            '64': nn.Conv2d(3, 3, 1),\n",
        "            '32': nn.Conv2d(3, 3, 1)\n",
        "        })\n",
        "        \n",
        "    def encode(self, x):\n",
        "        # Ensure input is 128x128\n",
        "        if x.shape[-1] != 128:\n",
        "            target_size = 128\n",
        "            x = F.interpolate(x, size=(target_size, target_size), mode='bilinear', align_corners=False)\n",
        "            if str(target_size) in self.input_projections:\n",
        "                x = self.input_projections[str(target_size)](x)\n",
        "        \n",
        "        # Encode to latent space\n",
        "        encoded = self.encoder(x)\n",
        "        mu, logvar = torch.chunk(encoded, 2, dim=1)\n",
        "        \n",
        "        # KL divergence loss\n",
        "        kl_loss = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
        "        \n",
        "        # Reparameterization trick\n",
        "        std = torch.exp(0.5 * logvar)\n",
        "        eps = torch.randn_like(std)\n",
        "        z = mu + eps * std\n",
        "        \n",
        "        return z, mu, logvar, kl_loss\n",
        "    \n",
        "    def decode(self, z):\n",
        "        return self.decoder(z)\n",
        "\n",
        "class CrossAttention(nn.Module):\n",
        "    def __init__(self, query_dim, context_dim=None, heads=8, dim_head=64, dropout=0.0):\n",
        "        super().__init__()\n",
        "        inner_dim = dim_head * heads\n",
        "        context_dim = context_dim if context_dim is not None else query_dim\n",
        "        \n",
        "        self.scale = dim_head ** -0.5\n",
        "        self.heads = heads\n",
        "        \n",
        "        # Pre-define all projection layers\n",
        "        self.to_q = nn.Linear(query_dim, inner_dim, bias=False)\n",
        "        self.to_k = nn.Linear(context_dim, inner_dim, bias=False)\n",
        "        self.to_v = nn.Linear(context_dim, inner_dim, bias=False)\n",
        "        self.to_out = nn.Sequential(\n",
        "            nn.Linear(inner_dim, query_dim),\n",
        "            nn.Dropout(dropout)\n",
        "        )\n",
        "        \n",
        "    def forward(self, x, context=None):\n",
        "        h = self.heads\n",
        "        \n",
        "        if context is None:\n",
        "            context = x\n",
        "        \n",
        "        # Reshape x for attention: (B, C, H, W) -> (B, H*W, C)\n",
        "        batch_size, channels, height, width = x.shape\n",
        "        x_flat = x.flatten(2).transpose(1, 2)  # (B, H*W, C)\n",
        "        \n",
        "        # Apply attention\n",
        "        q = self.to_q(x_flat)\n",
        "        k = self.to_k(context)\n",
        "        v = self.to_v(context)\n",
        "        \n",
        "        q, k, v = map(lambda t: t.reshape(*t.shape[:2], h, -1).transpose(1, 2), (q, k, v))\n",
        "        \n",
        "        sim = torch.einsum('b h i d, b h j d -> b h i j', q, k) * self.scale\n",
        "        attn = sim.softmax(dim=-1)\n",
        "        \n",
        "        out = torch.einsum('b h i j, b h j d -> b h i d', attn, v)\n",
        "        out = out.transpose(1, 2).reshape(*x_flat.shape[:2], -1)\n",
        "        \n",
        "        # Apply output projection\n",
        "        out = self.to_out(out)\n",
        "        \n",
        "        # Reshape back to spatial dimensions: (B, H*W, C) -> (B, C, H, W)\n",
        "        out = out.transpose(1, 2).reshape(batch_size, channels, height, width)\n",
        "        \n",
        "        return out\n",
        "\n",
        "class ResBlock(nn.Module):\n",
        "    def __init__(self, channels, time_dim, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.channels = channels\n",
        "        \n",
        "        # Calculate appropriate group size for GroupNorm\n",
        "        # GroupNorm requires that the number of groups divides the number of channels\n",
        "        if channels >= 32:\n",
        "            num_groups = 32\n",
        "        elif channels >= 16:\n",
        "            num_groups = 16\n",
        "        elif channels >= 8:\n",
        "            num_groups = 8\n",
        "        elif channels >= 4:\n",
        "            num_groups = 4\n",
        "        else:\n",
        "            num_groups = 1\n",
        "        \n",
        "        self.time_mlp = nn.Sequential(\n",
        "            nn.SiLU(),\n",
        "            nn.Linear(time_dim, channels)\n",
        "        )\n",
        "        \n",
        "        self.block1 = nn.Sequential(\n",
        "            nn.GroupNorm(num_groups, channels),\n",
        "            nn.SiLU(),\n",
        "            nn.Conv2d(channels, channels, 3, padding=1)\n",
        "        )\n",
        "        \n",
        "        self.block2 = nn.Sequential(\n",
        "            nn.GroupNorm(num_groups, channels),\n",
        "            nn.SiLU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Conv2d(channels, channels, 3, padding=1)\n",
        "        )\n",
        "        \n",
        "        # Pre-define time embedding projection\n",
        "        self.time_proj = nn.Linear(time_dim, channels)\n",
        "        \n",
        "    def forward(self, x, time_emb):\n",
        "        h = self.block1(x)\n",
        "        \n",
        "        # Use pre-defined time projection\n",
        "        time_emb = self.time_proj(time_emb)\n",
        "        time_emb = time_emb.view(x.shape[0], -1, 1, 1)\n",
        "        h = h + time_emb\n",
        "        \n",
        "        h = self.block2(h)\n",
        "        return h + x\n",
        "\n",
        "class UNet2DConditionModel(nn.Module):\n",
        "    def __init__(self, in_channels=4, out_channels=4, model_channels=128, num_res_blocks=2, \n",
        "                 attention_resolutions=(8, 16), dropout=0.1, channel_mult=(1, 2, 4), \n",
        "                 conv_resample=True, num_heads=8, context_dim=512):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.in_channels = in_channels\n",
        "        self.model_channels = model_channels\n",
        "        self.num_res_blocks = num_res_blocks\n",
        "        self.attention_resolutions = attention_resolutions\n",
        "        self.dropout = dropout\n",
        "        self.channel_mult = channel_mult\n",
        "        self.conv_resample = conv_resample\n",
        "        self.num_heads = num_heads\n",
        "        self.context_dim = context_dim\n",
        "        \n",
        "        # Time embedding\n",
        "        time_embed_dim = model_channels * 4\n",
        "        self.time_embedding = nn.Sequential(\n",
        "            nn.Linear(1, time_embed_dim),\n",
        "            nn.SiLU(),\n",
        "            nn.Linear(time_embed_dim, time_embed_dim)\n",
        "        )\n",
        "        \n",
        "        # Input projection\n",
        "        self.input_blocks = nn.ModuleList([\n",
        "            nn.Conv2d(in_channels, model_channels, kernel_size=3, padding=1)\n",
        "        ])\n",
        "        \n",
        "        # Downsampling blocks - simplified architecture\n",
        "        input_block_chans = [model_channels]\n",
        "        ch = model_channels\n",
        "        \n",
        "        for level, mult in enumerate(channel_mult):\n",
        "            # Add ResBlock\n",
        "            self.input_blocks.append(\n",
        "                nn.ModuleList([ResBlock(ch, time_embed_dim, dropout)])\n",
        "            )\n",
        "            input_block_chans.append(ch)\n",
        "            \n",
        "            # Add CrossAttention if at attention resolution\n",
        "            if level in attention_resolutions:\n",
        "                self.input_blocks.append(\n",
        "                    nn.ModuleList([CrossAttention(ch, context_dim, num_heads)])\n",
        "                )\n",
        "                input_block_chans.append(ch)\n",
        "            \n",
        "            # Downsample\n",
        "            if level < len(channel_mult) - 1:\n",
        "                ch = mult * model_channels\n",
        "                self.input_blocks.append(\n",
        "                    nn.ModuleList([nn.Conv2d(input_block_chans[-1], ch, 3, stride=2, padding=1)])\n",
        "                )\n",
        "                input_block_chans.append(ch)\n",
        "        \n",
        "        # Middle block\n",
        "        self.middle_block = nn.ModuleList([\n",
        "            ResBlock(ch, time_embed_dim, dropout),\n",
        "            CrossAttention(ch, context_dim, num_heads),\n",
        "            ResBlock(ch, time_embed_dim, dropout)\n",
        "        ])\n",
        "        \n",
        "        # Output blocks - simplified\n",
        "        self.output_blocks = nn.ModuleList([])\n",
        "        for level, mult in list(enumerate(channel_mult))[::-1]:\n",
        "            # Upsample\n",
        "            if level < len(channel_mult) - 1:\n",
        "                self.output_blocks.append(\n",
        "                    nn.ModuleList([nn.ConvTranspose2d(ch, ch//2, 4, stride=2, padding=1)])\n",
        "                )\n",
        "                ch = ch // 2\n",
        "            \n",
        "            # Add ResBlock\n",
        "            self.output_blocks.append(\n",
        "                nn.ModuleList([ResBlock(ch, time_embed_dim, dropout)])\n",
        "            )\n",
        "            \n",
        "            # Add CrossAttention if at attention resolution\n",
        "            if level in attention_resolutions:\n",
        "                self.output_blocks.append(\n",
        "                    nn.ModuleList([CrossAttention(ch, context_dim, num_heads)])\n",
        "                )\n",
        "        \n",
        "        # Output projection\n",
        "        # Calculate appropriate group size for GroupNorm\n",
        "        if ch >= 32:\n",
        "            num_groups = 32\n",
        "        elif ch >= 16:\n",
        "            num_groups = 16\n",
        "        elif ch >= 8:\n",
        "            num_groups = 8\n",
        "        elif ch >= 4:\n",
        "            num_groups = 4\n",
        "        else:\n",
        "            num_groups = 1\n",
        "        \n",
        "        self.out = nn.Sequential(\n",
        "            nn.GroupNorm(num_groups, ch),\n",
        "            nn.SiLU(),\n",
        "            nn.Conv2d(ch, out_channels, kernel_size=3, padding=1)\n",
        "        )\n",
        "        \n",
        "    def forward(self, x, timesteps, context=None):\n",
        "        # Time embedding\n",
        "        t = self.time_embedding(timesteps.unsqueeze(-1).float())\n",
        "        if t.dim() == 1:\n",
        "            t = t.unsqueeze(0)\n",
        "        \n",
        "        # Ensure time embedding matches batch size\n",
        "        batch_size = x.shape[0]\n",
        "        if t.shape[0] != batch_size:\n",
        "            if t.shape[0] == 1:\n",
        "                t = t.expand(batch_size, -1)\n",
        "            else:\n",
        "                t = t.repeat(batch_size // t.shape[0], 1)\n",
        "        \n",
        "        # Input blocks\n",
        "        h = x\n",
        "        for module in self.input_blocks:\n",
        "            if isinstance(module, nn.ModuleList):\n",
        "                for layer in module:\n",
        "                    if isinstance(layer, ResBlock):\n",
        "                        h = layer(h, t)\n",
        "                    elif isinstance(layer, CrossAttention):\n",
        "                        h = layer(h, context)\n",
        "                    else:\n",
        "                        h = layer(h)\n",
        "            else:\n",
        "                h = module(h)\n",
        "        \n",
        "        # Middle block\n",
        "        for module in self.middle_block:\n",
        "            if isinstance(module, ResBlock):\n",
        "                h = module(h, t)\n",
        "            elif isinstance(module, CrossAttention):\n",
        "                h = module(h, context)\n",
        "            else:\n",
        "                h = module(h)\n",
        "        \n",
        "        # Output blocks\n",
        "        for module in self.output_blocks:\n",
        "            if isinstance(module, nn.ModuleList):\n",
        "                for layer in module:\n",
        "                    if isinstance(layer, ResBlock):\n",
        "                        h = layer(h, t)\n",
        "                    elif isinstance(layer, CrossAttention):\n",
        "                        h = layer(h, context)\n",
        "                    else:\n",
        "                        h = layer(h)\n",
        "            else:\n",
        "                h = module(h)\n",
        "        \n",
        "        return self.out(h)\n",
        "\n",
        "class DDPMScheduler:\n",
        "    def __init__(self, num_train_timesteps=1000, beta_start=0.0001, beta_end=0.02):\n",
        "        self.num_train_timesteps = num_train_timesteps\n",
        "        self.beta_start = beta_start\n",
        "        self.beta_end = beta_end\n",
        "        \n",
        "        # Linear schedule\n",
        "        self.betas = torch.linspace(beta_start, beta_end, num_train_timesteps)\n",
        "        self.alphas = 1.0 - self.betas\n",
        "        self.alphas_cumprod = torch.cumprod(self.alphas, dim=0)\n",
        "        self.alphas_cumprod_prev = torch.cat([torch.tensor([1.0]), self.alphas_cumprod[:-1]])\n",
        "        \n",
        "        # Pre-compute values\n",
        "        self.sqrt_alphas_cumprod = torch.sqrt(self.alphas_cumprod)\n",
        "        self.sqrt_one_minus_alphas_cumprod = torch.sqrt(1.0 - self.alphas_cumprod)\n",
        "        self.log_one_minus_alphas_cumprod = torch.log(1.0 - self.alphas_cumprod)\n",
        "        self.sqrt_recip_alphas_cumprod = torch.sqrt(1.0 / self.alphas_cumprod)\n",
        "        self.sqrt_recipm1_alphas_cumprod = torch.sqrt(1.0 / self.alphas_cumprod - 1)\n",
        "        \n",
        "    def add_noise(self, original_samples, noise, timesteps):\n",
        "        # Ensure timesteps are within bounds\n",
        "        timesteps = torch.clamp(timesteps, 0, self.num_train_timesteps - 1)\n",
        "        \n",
        "        # Move timesteps to CPU for indexing, then back to device\n",
        "        device = timesteps.device\n",
        "        timesteps_cpu = timesteps.cpu()\n",
        "        \n",
        "        sqrt_alpha = self.sqrt_alphas_cumprod[timesteps_cpu].view(-1, 1, 1, 1).to(device)\n",
        "        sqrt_one_minus_alpha = self.sqrt_one_minus_alphas_cumprod[timesteps_cpu].view(-1, 1, 1, 1).to(device)\n",
        "        \n",
        "        return sqrt_alpha * original_samples + sqrt_one_minus_alpha * noise\n",
        "    \n",
        "    def step(self, model_output, timestep, sample):\n",
        "        # Ensure timestep is within bounds\n",
        "        timestep = torch.clamp(timestep, 0, self.num_train_timesteps - 1)\n",
        "        \n",
        "        # Move timestep to CPU for indexing, then back to device\n",
        "        device = timestep.device\n",
        "        timestep_cpu = timestep.cpu()\n",
        "        \n",
        "        # Simple DDPM step\n",
        "        alpha = self.alphas_cumprod[timestep_cpu].to(device)\n",
        "        alpha_prev = self.alphas_cumprod_prev[timestep_cpu].to(device)\n",
        "        \n",
        "        # Ensure alpha tensors have correct shape\n",
        "        if alpha.dim() == 0:\n",
        "            alpha = alpha.unsqueeze(0)\n",
        "        if alpha_prev.dim() == 0:\n",
        "            alpha_prev = alpha_prev.unsqueeze(0)\n",
        "        \n",
        "        # Ensure alpha tensors have correct spatial dimensions\n",
        "        alpha = alpha.view(-1, 1, 1, 1)\n",
        "        alpha_prev = alpha_prev.view(-1, 1, 1, 1)\n",
        "        \n",
        "        # Predict x0\n",
        "        pred_original_sample = (sample - torch.sqrt(1 - alpha) * model_output) / torch.sqrt(alpha)\n",
        "        \n",
        "        # Predict previous sample\n",
        "        pred_sample_direction = torch.sqrt(1 - alpha_prev) * model_output\n",
        "        pred_prev_sample = torch.sqrt(alpha_prev) * pred_original_sample + pred_sample_direction\n",
        "        \n",
        "        return pred_prev_sample\n",
        "    \n",
        "    @property\n",
        "    def timesteps(self):\n",
        "        return torch.arange(self.num_train_timesteps)\n",
        "    \n",
        "    def set_timesteps(self, num_inference_steps):\n",
        "        self.num_inference_steps = num_inference_steps\n",
        "        step_ratio = self.num_train_timesteps // num_inference_steps\n",
        "        timesteps = (torch.arange(0, num_inference_steps) * step_ratio).flip(0)\n",
        "        return timesteps\n",
        "\n",
        "class StableDiffusionPipeline:\n",
        "    def __init__(self, device='cuda' if torch.cuda.is_available() else 'cpu'):\n",
        "        self.device = device\n",
        "        \n",
        "        # Initialize components\n",
        "        self.vae = VAE().to(device)\n",
        "        self.unet = UNet2DConditionModel().to(device)\n",
        "        self.scheduler = DDPMScheduler()\n",
        "        \n",
        "        # CLIP text encoder\n",
        "        self.tokenizer = CLIPTokenizer.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
        "        self.text_encoder = CLIPTextModel.from_pretrained(\"openai/clip-vit-base-patch32\").to(device)\n",
        "        \n",
        "        # Set to evaluation mode\n",
        "        self.text_encoder.eval()\n",
        "        self.vae.eval()\n",
        "        \n",
        "    def _encode_prompt(self, prompt):\n",
        "        # Tokenize and encode text\n",
        "        tokens = self.tokenizer(prompt, padding=True, return_tensors=\"pt\").to(self.device)\n",
        "        with torch.no_grad():\n",
        "            text_embeddings = self.text_encoder(**tokens).last_hidden_state\n",
        "        return text_embeddings\n",
        "    \n",
        "    def _parse_kanji_prompt(self, prompt):\n",
        "        # Enhanced prompt parsing for Kanji generation\n",
        "        base_prompt = f\"kanji character representing {prompt}, traditional calligraphy style, black ink on white paper, high contrast, detailed strokes\"\n",
        "        return base_prompt\n",
        "    \n",
        "    def generate(self, prompt, height=128, width=128, num_inference_steps=50, \n",
        "                guidance_scale=9.0, seed=None):\n",
        "        # Set seed for reproducibility\n",
        "        if seed is not None:\n",
        "            torch.manual_seed(seed)\n",
        "            torch.cuda.manual_seed(seed)\n",
        "        \n",
        "        # Encode prompt\n",
        "        text_embeddings = self._encode_prompt(self._parse_kanji_prompt(prompt))\n",
        "        \n",
        "        # Initialize latents\n",
        "        latent_height = height // 8\n",
        "        latent_width = width // 8\n",
        "        latents = torch.randn(1, 4, latent_height, latent_width, device=self.device)\n",
        "        \n",
        "        # Set timesteps\n",
        "        timesteps = self.scheduler.set_timesteps(num_inference_steps)\n",
        "        timesteps = timesteps.to(self.device)\n",
        "        \n",
        "        # Enhanced denoising loop with better guidance\n",
        "        for i, t in enumerate(timesteps):\n",
        "            # Expand latents for batch processing\n",
        "            latent_model_input = torch.cat([latents] * 2)\n",
        "            t_expanded = t.expand(2)\n",
        "            \n",
        "            # Predict noise\n",
        "            with torch.no_grad():\n",
        "                noise_pred = self.unet(latent_model_input, t_expanded, text_embeddings)\n",
        "            \n",
        "            # Perform enhanced guidance with optimal scale\n",
        "            noise_pred_uncond, noise_pred_text = noise_pred.chunk(2)\n",
        "            \n",
        "            # Clamp guidance scale for stability\n",
        "            guidance_scale = torch.clamp(torch.tensor(guidance_scale), min=1.0, max=15.0)\n",
        "            \n",
        "            # Enhanced classifier-free guidance\n",
        "            noise_pred = noise_pred_uncond + guidance_scale * (noise_pred_text - noise_pred_uncond)\n",
        "            \n",
        "            # Compute previous sample\n",
        "            latents = self.scheduler.step(noise_pred, t, latents)\n",
        "            \n",
        "            # Optional: Add noise for better exploration\n",
        "            if i < len(timesteps) - 1:\n",
        "                next_timestep = timesteps[i + 1]\n",
        "                latents = self.scheduler.add_noise(latents, torch.randn_like(latents), next_timestep)\n",
        "        \n",
        "        # Decode latents\n",
        "        with torch.no_grad():\n",
        "            image = self.vae.decode(latents)\n",
        "        \n",
        "        return image\n",
        "    \n",
        "    def generate_concept_kanji(self, concept, style=\"traditional\", guidance_scale=9.0):\n",
        "        # Generate Kanji for modern concepts with optimized guidance\n",
        "        style_prompts = {\n",
        "            \"traditional\": \"traditional calligraphy, brush strokes, ink wash, authentic\",\n",
        "            \"modern\": \"modern typography, clean lines, minimalist, contemporary\",\n",
        "            \"artistic\": \"artistic interpretation, creative design, unique style, expressive\",\n",
        "            \"professional\": \"professional design, corporate style, clean and clear\",\n",
        "            \"creative\": \"creative interpretation, innovative design, artistic flair\"\n",
        "        }\n",
        "        \n",
        "        style_desc = style_prompts.get(style, style_prompts[\"traditional\"])\n",
        "        prompt = f\"kanji character for {concept}, {style_desc}, high quality, detailed, well-balanced composition\"\n",
        "        \n",
        "        return self.generate(prompt, guidance_scale=guidance_scale)\n",
        "    \n",
        "    def semantic_interpolation(self, prompt1, prompt2, num_steps=5, guidance_scale=9.0):\n",
        "        # Generate images by interpolating between two prompts with enhanced quality\n",
        "        embeddings1 = self._encode_prompt(prompt1)\n",
        "        embeddings2 = self._encode_prompt(prompt2)\n",
        "        \n",
        "        images = []\n",
        "        for i in range(num_steps):\n",
        "            alpha = i / (num_steps - 1)\n",
        "            interpolated_embeddings = (1 - alpha) * embeddings1 + alpha * embeddings2\n",
        "            \n",
        "            # Generate with interpolated embeddings and optimal guidance\n",
        "            image = self._generate_with_custom_embedding(interpolated_embeddings, guidance_scale)\n",
        "            images.append(image)\n",
        "        \n",
        "        return images\n",
        "    \n",
        "    def _generate_with_custom_embedding(self, text_embeddings, guidance_scale=9.0):\n",
        "        # Helper method for custom embeddings with enhanced quality\n",
        "        latents = torch.randn(1, 4, 16, 16, device=self.device)\n",
        "        \n",
        "        # Set timesteps for generation\n",
        "        num_inference_steps = 50\n",
        "        self.scheduler.set_timesteps(num_inference_steps)\n",
        "        timesteps = self.scheduler.timesteps\n",
        "        \n",
        "        for i, t in enumerate(timesteps):\n",
        "            with torch.no_grad():\n",
        "                # Generate with and without conditioning\n",
        "                latent_model_input = torch.cat([latents] * 2)\n",
        "                t_expanded = t.expand(2)\n",
        "                \n",
        "                noise_pred = self.unet(latent_model_input, t_expanded, text_embeddings)\n",
        "                noise_pred_uncond, noise_pred_text = noise_pred.chunk(2)\n",
        "                \n",
        "                # Apply guidance\n",
        "                guidance_scale = torch.clamp(torch.tensor(guidance_scale), min=1.0, max=15.0)\n",
        "                noise_pred = noise_pred_uncond + guidance_scale * (noise_pred_text - noise_pred_uncond)\n",
        "                \n",
        "                # Denoise step\n",
        "                latents = self.scheduler.step(noise_pred, t, latents)\n",
        "        \n",
        "        return self.vae.decode(latents)\n",
        "    \n",
        "    def generate_creative_kanji(self, base_concept, modifiers=None, guidance_scale=10.0):\n",
        "        # Generate creative Kanji with concept modifiers and enhanced quality\n",
        "        if modifiers is None:\n",
        "            modifiers = [\"elegant\", \"powerful\", \"mystical\", \"balanced\", \"harmonious\"]\n",
        "        \n",
        "        prompt = f\"kanji character for {base_concept}, {' '.join(modifiers)}, artistic interpretation, high quality, detailed strokes\"\n",
        "        return self.generate(prompt, guidance_scale=guidance_scale)\n",
        "    \n",
        "    def generate_high_quality_kanji(self, prompt, num_variations=3, guidance_scales=[7.0, 9.0, 12.0]):\n",
        "        # Generate multiple high-quality variations with different guidance scales\n",
        "        variations = []\n",
        "        \n",
        "        for i in range(num_variations):\n",
        "            seed = torch.randint(0, 1000000, (1,)).item()\n",
        "            guidance_scale = guidance_scales[i % len(guidance_scales)]\n",
        "            \n",
        "            variation = self.generate(\n",
        "                prompt, \n",
        "                num_inference_steps=75,  # More steps for higher quality\n",
        "                guidance_scale=guidance_scale,\n",
        "                seed=seed\n",
        "            )\n",
        "            variations.append({\n",
        "                'image': variation,\n",
        "                'seed': seed,\n",
        "                'guidance_scale': guidance_scale\n",
        "            })\n",
        "        \n",
        "        return variations\n"
      ],
      "outputs": [],
      "execution_count": null
    }
  ],
  "metadata": {
    "anaconda-cloud": {},
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}