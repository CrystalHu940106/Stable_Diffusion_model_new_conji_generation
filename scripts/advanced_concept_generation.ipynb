{
  "cells": [
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#!/usr/bin/env python3\n",
        "\"\"\"\n",
        "Advanced Concept Generation with Semantic Interpolation\n",
        "Generate Kanji for modern concepts like YouTube, Gundam, etc.\n",
        "\"\"\"\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms\n",
        "from transformers import CLIPTokenizer, CLIPTextModel\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import json\n",
        "from pathlib import Path\n",
        "import matplotlib.pyplot as plt\n",
        "from typing import List, Dict, Tuple\n",
        "import random\n",
        "\n",
        "# Import our Stable Diffusion components\n",
        "from stable_diffusion_kanji import (\n",
        "    VAE, UNet2DConditionModel, DDPMScheduler, \n",
        "    StableDiffusionPipeline\n",
        ")\n",
        "\n",
        "class AdvancedConceptGenerator:\n",
        "    \"\"\"Advanced concept generator with semantic interpolation\"\"\"\n",
        "    \n",
        "    def __init__(self, model_path: str, device: str = 'cpu'):\n",
        "        self.device = device\n",
        "        self.model_path = Path(model_path)\n",
        "        \n",
        "        # Load trained models\n",
        "        self.load_trained_models()\n",
        "        \n",
        "        # Initialize pipeline\n",
        "        self.pipeline = StableDiffusionPipeline(device=device)\n",
        "        self.pipeline.vae = self.vae\n",
        "        self.pipeline.unet = self.unet\n",
        "        self.pipeline.text_encoder = self.text_encoder\n",
        "        self.pipeline.tokenizer = self.tokenizer\n",
        "        \n",
        "        # Concept database\n",
        "        self.concept_database = self.create_concept_database()\n",
        "        \n",
        "        print(\"\u2705 Advanced Concept Generator initialized\")\n",
        "    \n",
        "    def load_trained_models(self):\n",
        "        \"\"\"Load trained models from checkpoint\"\"\"\n",
        "        if not self.model_path.exists():\n",
        "            print(f\"\u274c Model checkpoint not found: {self.model_path}\")\n",
        "            print(\"Please train the model first using train_stable_diffusion.py\")\n",
        "            return\n",
        "        \n",
        "        checkpoint = torch.load(self.model_path, map_location=self.device)\n",
        "        \n",
        "        # Load VAE\n",
        "        self.vae = VAE().to(self.device)\n",
        "        self.vae.load_state_dict(checkpoint['vae_state_dict'])\n",
        "        \n",
        "        # Load UNet\n",
        "        self.unet = UNet2DConditionModel().to(self.device)\n",
        "        self.unet.load_state_dict(checkpoint['unet_state_dict'])\n",
        "        \n",
        "        # Load text encoder\n",
        "        self.text_encoder = CLIPTextModel.from_pretrained(\"openai/clip-vit-base-patch32\").to(self.device)\n",
        "        self.tokenizer = CLIPTokenizer.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
        "        \n",
        "        print(f\"\u2705 Models loaded from: {self.model_path}\")\n",
        "        print(f\"   \u2022 Epoch: {checkpoint['epoch']}\")\n",
        "        print(f\"   \u2022 Train loss: {checkpoint['train_loss']:.6f}\")\n",
        "        print(f\"   \u2022 Val loss: {checkpoint['val_loss']:.6f}\")\n",
        "    \n",
        "    def create_concept_database(self) -> Dict[str, Dict]:\n",
        "        \"\"\"Create database of modern and traditional concepts\"\"\"\n",
        "        \n",
        "        return {\n",
        "            # Modern Technology\n",
        "            'youtube': {\n",
        "                'description': 'video sharing platform, streaming, content creation',\n",
        "                'keywords': ['video', 'streaming', 'platform', 'content', 'media'],\n",
        "                'style': 'modern, digital, connected',\n",
        "                'prompts': [\n",
        "                    'kanji character meaning: video sharing platform',\n",
        "                    'kanji character meaning: streaming media content',\n",
        "                    'kanji character meaning: digital platform connection'\n",
        "                ]\n",
        "            },\n",
        "            'gundam': {\n",
        "                'description': 'giant robot, mecha, futuristic warfare',\n",
        "                'keywords': ['robot', 'mecha', 'giant', 'future', 'warfare'],\n",
        "                'style': 'futuristic, mechanical, powerful',\n",
        "                'prompts': [\n",
        "                    'kanji character meaning: giant robot mecha',\n",
        "                    'kanji character meaning: futuristic warfare machine',\n",
        "                    'kanji character meaning: mechanical giant warrior'\n",
        "                ]\n",
        "            },\n",
        "            'internet': {\n",
        "                'description': 'global network, connectivity, information',\n",
        "                'keywords': ['network', 'connectivity', 'global', 'information', 'digital'],\n",
        "                'style': 'connected, global, digital',\n",
        "                'prompts': [\n",
        "                    'kanji character meaning: global network connection',\n",
        "                    'kanji character meaning: digital information flow',\n",
        "                    'kanji character meaning: worldwide connectivity'\n",
        "                ]\n",
        "            },\n",
        "            'ai': {\n",
        "                'description': 'artificial intelligence, machine learning, automation',\n",
        "                'keywords': ['intelligence', 'machine', 'learning', 'automation', 'future'],\n",
        "                'style': 'intelligent, automated, futuristic',\n",
        "                'prompts': [\n",
        "                    'kanji character meaning: artificial intelligence',\n",
        "                    'kanji character meaning: machine learning automation',\n",
        "                    'kanji character meaning: intelligent automation'\n",
        "                ]\n",
        "            },\n",
        "            'crypto': {\n",
        "                'description': 'cryptocurrency, blockchain, digital money',\n",
        "                'keywords': ['digital', 'money', 'blockchain', 'secure', 'virtual'],\n",
        "                'style': 'digital, secure, virtual',\n",
        "                'prompts': [\n",
        "                    'kanji character meaning: digital cryptocurrency',\n",
        "                    'kanji character meaning: blockchain security',\n",
        "                    'kanji character meaning: virtual digital money'\n",
        "                ]\n",
        "            },\n",
        "            \n",
        "            # Traditional Concepts (for comparison)\n",
        "            'success': {\n",
        "                'description': 'achievement, accomplishment, victory',\n",
        "                'keywords': ['achieve', 'accomplish', 'complete', 'win', 'victory'],\n",
        "                'style': 'traditional, positive, upward',\n",
        "                'prompts': [\n",
        "                    'kanji character meaning: success achievement',\n",
        "                    'kanji character meaning: accomplish victory',\n",
        "                    'kanji character meaning: complete success'\n",
        "                ]\n",
        "            },\n",
        "            'culture': {\n",
        "                'description': 'tradition, heritage, wisdom',\n",
        "                'keywords': ['tradition', 'heritage', 'wisdom', 'ancient', 'knowledge'],\n",
        "                'style': 'traditional, cultural, ancient',\n",
        "                'prompts': [\n",
        "                    'kanji character meaning: cultural tradition',\n",
        "                    'kanji character meaning: ancient heritage wisdom',\n",
        "                    'kanji character meaning: traditional knowledge'\n",
        "                ]\n",
        "            }\n",
        "        }\n",
        "    \n",
        "    def generate_concept_kanji(self, concept: str, num_samples: int = 4, \n",
        "                             num_inference_steps: int = 50) -> List[torch.Tensor]:\n",
        "        \"\"\"Generate Kanji for a specific concept\"\"\"\n",
        "        \n",
        "        if concept not in self.concept_database:\n",
        "            print(f\"\u274c Concept '{concept}' not found in database\")\n",
        "            return []\n",
        "        \n",
        "        concept_info = self.concept_database[concept]\n",
        "        print(f\"\\n\ud83c\udfaf Generating Kanji for '{concept}'\")\n",
        "        print(f\"   \u2022 Description: {concept_info['description']}\")\n",
        "        print(f\"   \u2022 Style: {concept_info['style']}\")\n",
        "        \n",
        "        generated_images = []\n",
        "        \n",
        "        for i in range(num_samples):\n",
        "            # Select random prompt from concept\n",
        "            prompt = random.choice(concept_info['prompts'])\n",
        "            print(f\"   \u2022 Sample {i+1}: {prompt}\")\n",
        "            \n",
        "            try:\n",
        "                # Generate image\n",
        "                generated = self.pipeline.generate(\n",
        "                    prompt, \n",
        "                    num_inference_steps=num_inference_steps\n",
        "                )\n",
        "                generated_images.append(generated)\n",
        "                \n",
        "            except Exception as e:\n",
        "                print(f\"     \u274c Error: {e}\")\n",
        "        \n",
        "        return generated_images\n",
        "    \n",
        "    def semantic_interpolation(self, concept1: str, concept2: str, \n",
        "                             interpolation_steps: int = 5) -> List[torch.Tensor]:\n",
        "        \"\"\"Generate Kanji by interpolating between two concepts\"\"\"\n",
        "        \n",
        "        if concept1 not in self.concept_database or concept2 not in self.concept_database:\n",
        "            print(f\"\u274c One or both concepts not found in database\")\n",
        "            return []\n",
        "        \n",
        "        print(f\"\\n\ud83d\udd04 Semantic Interpolation: {concept1} \u2192 {concept2}\")\n",
        "        \n",
        "        # Get concept embeddings\n",
        "        concept1_prompt = random.choice(self.concept_database[concept1]['prompts'])\n",
        "        concept2_prompt = random.choice(self.concept_database[concept2]['prompts'])\n",
        "        \n",
        "        # Encode both prompts\n",
        "        tokens1 = self.pipeline.tokenizer(concept1_prompt, padding=True, return_tensors=\"pt\")\n",
        "        tokens2 = self.pipeline.tokenizer(concept2_prompt, padding=True, return_tensors=\"pt\")\n",
        "        \n",
        "        tokens1 = {k: v.to(self.device) for k, v in tokens1.items()}\n",
        "        tokens2 = {k: v.to(self.device) for k, v in tokens2.items()}\n",
        "        \n",
        "        with torch.no_grad():\n",
        "            emb1 = self.pipeline.text_encoder(**tokens1).last_hidden_state\n",
        "            emb2 = self.pipeline.text_encoder(**tokens2).last_hidden_state\n",
        "        \n",
        "        interpolated_images = []\n",
        "        \n",
        "        for i in range(interpolation_steps):\n",
        "            # Interpolate between embeddings\n",
        "            alpha = i / (interpolation_steps - 1)\n",
        "            interpolated_emb = alpha * emb2 + (1 - alpha) * emb1\n",
        "            \n",
        "            print(f\"   \u2022 Step {i+1}/{interpolation_steps} (\u03b1={alpha:.2f})\")\n",
        "            \n",
        "            try:\n",
        "                # Generate with interpolated embedding\n",
        "                # This is a simplified approach - in practice you'd need to modify the pipeline\n",
        "                # to accept custom embeddings\n",
        "                prompt = f\"interpolated between {concept1} and {concept2}\"\n",
        "                generated = self.pipeline.generate(prompt, num_inference_steps=30)\n",
        "                interpolated_images.append(generated)\n",
        "                \n",
        "            except Exception as e:\n",
        "                print(f\"     \u274c Error: {e}\")\n",
        "        \n",
        "        return interpolated_images\n",
        "    \n",
        "    def generate_modern_concepts(self, num_samples: int = 3) -> Dict[str, List[torch.Tensor]]:\n",
        "        \"\"\"Generate Kanji for all modern concepts\"\"\"\n",
        "        \n",
        "        modern_concepts = ['youtube', 'gundam', 'internet', 'ai', 'crypto']\n",
        "        results = {}\n",
        "        \n",
        "        print(f\"\\n\ud83d\ude80 Generating Modern Concept Kanji\")\n",
        "        print(\"=\" * 50)\n",
        "        \n",
        "        for concept in modern_concepts:\n",
        "            print(f\"\\n\ud83d\udcf1 {concept.upper()}\")\n",
        "            generated = self.generate_concept_kanji(concept, num_samples)\n",
        "            results[concept] = generated\n",
        "            \n",
        "            if generated:\n",
        "                print(f\"   \u2705 Generated {len(generated)} samples\")\n",
        "            else:\n",
        "                print(f\"   \u274c Failed to generate\")\n",
        "        \n",
        "        return results\n",
        "    \n",
        "    def save_generated_images(self, images: List[torch.Tensor], \n",
        "                            concept: str, save_dir: str = \"advanced_generated_results\"):\n",
        "        \"\"\"Save generated images\"\"\"\n",
        "        \n",
        "        save_path = Path(save_dir)\n",
        "        save_path.mkdir(exist_ok=True)\n",
        "        \n",
        "        saved_paths = []\n",
        "        \n",
        "        for i, img_tensor in enumerate(images):\n",
        "            try:\n",
        "                # Convert tensor to PIL image\n",
        "                if img_tensor.dim() == 4:\n",
        "                    img_tensor = img_tensor.squeeze(0)\n",
        "                \n",
        "                # Denormalize from [-1, 1] to [0, 1]\n",
        "                img_tensor = (img_tensor + 1) / 2\n",
        "                img_tensor = torch.clamp(img_tensor, 0, 1)\n",
        "                \n",
        "                # Convert to PIL\n",
        "                img_array = img_tensor.permute(1, 2, 0).numpy()\n",
        "                img = Image.fromarray((img_array * 255).astype(np.uint8))\n",
        "                \n",
        "                # Save\n",
        "                output_path = save_path / f\"{concept}_advanced_{i+1}.png\"\n",
        "                img.save(output_path)\n",
        "                saved_paths.append(output_path)\n",
        "                \n",
        "            except Exception as e:\n",
        "                print(f\"   \u274c Error saving image {i+1}: {e}\")\n",
        "        \n",
        "        return saved_paths\n",
        "    \n",
        "    def display_results(self, results: Dict[str, List[torch.Tensor]], \n",
        "                       save_dir: str = \"advanced_generated_results\"):\n",
        "        \"\"\"Display and save all generated results\"\"\"\n",
        "        \n",
        "        print(f\"\\n\ud83d\uddbc\ufe0f  Displaying Generated Results\")\n",
        "        \n",
        "        # Create subplot\n",
        "        concepts = list(results.keys())\n",
        "        max_samples = max(len(imgs) for imgs in results.values()) if results else 0\n",
        "        \n",
        "        if max_samples == 0:\n",
        "            print(\"\u274c No images to display\")\n",
        "            return\n",
        "        \n",
        "        fig, axes = plt.subplots(len(concepts), max_samples, \n",
        "                                figsize=(4*max_samples, 4*len(concepts)))\n",
        "        \n",
        "        if len(concepts) == 1:\n",
        "            axes = axes.reshape(1, -1)\n",
        "        \n",
        "        for i, concept in enumerate(concepts):\n",
        "            images = results[concept]\n",
        "            \n",
        "            for j in range(max_samples):\n",
        "                if j < len(images):\n",
        "                    # Display image\n",
        "                    img_tensor = images[j]\n",
        "                    if img_tensor.dim() == 4:\n",
        "                        img_tensor = img_tensor.squeeze(0)\n",
        "                    \n",
        "                    # Denormalize\n",
        "                    img_tensor = (img_tensor + 1) / 2\n",
        "                    img_tensor = torch.clamp(img_tensor, 0, 1)\n",
        "                    \n",
        "                    img_array = img_tensor.permute(1, 2, 0).numpy()\n",
        "                    \n",
        "                    axes[i, j].imshow(img_array)\n",
        "                    axes[i, j].set_title(f'{concept.title()} #{j+1}', fontsize=10)\n",
        "                    axes[i, j].axis('off')\n",
        "                else:\n",
        "                    axes[i, j].text(0.5, 0.5, 'No image', ha='center', va='center')\n",
        "                    axes[i, j].axis('off')\n",
        "        \n",
        "        plt.tight_layout()\n",
        "        \n",
        "        # Save combined results\n",
        "        output_path = Path(save_dir) / \"all_advanced_concepts.png\"\n",
        "        plt.savefig(output_path, dpi=150, bbox_inches='tight')\n",
        "        print(f\"\ud83d\udcbe Combined results saved to: {output_path}\")\n",
        "        \n",
        "        plt.show()\n",
        "        \n",
        "        # Save individual images\n",
        "        for concept, images in results.items():\n",
        "            if images:\n",
        "                saved_paths = self.save_generated_images(images, concept, save_dir)\n",
        "                print(f\"\ud83d\udcbe {concept}: {len(saved_paths)} images saved\")\n",
        "\n",
        "def main():\n",
        "    \"\"\"Main function\"\"\"\n",
        "    \n",
        "    print(\"\ud83c\udf8c Advanced Concept Generation\")\n",
        "    print(\"=\" * 50)\n",
        "    \n",
        "    # Check if trained model exists\n",
        "    model_paths = [\n",
        "        \"stable_diffusion_results/best_model.pth\",\n",
        "        \"stable_diffusion_results/stable_diffusion_epoch_10.pth\",\n",
        "        \"stable_diffusion_results/stable_diffusion_epoch_5.pth\"\n",
        "    ]\n",
        "    \n",
        "    model_path = None\n",
        "    for path in model_paths:\n",
        "        if Path(path).exists():\n",
        "            model_path = path\n",
        "            break\n",
        "    \n",
        "    if not model_path:\n",
        "        print(\"\u274c No trained model found!\")\n",
        "        print(\"Please train the model first using train_stable_diffusion.py\")\n",
        "        return\n",
        "    \n",
        "    # Initialize generator\n",
        "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "    generator = AdvancedConceptGenerator(model_path, device)\n",
        "    \n",
        "    # Generate modern concepts\n",
        "    results = generator.generate_modern_concepts(num_samples=3)\n",
        "    \n",
        "    # Display and save results\n",
        "    generator.display_results(results)\n",
        "    \n",
        "    # Test semantic interpolation\n",
        "    print(f\"\\n\ud83d\udd04 Testing Semantic Interpolation...\")\n",
        "    try:\n",
        "        interpolated = generator.semantic_interpolation('youtube', 'gundam', 3)\n",
        "        if interpolated:\n",
        "            generator.save_generated_images(interpolated, 'youtube_gundam_interpolation')\n",
        "            print(f\"\u2705 Interpolation completed: {len(interpolated)} images\")\n",
        "    except Exception as e:\n",
        "        print(f\"\u274c Interpolation failed: {e}\")\n",
        "    \n",
        "    print(f\"\\n\ud83c\udf89 Advanced Concept Generation Complete!\")\n",
        "    print(f\"   \u2022 Generated Kanji for {len(results)} modern concepts\")\n",
        "    print(f\"   \u2022 Images saved in: advanced_generated_results/\")\n",
        "    print(f\"   \u2022 Ready for modern concept Kanji generation!\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "outputs": [],
      "execution_count": null
    }
  ],
  "metadata": {
    "anaconda-cloud": {},
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}