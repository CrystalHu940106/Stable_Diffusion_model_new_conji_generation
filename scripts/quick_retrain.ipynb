{
  "cells": [
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#!/usr/bin/env python3\n",
        "\"\"\"\n",
        "Quick retraining script to fix VAE architecture\n",
        "\"\"\"\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms\n",
        "import json\n",
        "from pathlib import Path\n",
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "from stable_diffusion_kanji import VAE, UNet2DConditionModel, DDPMScheduler\n",
        "from PIL import Image\n",
        "\n",
        "class KanjiDataset(Dataset):\n",
        "    def __init__(self, dataset_path, transform=None, max_samples=None):\n",
        "        self.dataset_path = Path(dataset_path)\n",
        "        self.transform = transform\n",
        "        \n",
        "        # Load metadata\n",
        "        metadata_path = self.dataset_path / \"metadata\" / \"dataset.json\"\n",
        "        with open(metadata_path, 'r', encoding='utf-8') as f:\n",
        "            self.data = json.load(f)\n",
        "        \n",
        "        # Limit samples if specified\n",
        "        if max_samples:\n",
        "            self.data = self.data[:max_samples]\n",
        "        \n",
        "        print(f\"\ud83d\udcda Loaded {len(self.data)} Kanji entries\")\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        entry = self.data[idx]\n",
        "        \n",
        "        # Load image\n",
        "        image_path = self.dataset_path / \"images\" / entry['image_file']\n",
        "        image = Image.open(image_path).convert('RGB')\n",
        "        \n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "        \n",
        "        return image\n",
        "\n",
        "def quick_retrain():\n",
        "    \"\"\"Quick retraining with fixed VAE architecture\"\"\"\n",
        "    print(\"\ud83d\ude80 Quick Retraining with Fixed VAE Architecture\")\n",
        "    print(\"=\" * 50)\n",
        "    \n",
        "    # Configuration\n",
        "    device = 'mps' if torch.backends.mps.is_available() else 'cpu'\n",
        "    batch_size = 4\n",
        "    num_epochs = 5  # Quick training\n",
        "    learning_rate = 1e-4\n",
        "    \n",
        "    print(f\"\ud83d\udd27 Configuration:\")\n",
        "    print(f\"   \u2022 Device: {device}\")\n",
        "    print(f\"   \u2022 Batch Size: {batch_size}\")\n",
        "    print(f\"   \u2022 Epochs: {num_epochs}\")\n",
        "    print(f\"   \u2022 Learning Rate: {learning_rate}\")\n",
        "    \n",
        "    # Data transforms\n",
        "    transform = transforms.Compose([\n",
        "        transforms.Resize((128, 128)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
        "    ])\n",
        "    \n",
        "    # Load dataset\n",
        "    dataset_path = \"data/fixed_kanji_dataset\"\n",
        "    dataset = KanjiDataset(dataset_path, transform=transform, max_samples=500)  # Reduced for quick training\n",
        "    \n",
        "    # Split dataset\n",
        "    train_size = int(0.8 * len(dataset))\n",
        "    val_size = len(dataset) - train_size\n",
        "    train_dataset, val_dataset = torch.utils.data.random_split(dataset, [train_size, val_size])\n",
        "    \n",
        "    # Create dataloaders\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
        "    \n",
        "    print(f\"\ud83d\udcca Dataset: {len(dataset)} total, {len(train_dataset)} train, {len(val_dataset)} val\")\n",
        "    \n",
        "    # Initialize models with fixed architecture\n",
        "    vae = VAE(hidden_dims=[128, 256, 512, 1024]).to(device)\n",
        "    unet = UNet2DConditionModel(\n",
        "        model_channels=256,\n",
        "        num_res_blocks=3,\n",
        "        channel_mult=(1, 2, 4, 8),\n",
        "        attention_resolutions=(8,),\n",
        "        num_heads=16\n",
        "    ).to(device)\n",
        "    \n",
        "    # Test VAE dimensions\n",
        "    test_input = torch.randn(1, 3, 128, 128).to(device)\n",
        "    latents, mu, logvar, kl_loss = vae.encode(test_input)\n",
        "    reconstructed = vae.decode(latents)\n",
        "    print(f\"\ud83e\uddea VAE Test: input {test_input.shape} \u2192 latents {latents.shape} \u2192 output {reconstructed.shape}\")\n",
        "    \n",
        "    if reconstructed.shape[-2:] != test_input.shape[-2:]:\n",
        "        print(\"\u274c VAE output dimensions still incorrect!\")\n",
        "        return\n",
        "    \n",
        "    print(\"\u2705 VAE architecture is correct!\")\n",
        "    \n",
        "    # Initialize scheduler\n",
        "    scheduler = DDPMScheduler(num_train_timesteps=1000)\n",
        "    \n",
        "    # Optimizers\n",
        "    vae_optimizer = torch.optim.AdamW(vae.parameters(), lr=learning_rate)\n",
        "    unet_optimizer = torch.optim.AdamW(unet.parameters(), lr=learning_rate/10)\n",
        "    \n",
        "    # Loss function\n",
        "    mse_loss = nn.MSELoss()\n",
        "    \n",
        "    # Training loop\n",
        "    print(f\"\\n\ud83d\ude80 Starting quick training for {num_epochs} epochs...\")\n",
        "    \n",
        "    for epoch in range(1, num_epochs + 1):\n",
        "        print(f\"\\n{'='*20} Epoch {epoch}/{num_epochs} {'='*20}\")\n",
        "        \n",
        "        # Training\n",
        "        vae.train()\n",
        "        unet.train()\n",
        "        total_loss = 0\n",
        "        \n",
        "        for batch_idx, images in enumerate(tqdm(train_loader, desc=f\"Epoch {epoch}\")):\n",
        "            images = images.to(device)\n",
        "            \n",
        "            # VAE reconstruction\n",
        "            latents, mu, logvar, kl_loss = vae.encode(images)\n",
        "            reconstructed = vae.decode(latents)\n",
        "            recon_loss = mse_loss(reconstructed, images)\n",
        "            \n",
        "            # UNet noise prediction (simplified)\n",
        "            timesteps = torch.randint(0, scheduler.num_train_timesteps, (images.shape[0],), device=device)\n",
        "            noise = torch.randn_like(latents)\n",
        "            noisy_latents = scheduler.add_noise(latents, noise, timesteps)\n",
        "            noise_pred = unet(noisy_latents, timesteps, torch.randn(images.shape[0], 77, 512).to(device))\n",
        "            noise_loss = mse_loss(noise_pred, noise)\n",
        "            \n",
        "            # Total loss\n",
        "            loss = recon_loss + 0.01 * kl_loss + noise_loss\n",
        "            \n",
        "            # Backward pass\n",
        "            vae_optimizer.zero_grad()\n",
        "            unet_optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            vae_optimizer.step()\n",
        "            unet_optimizer.step()\n",
        "            \n",
        "            total_loss += loss.item()\n",
        "            \n",
        "            if batch_idx % 50 == 0:\n",
        "                print(f\"   Batch {batch_idx}: Loss={loss.item():.6f}, Recon={recon_loss.item():.6f}, Noise={noise_loss.item():.6f}\")\n",
        "        \n",
        "        avg_loss = total_loss / len(train_loader)\n",
        "        print(f\"\ud83d\udcca Epoch {epoch} Average Loss: {avg_loss:.6f}\")\n",
        "        \n",
        "        # Save checkpoint every epoch\n",
        "        checkpoint = {\n",
        "            'epoch': epoch,\n",
        "            'vae_state_dict': vae.state_dict(),\n",
        "            'unet_state_dict': unet.state_dict(),\n",
        "            'vae_optimizer_state_dict': vae_optimizer.state_dict(),\n",
        "            'unet_optimizer_state_dict': unet_optimizer.state_dict(),\n",
        "            'loss': avg_loss\n",
        "        }\n",
        "        torch.save(checkpoint, f'fixed_vae_checkpoint_epoch_{epoch}.pth')\n",
        "        print(f\"\ud83d\udcbe Checkpoint saved: fixed_vae_checkpoint_epoch_{epoch}.pth\")\n",
        "    \n",
        "    print(f\"\\n\ud83c\udf89 Quick retraining completed!\")\n",
        "    print(f\"\ud83d\udcbe Final model saved as: fixed_vae_checkpoint_epoch_{num_epochs}.pth\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    quick_retrain()\n"
      ],
      "outputs": [],
      "execution_count": null
    }
  ],
  "metadata": {
    "anaconda-cloud": {},
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}