#!/usr/bin/env python3
"""
æ”¹è¿›çš„Stable Diffusionå®ç°
å€Ÿé‰´å®˜æ–¹CompVis/stable-diffusionçš„æœ€ä½³å®è·µ
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
from transformers import CLIPTokenizer, CLIPTextModel
import math
from typing import Optional, Union, Tuple
import numpy as np

class ImprovedVAE(nn.Module):
    """
    æ”¹è¿›çš„VAEå®ç°ï¼Œå€Ÿé‰´å®˜æ–¹æ¶æ„
    """
    def __init__(self, in_channels=3, latent_channels=4, hidden_dims=[128, 256, 512, 1024]):
        super().__init__()
        self.latent_channels = latent_channels
        
        # Encoder - ä½¿ç”¨æ›´æ·±çš„ç½‘ç»œ
        encoder_layers = []
        in_ch = in_channels
        for h_dim in hidden_dims:
            # è®¡ç®—åˆé€‚çš„GroupNormç»„æ•°
            num_groups = min(32, h_dim)
            while h_dim % num_groups != 0 and num_groups > 1:
                num_groups -= 1
            
            encoder_layers.extend([
                nn.Conv2d(in_ch, h_dim, kernel_size=3, stride=2, padding=1),
                nn.GroupNorm(num_groups, h_dim),  # ä½¿ç”¨GroupNormæ›¿ä»£BatchNorm
                nn.SiLU()  # ä½¿ç”¨SiLUæ›¿ä»£LeakyReLU
            ])
            in_ch = h_dim
        
        # Final encoding layer
        final_channels = latent_channels * 2
        num_groups = min(32, final_channels)
        while final_channels % num_groups != 0 and num_groups > 1:
            num_groups -= 1
        
        encoder_layers.extend([
            nn.Conv2d(hidden_dims[-1], final_channels, kernel_size=3, padding=1),
            nn.GroupNorm(num_groups, final_channels)
        ])
        
        self.encoder = nn.Sequential(*encoder_layers)
        
        # Decoder - ç¡®ä¿ç²¾ç¡®çš„128x128è¾“å‡º
        decoder_layers = []
        in_ch = latent_channels
        
        # ä½¿ç”¨hidden_dimsçš„ååºè¿›è¡Œä¸Šé‡‡æ ·
        hidden_dims_rev = hidden_dims[::-1]
        
        for i, h_dim in enumerate(hidden_dims_rev):
            # è®¡ç®—åˆé€‚çš„GroupNormç»„æ•°
            num_groups = min(32, h_dim)
            while h_dim % num_groups != 0 and num_groups > 1:
                num_groups -= 1
            
            decoder_layers.extend([
                nn.ConvTranspose2d(in_ch, h_dim, kernel_size=4, stride=2, padding=1),
                nn.GroupNorm(num_groups, h_dim),
                nn.SiLU()
            ])
            in_ch = h_dim
        
        # æœ€ç»ˆè¾“å‡ºå±‚
        decoder_layers.extend([
            nn.Conv2d(in_ch, in_channels, kernel_size=3, stride=1, padding=1),
            nn.Tanh()
        ])
        
        self.decoder = nn.Sequential(*decoder_layers)
        
    def encode(self, x):
        # ç¡®ä¿è¾“å…¥æ˜¯128x128
        if x.shape[-1] != 128:
            x = F.interpolate(x, size=(128, 128), mode='bilinear', align_corners=False)
        
        # ç¼–ç åˆ°æ½œåœ¨ç©ºé—´
        encoded = self.encoder(x)
        mu, logvar = torch.chunk(encoded, 2, dim=1)
        
        # KLæ•£åº¦æŸå¤±
        kl_loss = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())
        
        # é‡å‚æ•°åŒ–æŠ€å·§
        std = torch.exp(0.5 * logvar)
        eps = torch.randn_like(std)
        z = mu + eps * std
        
        return z, mu, logvar, kl_loss
    
    def decode(self, z):
        return self.decoder(z)

class ImprovedCrossAttention(nn.Module):
    """
    æ”¹è¿›çš„äº¤å‰æ³¨æ„åŠ›å®ç°ï¼Œå€Ÿé‰´å®˜æ–¹ç‰ˆæœ¬
    """
    def __init__(self, query_dim, context_dim=None, heads=8, dim_head=64, dropout=0.0):
        super().__init__()
        inner_dim = dim_head * heads
        context_dim = context_dim if context_dim is not None else query_dim
        
        self.scale = dim_head ** -0.5
        self.heads = heads
        
        self.to_q = nn.Linear(query_dim, inner_dim, bias=False)
        self.to_k = nn.Linear(context_dim, inner_dim, bias=False)
        self.to_v = nn.Linear(context_dim, inner_dim, bias=False)
        
        self.to_out = nn.Sequential(
            nn.Linear(inner_dim, query_dim),
            nn.Dropout(dropout)
        )
        
    def forward(self, x, context=None):
        context = context if context is not None else x
        
        # ä¿å­˜åŸå§‹è¾“å…¥å½¢çŠ¶
        original_shape = x.shape
        
        # å¤„ç†4Dè¾“å…¥ (B, C, H, W) -> (B, H*W, C)
        if x.dim() == 4:
            B, C, H, W = x.shape
            x = x.view(B, C, H * W).transpose(1, 2)  # (B, H*W, C)
        
        # å¤„ç†ä¸Šä¸‹æ–‡
        if context.dim() == 4:
            B, C, H, W = context.shape
            context = context.view(B, C, H * W).transpose(1, 2)  # (B, H*W, C)
        
        q = self.to_q(x)
        k = self.to_k(context)
        v = self.to_v(context)
        
        # é‡å¡‘ä¸ºå¤šå¤´æ³¨æ„åŠ›
        q = q.view(q.shape[0], -1, self.heads, q.shape[-1] // self.heads).transpose(1, 2)
        k = k.view(k.shape[0], -1, self.heads, k.shape[-1] // self.heads).transpose(1, 2)
        v = v.view(v.shape[0], -1, self.heads, v.shape[-1] // self.heads).transpose(1, 2)
        
        # è®¡ç®—æ³¨æ„åŠ›
        scores = torch.matmul(q, k.transpose(-2, -1)) * self.scale
        attn = F.softmax(scores, dim=-1)
        
        # åº”ç”¨æ³¨æ„åŠ›
        out = torch.matmul(attn, v)
        out = out.transpose(1, 2).contiguous().view(out.shape[0], -1, out.shape[-1] * self.heads)
        
        # åº”ç”¨è¾“å‡ºæŠ•å½±
        out = self.to_out(out)
        
        # è½¬æ¢å›4Dæ ¼å¼ (B, H*W, C) -> (B, C, H, W)
        if len(original_shape) == 4:
            B, C, H, W = original_shape
            out = out.transpose(1, 2).view(B, C, H, W)
        
        return out

class ImprovedResBlock(nn.Module):
    """
    æ”¹è¿›çš„æ®‹å·®å—ï¼Œå€Ÿé‰´å®˜æ–¹å®ç°
    """
    def __init__(self, channels, time_dim, dropout=0.0):
        super().__init__()
        
        # åŠ¨æ€è®¡ç®—GroupNormçš„ç»„æ•°ï¼Œç¡®ä¿channelsèƒ½è¢«num_groupsæ•´é™¤
        if channels >= 32:
            num_groups = min(32, channels // (channels // 32))
        elif channels >= 16:
            num_groups = min(16, channels // (channels // 16))
        elif channels >= 8:
            num_groups = min(8, channels // (channels // 8))
        elif channels >= 4:
            num_groups = min(4, channels // (channels // 4))
        else:
            num_groups = 1
        
        # ç¡®ä¿num_groupsèƒ½æ•´é™¤channels
        while channels % num_groups != 0 and num_groups > 1:
            num_groups -= 1
        
        self.time_mlp = nn.Sequential(
            nn.SiLU(),
            nn.Linear(time_dim, channels)
        )
        
        self.block1 = nn.Sequential(
            nn.GroupNorm(num_groups, channels),
            nn.SiLU(),
            nn.Conv2d(channels, channels, 3, padding=1)
        )
        
        self.block2 = nn.Sequential(
            nn.GroupNorm(num_groups, channels),
            nn.SiLU(),
            nn.Dropout(dropout),
            nn.Conv2d(channels, channels, 3, padding=1)
        )
        
        # æ—¶é—´åµŒå…¥æŠ•å½±
        self.time_proj = nn.Linear(time_dim, channels)
        
    def forward(self, x, time_emb):
        h = self.block1(x)
        
        # æ—¶é—´åµŒå…¥å¤„ç†
        time_emb = self.time_proj(time_emb)
        time_emb = time_emb.view(x.shape[0], -1, 1, 1)
        h = h + time_emb
        
        h = self.block2(h)
        return h + x

class ImprovedUNet2DConditionModel(nn.Module):
    """
    ç®€åŒ–çš„UNetå®ç°ï¼Œé¿å…å¤æ‚çš„è·³è·ƒè¿æ¥
    """
    def __init__(self, in_channels=4, out_channels=4, model_channels=128, num_res_blocks=2, 
                 attention_resolutions=(8, 16), dropout=0.1, channel_mult=(1, 2, 4, 8), 
                 conv_resample=True, num_heads=8, context_dim=512):
        super().__init__()
        
        self.in_channels = in_channels
        self.model_channels = model_channels
        self.num_res_blocks = num_res_blocks
        self.attention_resolutions = attention_resolutions
        self.dropout = dropout
        self.channel_mult = channel_mult
        self.conv_resample = conv_resample
        self.num_heads = num_heads
        self.context_dim = context_dim
        
        # æ—¶é—´åµŒå…¥
        time_embed_dim = model_channels * 4
        self.time_embedding = nn.Sequential(
            nn.Linear(1, time_embed_dim),
            nn.SiLU(),
            nn.Linear(time_embed_dim, time_embed_dim),
            nn.SiLU(),
            nn.Linear(time_embed_dim, time_embed_dim)
        )
        
        # è¾“å…¥æŠ•å½±
        self.input_proj = nn.Conv2d(in_channels, model_channels, kernel_size=3, padding=1)
        
        # ç¼–ç å™¨å—
        self.encoder_blocks = nn.ModuleList()
        ch = model_channels
        
        for level, mult in enumerate(channel_mult):
            # ResBlock
            for _ in range(num_res_blocks):
                self.encoder_blocks.append(ImprovedResBlock(ch, time_embed_dim, dropout))
            
            # CrossAttention
            if level in attention_resolutions:
                self.encoder_blocks.append(ImprovedCrossAttention(ch, context_dim, num_heads, dropout=dropout))
            
            # ä¸‹é‡‡æ ·
            if level < len(channel_mult) - 1:
                ch = mult * model_channels
                self.encoder_blocks.append(nn.Conv2d(self.encoder_blocks[-1].block1[0].num_features, ch, 3, stride=2, padding=1))
        
        # ä¸­é—´å—
        self.middle_block = nn.ModuleList([
            ImprovedResBlock(ch, time_embed_dim, dropout),
            ImprovedCrossAttention(ch, context_dim, num_heads, dropout=dropout),
            ImprovedResBlock(ch, time_embed_dim, dropout)
        ])
        
        # è§£ç å™¨å—
        self.decoder_blocks = nn.ModuleList()
        
        for level, mult in list(enumerate(channel_mult))[::-1]:
            # ä¸Šé‡‡æ ·
            if level < len(channel_mult) - 1:
                ch = ch // 2
                self.decoder_blocks.append(nn.ConvTranspose2d(ch * 2, ch, 4, stride=2, padding=1))
            
            # ResBlock
            for _ in range(num_res_blocks + 1):
                self.decoder_blocks.append(ImprovedResBlock(ch, time_embed_dim, dropout))
            
            # CrossAttention
            if level in attention_resolutions:
                self.decoder_blocks.append(ImprovedCrossAttention(ch, context_dim, num_heads, dropout=dropout))
        
        # è¾“å‡ºæŠ•å½±
        self.out = nn.Sequential(
            nn.GroupNorm(min(32, ch), ch),
            nn.SiLU(),
            nn.Conv2d(ch, out_channels, kernel_size=3, padding=1)
        )
        
    def forward(self, x, timesteps, context=None):
        # æ—¶é—´åµŒå…¥
        t = self.time_embedding(timesteps.unsqueeze(-1).float())
        if t.dim() == 1:
            t = t.unsqueeze(0)
        
        # è¾“å…¥æŠ•å½±
        h = self.input_proj(x)
        
        # ç¼–ç å™¨
        for module in self.encoder_blocks:
            if isinstance(module, ImprovedCrossAttention):
                h = module(h, context)
            elif isinstance(module, ImprovedResBlock):
                h = module(h, t)
            else:
                h = module(h)
        
        # ä¸­é—´å—
        for module in self.middle_block:
            if isinstance(module, ImprovedCrossAttention):
                h = module(h, context)
            else:
                h = module(h, t)
        
        # è¾“å‡ºå—
        for module in self.output_blocks:
            if isinstance(module, nn.ModuleList):
                # å¤„ç†ModuleListä¸­çš„æ¨¡å—
                for submodule in module:
                    if isinstance(submodule, ImprovedCrossAttention):
                        h = submodule(h, context)
                    elif isinstance(submodule, ImprovedResBlock):
                        h = submodule(h, t)
                    else:
                        h = submodule(h)
            else:
                # ç›´æ¥å¤„ç†å•ä¸ªæ¨¡å—
                if isinstance(module, ImprovedCrossAttention):
                    h = module(h, context)
                elif isinstance(module, ImprovedResBlock):
                    h = module(h, t)
                else:
                    h = module(h)
            
            # è·³è·ƒè¿æ¥
            if hs:
                skip_h = hs.pop()
                # ç®€å•çš„è·³è·ƒè¿æ¥ï¼Œä¸è¿›è¡Œé€šé“è°ƒæ•´
                h = torch.cat([h, skip_h], dim=1)
        
        return self.out(h)

class ImprovedDDPMScheduler:
    """
    æ”¹è¿›çš„DDPMè°ƒåº¦å™¨ï¼Œå€Ÿé‰´å®˜æ–¹å®ç°
    """
    def __init__(self, num_train_timesteps=1000, beta_start=0.0001, beta_end=0.02):
        self.num_train_timesteps = num_train_timesteps
        
        # çº¿æ€§å™ªå£°è°ƒåº¦
        self.betas = torch.linspace(beta_start, beta_end, num_train_timesteps)
        self.alphas = 1.0 - self.betas
        self.alphas_cumprod = torch.cumprod(self.alphas, dim=0)
        self.alphas_cumprod_prev = torch.cat([torch.tensor([1.0]), self.alphas_cumprod[:-1]])
        
        # è®¡ç®—å™ªå£°é¢„æµ‹çš„ç³»æ•°
        self.sqrt_alphas_cumprod = torch.sqrt(self.alphas_cumprod)
        self.sqrt_one_minus_alphas_cumprod = torch.sqrt(1.0 - self.alphas_cumprod)
        
    def add_noise(self, original_samples, noise, timesteps):
        """æ·»åŠ å™ªå£°åˆ°åŸå§‹æ ·æœ¬"""
        sqrt_alpha = self.sqrt_alphas_cumprod[timesteps].view(-1, 1, 1, 1)
        sqrt_one_minus_alpha = self.sqrt_one_minus_alphas_cumprod[timesteps].view(-1, 1, 1, 1)
        
        return sqrt_alpha * original_samples + sqrt_one_minus_alpha * noise
    
    def step(self, model_output, timestep, sample):
        """å»å™ªæ­¥éª¤"""
        alpha = self.alphas_cumprod[timestep].view(-1, 1, 1, 1)
        alpha_prev = self.alphas_cumprod_prev[timestep].view(-1, 1, 1, 1)
        
        # é¢„æµ‹x0
        pred_original_sample = (sample - torch.sqrt(1 - alpha) * model_output) / torch.sqrt(alpha)
        
        # é¢„æµ‹å‰ä¸€ä¸ªæ ·æœ¬
        pred_sample_direction = torch.sqrt(1 - alpha_prev) * model_output
        pred_prev_sample = torch.sqrt(alpha_prev) * pred_original_sample + pred_sample_direction
        
        return pred_prev_sample
    
    def set_timesteps(self, num_inference_steps):
        """è®¾ç½®æ¨ç†æ—¶é—´æ­¥"""
        self.num_inference_steps = num_inference_steps
        step_ratio = self.num_train_timesteps // num_inference_steps
        timesteps = (torch.arange(0, num_inference_steps) * step_ratio).flip(0)
        return timesteps

class ImprovedStableDiffusionPipeline:
    """
    æ”¹è¿›çš„Stable Diffusion Pipelineï¼Œå€Ÿé‰´å®˜æ–¹æœ€ä½³å®è·µ
    """
    def __init__(self, device='cuda' if torch.cuda.is_available() else 'cpu'):
        self.device = device
        
        # åˆå§‹åŒ–ç»„ä»¶
        self.vae = ImprovedVAE().to(device)
        self.unet = ImprovedUNet2DConditionModel(
            in_channels=4,
            out_channels=4,
            model_channels=128,
            channel_mult=(1, 2, 4, 8),
            attention_resolutions=(8, 16),
            context_dim=512
        ).to(device)
        self.scheduler = ImprovedDDPMScheduler()
        
        # CLIPæ–‡æœ¬ç¼–ç å™¨
        self.tokenizer = CLIPTokenizer.from_pretrained("openai/clip-vit-base-patch32")
        self.text_encoder = CLIPTextModel.from_pretrained("openai/clip-vit-base-patch32").to(device)
        
        # è®¾ç½®ä¸ºè¯„ä¼°æ¨¡å¼
        self.text_encoder.eval()
        self.vae.eval()
        
    def _encode_prompt(self, prompt):
        """ç¼–ç æ–‡æœ¬æç¤º"""
        tokens = self.tokenizer(prompt, padding=True, return_tensors="pt").to(self.device)
        with torch.no_grad():
            text_embeddings = self.text_encoder(**tokens).last_hidden_state
        return text_embeddings
    
    def _parse_kanji_prompt(self, prompt):
        """è§£ææ±‰å­—æç¤ºï¼Œä½¿ç”¨æ›´è¯¦ç»†çš„æè¿°"""
        base_prompt = f"kanji character representing {prompt}, traditional calligraphy style, black ink on white paper, high contrast, detailed strokes, clear lines, professional quality, artistic interpretation"
        return base_prompt
    
    def generate(self, prompt, height=128, width=128, num_inference_steps=50, 
                guidance_scale=7.5, seed=None):
        """ç”Ÿæˆå›¾åƒï¼Œä½¿ç”¨å®˜æ–¹æ¨èçš„å‚æ•°"""
        
        # è®¾ç½®éšæœºç§å­
        if seed is not None:
            torch.manual_seed(seed)
            torch.cuda.manual_seed(seed)
        
        # ç¼–ç æç¤º
        text_embeddings = self._encode_prompt(self._parse_kanji_prompt(prompt))
        
        # åˆå§‹åŒ–æ½œåœ¨å˜é‡
        latent_height = height // 8
        latent_width = width // 8
        latents = torch.randn(1, 4, latent_height, latent_width, device=self.device)
        
        # è®¾ç½®æ—¶é—´æ­¥
        timesteps = self.scheduler.set_timesteps(num_inference_steps)
        timesteps = timesteps.to(self.device)
        
        # æ”¹è¿›çš„å»å™ªå¾ªç¯
        for i, t in enumerate(timesteps):
            # æ‰©å±•æ½œåœ¨å˜é‡ç”¨äºæ‰¹å¤„ç†
            latent_model_input = torch.cat([latents] * 2)
            t_expanded = t.expand(2)
            
            # é¢„æµ‹å™ªå£°
            with torch.no_grad():
                noise_pred = self.unet(latent_model_input, t_expanded, text_embeddings)
            
            # æ‰§è¡Œclassifier-free guidance
            noise_pred_uncond, noise_pred_text = noise_pred.chunk(2)
            
            # ä½¿ç”¨å®˜æ–¹æ¨èçš„guidance scale
            guidance_scale = torch.clamp(torch.tensor(guidance_scale), min=1.0, max=20.0)
            noise_pred = noise_pred_uncond + guidance_scale * (noise_pred_text - noise_pred_uncond)
            
            # è®¡ç®—å‰ä¸€ä¸ªæ ·æœ¬
            latents = self.scheduler.step(noise_pred, t, latents)
        
        # è§£ç æ½œåœ¨å˜é‡
        with torch.no_grad():
            image = self.vae.decode(latents)
        
        return image

if __name__ == "__main__":
    print("ğŸŒ æ”¹è¿›çš„Stable Diffusionå®ç°")
    print("=" * 50)
    
    # æµ‹è¯•æ¨¡å‹
    device = 'cuda' if torch.cuda.is_available() else 'cpu'
    print(f"ğŸ”§ ä½¿ç”¨è®¾å¤‡: {device}")
    
    try:
        pipeline = ImprovedStableDiffusionPipeline(device=device)
        print("âœ… æ¨¡å‹åˆå§‹åŒ–æˆåŠŸ")
        
        # æµ‹è¯•ç”Ÿæˆ
        print("ğŸŒŠ æµ‹è¯•ç”Ÿæˆ...")
        result = pipeline.generate(
            "water",
            height=128,
            width=128,
            num_inference_steps=50,
            guidance_scale=7.5,
            seed=42
        )
        print("âœ… ç”Ÿæˆå®Œæˆ")
        
    except Exception as e:
        print(f"âŒ é”™è¯¯: {e}")
