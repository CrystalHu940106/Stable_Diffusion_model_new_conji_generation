{
  "cells": [
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#!/usr/bin/env python3\n",
        "\"\"\"\n",
        "Verify Kanji Dataset\n",
        "\n",
        "This script verifies the generated dataset and shows some examples.\n",
        "\"\"\"\n",
        "\n",
        "import json\n",
        "import os\n",
        "from pathlib import Path\n",
        "from PIL import Image\n",
        "\n",
        "def verify_dataset():\n",
        "    \"\"\"Verify the dataset structure and show examples\"\"\"\n",
        "    \n",
        "    dataset_path = Path(\"kanji_dataset\")\n",
        "    metadata_path = dataset_path / \"metadata\" / \"dataset.json\"\n",
        "    images_path = dataset_path / \"images\"\n",
        "    \n",
        "    print(\"=== Kanji Dataset Verification ===\")\n",
        "    \n",
        "    # Check if files exist\n",
        "    if not metadata_path.exists():\n",
        "        print(\"\u274c Dataset metadata not found!\")\n",
        "        return\n",
        "    \n",
        "    if not images_path.exists():\n",
        "        print(\"\u274c Images directory not found!\")\n",
        "        return\n",
        "    \n",
        "    # Load dataset\n",
        "    with open(metadata_path, 'r', encoding='utf-8') as f:\n",
        "        dataset = json.load(f)\n",
        "    \n",
        "    print(f\"\u2705 Dataset loaded: {len(dataset)} entries\")\n",
        "    \n",
        "    # Check image files\n",
        "    image_files = list(images_path.glob(\"*.png\"))\n",
        "    print(f\"\u2705 Images found: {len(image_files)} files\")\n",
        "    \n",
        "    # Show some examples\n",
        "    print(\"\\n=== Example Entries ===\")\n",
        "    for i, entry in enumerate(dataset[:10]):\n",
        "        kanji = entry['kanji']\n",
        "        meanings = entry['meanings'][:3]  # Show first 3 meanings\n",
        "        image_file = entry['image_file']\n",
        "        \n",
        "        # Check if image exists\n",
        "        image_path = images_path / image_file\n",
        "        image_exists = image_path.exists()\n",
        "        \n",
        "        print(f\"{i+1:2d}. {kanji} ({entry['unicode']}): {', '.join(meanings)}\")\n",
        "        print(f\"    Image: {image_file} {'\u2705' if image_exists else '\u274c'}\")\n",
        "        \n",
        "        if image_exists:\n",
        "            # Get image info\n",
        "            try:\n",
        "                img = Image.open(image_path)\n",
        "                print(f\"    Size: {img.size}, Mode: {img.mode}\")\n",
        "                \n",
        "                # Check if it's black and white\n",
        "                if img.mode == 'RGB':\n",
        "                    # Convert to grayscale to check\n",
        "                    gray = img.convert('L')\n",
        "                    # Get unique values\n",
        "                    unique_values = set(gray.getdata())\n",
        "                    if len(unique_values) <= 2:  # Only black and white\n",
        "                        print(f\"    \u2705 Pure black/white image\")\n",
        "                    else:\n",
        "                        print(f\"    \u26a0\ufe0f  Not pure black/white ({len(unique_values)} unique values)\")\n",
        "                \n",
        "            except Exception as e:\n",
        "                print(f\"    \u274c Error reading image: {e}\")\n",
        "        \n",
        "        print()\n",
        "    \n",
        "    # Statistics\n",
        "    print(\"=== Dataset Statistics ===\")\n",
        "    total_meanings = sum(len(entry['meanings']) for entry in dataset)\n",
        "    avg_meanings = total_meanings / len(dataset)\n",
        "    print(f\"Total Kanji: {len(dataset)}\")\n",
        "    print(f\"Total meanings: {total_meanings}\")\n",
        "    print(f\"Average meanings per Kanji: {avg_meanings:.1f}\")\n",
        "    \n",
        "    # Check for common Kanji\n",
        "    common_kanji = ['\u4eba', '\u5927', '\u5c0f', '\u5c71', '\u5ddd', '\u65e5', '\u6708', '\u706b', '\u6c34', '\u6728']\n",
        "    found_common = []\n",
        "    for kanji in common_kanji:\n",
        "        for entry in dataset:\n",
        "            if entry['kanji'] == kanji:\n",
        "                found_common.append(kanji)\n",
        "                break\n",
        "    \n",
        "    print(f\"\\nCommon Kanji found: {len(found_common)}/{len(common_kanji)}\")\n",
        "    for kanji in found_common:\n",
        "        for entry in dataset:\n",
        "            if entry['kanji'] == kanji:\n",
        "                print(f\"  {kanji}: {', '.join(entry['meanings'][:3])}\")\n",
        "                break\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    verify_dataset() "
      ],
      "outputs": [],
      "execution_count": null
    }
  ],
  "metadata": {
    "anaconda-cloud": {},
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}