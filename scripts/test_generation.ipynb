{
  "cells": [
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#!/usr/bin/env python3\n",
        "\"\"\"\n",
        "Test Generation with Trained Model\n",
        "\"\"\"\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "import json\n",
        "from pathlib import Path\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "class SimpleUNet(nn.Module):\n",
        "    \"\"\"Simple UNet for quick testing\"\"\"\n",
        "    \n",
        "    def __init__(self, in_channels=3, out_channels=3, image_size=64):\n",
        "        super().__init__()\n",
        "        \n",
        "        # Simple encoder-decoder structure\n",
        "        self.encoder = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, 32, 3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(32, 64, 3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2),\n",
        "            \n",
        "            nn.Conv2d(64, 64, 3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(64, 128, 3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2),\n",
        "        )\n",
        "        \n",
        "        self.decoder = nn.Sequential(\n",
        "            nn.ConvTranspose2d(128, 64, 2, stride=2),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(64, 64, 3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            \n",
        "            nn.ConvTranspose2d(64, 32, 2, stride=2),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(32, out_channels, 3, padding=1),\n",
        "            nn.Tanh()\n",
        "        )\n",
        "    \n",
        "    def forward(self, x):\n",
        "        encoded = self.encoder(x)\n",
        "        decoded = self.decoder(encoded)\n",
        "        return decoded\n",
        "\n",
        "class QuickKanjiDataset(Dataset):\n",
        "    \"\"\"Quick test dataset for Kanji characters\"\"\"\n",
        "    \n",
        "    def __init__(self, dataset_path, transform=None, use_test_data=True):\n",
        "        self.dataset_path = Path(dataset_path)\n",
        "        self.transform = transform\n",
        "        \n",
        "        # Load dataset metadata\n",
        "        if use_test_data:\n",
        "            metadata_path = self.dataset_path / \"metadata\" / \"test_dataset.json\"\n",
        "        else:\n",
        "            metadata_path = self.dataset_path / \"metadata\" / \"dataset.json\"\n",
        "            \n",
        "        with open(metadata_path, 'r', encoding='utf-8') as f:\n",
        "            self.data = json.load(f)\n",
        "        \n",
        "        print(f\"Loaded {len(self.data)} Kanji entries for testing\")\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        entry = self.data[idx]\n",
        "        \n",
        "        # Load image\n",
        "        image_path = self.dataset_path / \"images\" / entry['image_file']\n",
        "        image = Image.open(image_path).convert('RGB')\n",
        "        \n",
        "        # Apply transforms\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "        \n",
        "        return {\n",
        "            'image': image,\n",
        "            'prompt': entry['prompt'],\n",
        "            'kanji': entry['kanji'],\n",
        "            'meanings': entry['meanings']\n",
        "        }\n",
        "\n",
        "def create_transforms(image_size=64):\n",
        "    \"\"\"Create transforms for testing\"\"\"\n",
        "    return transforms.Compose([\n",
        "        transforms.Resize((image_size, image_size)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
        "    ])\n",
        "\n",
        "def denormalize(tensor):\n",
        "    \"\"\"Denormalize tensor back to [0, 1] range\"\"\"\n",
        "    return (tensor + 1) / 2\n",
        "\n",
        "def test_generation():\n",
        "    \"\"\"Test generation with trained model\"\"\"\n",
        "    \n",
        "    print(\"\ud83e\uddea Testing Generation with Trained Model\")\n",
        "    print(\"=\" * 50)\n",
        "    \n",
        "    # Load model\n",
        "    model = SimpleUNet(in_channels=3, out_channels=3, image_size=64)\n",
        "    \n",
        "    # Load checkpoint\n",
        "    checkpoint_path = Path(\"quick_test_results/quick_test_epoch_2.pth\")\n",
        "    if not checkpoint_path.exists():\n",
        "        print(\"\u274c Checkpoint not found! Please run quick_train_test.py first.\")\n",
        "        return\n",
        "    \n",
        "    checkpoint = torch.load(checkpoint_path, map_location='cpu')\n",
        "    model.load_state_dict(checkpoint['model_state_dict'])\n",
        "    model.eval()\n",
        "    \n",
        "    print(f\"\u2705 Model loaded from epoch {checkpoint['epoch']}\")\n",
        "    print(f\"   \u2022 Loss: {checkpoint['loss']:.6f}\")\n",
        "    \n",
        "    # Load dataset\n",
        "    transform = create_transforms(64)\n",
        "    dataset = QuickKanjiDataset(\"data/fixed_kanji_dataset\", transform=transform, use_test_data=True)\n",
        "    \n",
        "    # Get a few sample images\n",
        "    sample_indices = [0, 10, 20, 30]  # Test different kanji\n",
        "    sample_images = []\n",
        "    sample_kanji = []\n",
        "    \n",
        "    for idx in sample_indices:\n",
        "        sample = dataset[idx]\n",
        "        sample_images.append(sample['image'])\n",
        "        sample_kanji.append(sample['kanji'])\n",
        "    \n",
        "    # Stack images\n",
        "    input_tensor = torch.stack(sample_images)\n",
        "    \n",
        "    print(f\"\ud83d\udcca Input shape: {input_tensor.shape}\")\n",
        "    print(f\"\ud83d\udd24 Testing kanji: {sample_kanji}\")\n",
        "    \n",
        "    # Generate\n",
        "    with torch.no_grad():\n",
        "        generated = model(input_tensor)\n",
        "    \n",
        "    print(f\"\ud83d\udcca Output shape: {generated.shape}\")\n",
        "    \n",
        "    # Convert to images\n",
        "    input_images = []\n",
        "    output_images = []\n",
        "    \n",
        "    for i in range(len(sample_images)):\n",
        "        # Input image\n",
        "        input_img = denormalize(input_tensor[i]).permute(1, 2, 0).numpy()\n",
        "        input_img = np.clip(input_img, 0, 1)\n",
        "        input_images.append(input_img)\n",
        "        \n",
        "        # Generated image\n",
        "        output_img = denormalize(generated[i]).permute(1, 2, 0).numpy()\n",
        "        output_img = np.clip(output_img, 0, 1)\n",
        "        output_images.append(output_img)\n",
        "    \n",
        "    # Display results\n",
        "    fig, axes = plt.subplots(2, 4, figsize=(16, 8))\n",
        "    fig.suptitle('Kanji Generation Test Results', fontsize=16)\n",
        "    \n",
        "    for i in range(4):\n",
        "        # Input images\n",
        "        axes[0, i].imshow(input_images[i])\n",
        "        axes[0, i].set_title(f'Input: {sample_kanji[i]}', fontsize=12)\n",
        "        axes[0, i].axis('off')\n",
        "        \n",
        "        # Generated images\n",
        "        axes[1, i].imshow(output_images[i])\n",
        "        axes[1, i].set_title(f'Generated: {sample_kanji[i]}', fontsize=12)\n",
        "        axes[1, i].axis('off')\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    \n",
        "    # Save results\n",
        "    output_path = \"generation_test_results.png\"\n",
        "    plt.savefig(output_path, dpi=150, bbox_inches='tight')\n",
        "    print(f\"\ud83d\udcbe Results saved to: {output_path}\")\n",
        "    \n",
        "    # Show the plot\n",
        "    plt.show()\n",
        "    \n",
        "    # Analyze results\n",
        "    print(f\"\\n\ud83d\udcca Generation Analysis:\")\n",
        "    print(f\"   \u2022 Model successfully processed {len(sample_images)} images\")\n",
        "    print(f\"   \u2022 Input and output shapes match\")\n",
        "    print(f\"   \u2022 Generation completed without errors\")\n",
        "    \n",
        "    # Calculate some metrics\n",
        "    mse_loss = nn.MSELoss()(generated, input_tensor)\n",
        "    print(f\"   \u2022 MSE Loss: {mse_loss.item():.6f}\")\n",
        "    \n",
        "    if mse_loss.item() < 0.1:\n",
        "        print(f\"   \u2705 Good reconstruction quality\")\n",
        "    elif mse_loss.item() < 0.3:\n",
        "        print(f\"   \u26a0\ufe0f  Moderate reconstruction quality\")\n",
        "    else:\n",
        "        print(f\"   \u274c Poor reconstruction quality\")\n",
        "    \n",
        "    print(f\"\\n\ud83c\udf89 Generation test complete!\")\n",
        "    print(f\"   \u2022 Model is working correctly\")\n",
        "    print(f\"   \u2022 Ready for more advanced training\")\n",
        "\n",
        "def main():\n",
        "    \"\"\"Main function\"\"\"\n",
        "    print(\"\ud83c\udf8c Kanji Generation Test\")\n",
        "    print(\"=\" * 50)\n",
        "    \n",
        "    test_generation()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "outputs": [],
      "execution_count": null
    }
  ],
  "metadata": {
    "anaconda-cloud": {},
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}