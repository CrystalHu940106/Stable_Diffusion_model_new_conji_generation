{
  "cells": [
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#!/usr/bin/env python3\n",
        "\"\"\"\n",
        "Quick Training Test for Kanji Diffusion Model\n",
        "Ultra-fast training to validate the pipeline works correctly\n",
        "\"\"\"\n",
        "\n",
        "import json\n",
        "import os\n",
        "import time\n",
        "from pathlib import Path\n",
        "from PIL import Image\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms\n",
        "import numpy as np\n",
        "\n",
        "class QuickKanjiDataset(Dataset):\n",
        "    \"\"\"Quick test dataset for Kanji characters\"\"\"\n",
        "    \n",
        "    def __init__(self, dataset_path, transform=None, use_test_data=True):\n",
        "        self.dataset_path = Path(dataset_path)\n",
        "        self.transform = transform\n",
        "        \n",
        "        # Load dataset metadata\n",
        "        if use_test_data:\n",
        "            metadata_path = self.dataset_path / \"metadata\" / \"test_dataset.json\"\n",
        "        else:\n",
        "            metadata_path = self.dataset_path / \"metadata\" / \"dataset.json\"\n",
        "            \n",
        "        with open(metadata_path, 'r', encoding='utf-8') as f:\n",
        "            self.data = json.load(f)\n",
        "        \n",
        "        print(f\"Loaded {len(self.data)} Kanji entries for quick test\")\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        entry = self.data[idx]\n",
        "        \n",
        "        # Load image\n",
        "        image_path = self.dataset_path / \"images\" / entry['image_file']\n",
        "        image = Image.open(image_path).convert('RGB')\n",
        "        \n",
        "        # Apply transforms\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "        \n",
        "        return {\n",
        "            'image': image,\n",
        "            'prompt': entry['prompt'],\n",
        "            'kanji': entry['kanji'],\n",
        "            'meanings': entry['meanings']\n",
        "        }\n",
        "\n",
        "def create_quick_transforms(image_size=64):\n",
        "    \"\"\"Create transforms for quick training test\"\"\"\n",
        "    return transforms.Compose([\n",
        "        transforms.Resize((image_size, image_size)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
        "    ])\n",
        "\n",
        "def custom_collate_fn(batch):\n",
        "    \"\"\"Custom collate function\"\"\"\n",
        "    images = torch.stack([item['image'] for item in batch])\n",
        "    prompts = [item['prompt'] for item in batch]\n",
        "    kanji_chars = [item['kanji'] for item in batch]\n",
        "    \n",
        "    return {\n",
        "        'image': images,\n",
        "        'prompt': prompts,\n",
        "        'kanji': kanji_chars,\n",
        "    }\n",
        "\n",
        "class SimpleUNet(nn.Module):\n",
        "    \"\"\"Simple UNet for quick testing\"\"\"\n",
        "    \n",
        "    def __init__(self, in_channels=3, out_channels=3, image_size=64):\n",
        "        super().__init__()\n",
        "        \n",
        "        # Simple encoder-decoder structure\n",
        "        self.encoder = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, 32, 3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(32, 64, 3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2),\n",
        "            \n",
        "            nn.Conv2d(64, 64, 3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(64, 128, 3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2),\n",
        "        )\n",
        "        \n",
        "        self.decoder = nn.Sequential(\n",
        "            nn.ConvTranspose2d(128, 64, 2, stride=2),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(64, 64, 3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            \n",
        "            nn.ConvTranspose2d(64, 32, 2, stride=2),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(32, out_channels, 3, padding=1),\n",
        "            nn.Tanh(),\n",
        "        )\n",
        "        \n",
        "    def forward(self, x):\n",
        "        encoded = self.encoder(x)\n",
        "        decoded = self.decoder(encoded)\n",
        "        return decoded\n",
        "\n",
        "def quick_training_test():\n",
        "    \"\"\"Run quick training test\"\"\"\n",
        "    \n",
        "    print(\"\ud83d\ude80 Starting Quick Training Test\")\n",
        "    print(\"=\" * 40)\n",
        "    \n",
        "    # Configuration for quick test\n",
        "    config = {\n",
        "        'image_size': 64,\n",
        "        'batch_size': 8,\n",
        "        'learning_rate': 5e-4,\n",
        "        'num_epochs': 2,\n",
        "        'device': 'cpu',\n",
        "        'save_dir': 'quick_test_results'\n",
        "    }\n",
        "    \n",
        "    print(f\"\ud83d\udcca Configuration:\")\n",
        "    print(f\"   \u2022 Image size: {config['image_size']}x{config['image_size']}\")\n",
        "    print(f\"   \u2022 Batch size: {config['batch_size']}\")\n",
        "    print(f\"   \u2022 Learning rate: {config['learning_rate']}\")\n",
        "    print(f\"   \u2022 Epochs: {config['num_epochs']}\")\n",
        "    print(f\"   \u2022 Device: {config['device']}\")\n",
        "    \n",
        "    # Create save directory\n",
        "    save_dir = Path(config['save_dir'])\n",
        "    save_dir.mkdir(exist_ok=True)\n",
        "    \n",
        "    # Create dataset and dataloader\n",
        "    transform = create_quick_transforms(config['image_size'])\n",
        "    dataset = QuickKanjiDataset(\"data/fixed_kanji_dataset\", transform=transform, use_test_data=True)\n",
        "    \n",
        "    dataloader = DataLoader(\n",
        "        dataset,\n",
        "        batch_size=config['batch_size'],\n",
        "        shuffle=True,\n",
        "        num_workers=0,\n",
        "        collate_fn=custom_collate_fn\n",
        "    )\n",
        "    \n",
        "    print(f\"\\n\ud83d\udcda Dataset Info:\")\n",
        "    print(f\"   \u2022 Total samples: {len(dataset)}\")\n",
        "    print(f\"   \u2022 Batches per epoch: {len(dataloader)}\")\n",
        "    \n",
        "    # Create model\n",
        "    model = SimpleUNet(\n",
        "        in_channels=3,\n",
        "        out_channels=3,\n",
        "        image_size=config['image_size']\n",
        "    )\n",
        "    model.to(config['device'])\n",
        "    \n",
        "    # Loss and optimizer\n",
        "    criterion = nn.MSELoss()\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=config['learning_rate'])\n",
        "    \n",
        "    print(f\"\\n\ud83c\udfd7\ufe0f Model Info:\")\n",
        "    print(f\"   \u2022 Parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
        "    print(f\"   \u2022 Trainable: {sum(p.numel() for p in model.parameters() if p.requires_grad):,}\")\n",
        "    \n",
        "    # Training loop\n",
        "    print(f\"\\n\ud83c\udfaf Starting Training...\")\n",
        "    start_time = time.time()\n",
        "    \n",
        "    for epoch in range(config['num_epochs']):\n",
        "        epoch_start = time.time()\n",
        "        model.train()\n",
        "        total_loss = 0\n",
        "        \n",
        "        for batch_idx, batch in enumerate(dataloader):\n",
        "            images = batch['image'].to(config['device'])\n",
        "            \n",
        "            # Forward pass\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, images)\n",
        "            \n",
        "            # Backward pass\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            \n",
        "            total_loss += loss.item()\n",
        "            \n",
        "            # Progress update\n",
        "            if batch_idx % 10 == 0:\n",
        "                print(f\"   Epoch {epoch+1}/{config['num_epochs']}, \"\n",
        "                      f\"Batch {batch_idx+1}/{len(dataloader)}, \"\n",
        "                      f\"Loss: {loss.item():.6f}\")\n",
        "        \n",
        "        epoch_time = time.time() - epoch_start\n",
        "        avg_loss = total_loss / len(dataloader)\n",
        "        \n",
        "        print(f\"\\n\u2705 Epoch {epoch+1} completed:\")\n",
        "        print(f\"   \u2022 Average Loss: {avg_loss:.6f}\")\n",
        "        print(f\"   \u2022 Time: {epoch_time:.1f} seconds\")\n",
        "        \n",
        "        # Save checkpoint\n",
        "        checkpoint_path = save_dir / f\"quick_test_epoch_{epoch+1}.pth\"\n",
        "        torch.save({\n",
        "            'epoch': epoch + 1,\n",
        "            'model_state_dict': model.state_dict(),\n",
        "            'optimizer_state_dict': optimizer.state_dict(),\n",
        "            'loss': avg_loss,\n",
        "        }, checkpoint_path)\n",
        "        print(f\"   \u2022 Checkpoint saved: {checkpoint_path}\")\n",
        "    \n",
        "    total_time = time.time() - start_time\n",
        "    print(f\"\\n\ud83c\udf89 Quick Training Test Completed!\")\n",
        "    print(f\"   \u2022 Total time: {total_time:.1f} seconds ({total_time/60:.1f} minutes)\")\n",
        "    print(f\"   \u2022 Final loss: {avg_loss:.6f}\")\n",
        "    print(f\"   \u2022 Results saved in: {save_dir}\")\n",
        "    \n",
        "    # Test generation\n",
        "    print(f\"\\n\ud83e\uddea Testing Generation...\")\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        # Get a sample batch\n",
        "        sample_batch = next(iter(dataloader))\n",
        "        sample_images = sample_batch['image'][:4].to(config['device'])\n",
        "        \n",
        "        # Generate\n",
        "        generated = model(sample_images)\n",
        "        \n",
        "        print(f\"   \u2022 Input shape: {sample_images.shape}\")\n",
        "        print(f\"   \u2022 Output shape: {generated.shape}\")\n",
        "        print(f\"   \u2022 Generation successful!\")\n",
        "    \n",
        "    print(f\"\\n\u2705 Quick test validation complete!\")\n",
        "    print(f\"   \u2022 Training pipeline works correctly\")\n",
        "    print(f\"   \u2022 Model can process images\")\n",
        "    print(f\"   \u2022 Checkpoints saved successfully\")\n",
        "    print(f\"   \u2022 Ready for full training!\")\n",
        "\n",
        "def main():\n",
        "    \"\"\"Main function\"\"\"\n",
        "    \n",
        "    print(\"\ud83c\udf8c Quick Training Test for Kanji Diffusion\")\n",
        "    print(\"=\" * 50)\n",
        "    \n",
        "    # Check if test dataset exists\n",
        "    test_dataset_path = Path(\"data/fixed_kanji_dataset/metadata/test_dataset.json\")\n",
        "    if not test_dataset_path.exists():\n",
        "        print(\"\u274c Test dataset not found! Please run quick_test_config.py first.\")\n",
        "        return\n",
        "    \n",
        "    # Check if main dataset exists\n",
        "    main_dataset_path = Path(\"data/fixed_kanji_dataset/metadata/dataset.json\")\n",
        "    if not main_dataset_path.exists():\n",
        "        print(\"\u274c Main dataset not found! Please run fix_kanji_dataset.py first.\")\n",
        "        return\n",
        "    \n",
        "    # Run quick training test\n",
        "    quick_training_test()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "outputs": [],
      "execution_count": null
    }
  ],
  "metadata": {
    "anaconda-cloud": {},
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}