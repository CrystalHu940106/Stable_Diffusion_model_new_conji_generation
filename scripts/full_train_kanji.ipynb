{
  "cells": [
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#!/usr/bin/env python3\n",
        "\"\"\"\n",
        "Full Kanji Training Script\n",
        "Train the complete model on the full dataset\n",
        "\"\"\"\n",
        "\n",
        "import json\n",
        "import os\n",
        "import time\n",
        "from pathlib import Path\n",
        "from PIL import Image\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from datetime import datetime\n",
        "\n",
        "class KanjiDataset(Dataset):\n",
        "    \"\"\"Full Kanji dataset\"\"\"\n",
        "    \n",
        "    def __init__(self, dataset_path, transform=None, split='train', train_split=0.9):\n",
        "        self.dataset_path = Path(dataset_path)\n",
        "        self.transform = transform\n",
        "        \n",
        "        # Load dataset metadata\n",
        "        metadata_path = self.dataset_path / \"metadata\" / \"dataset.json\"\n",
        "        with open(metadata_path, 'r', encoding='utf-8') as f:\n",
        "            self.data = json.load(f)\n",
        "        \n",
        "        # Split data\n",
        "        total_size = len(self.data)\n",
        "        train_size = int(total_size * train_split)\n",
        "        \n",
        "        if split == 'train':\n",
        "            self.data = self.data[:train_size]\n",
        "        else:  # validation\n",
        "            self.data = self.data[train_size:]\n",
        "        \n",
        "        print(f\"Loaded {len(self.data)} Kanji entries for {split}\")\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        entry = self.data[idx]\n",
        "        \n",
        "        # Load image\n",
        "        image_path = self.dataset_path / \"images\" / entry['image_file']\n",
        "        image = Image.open(image_path).convert('RGB')\n",
        "        \n",
        "        # Apply transforms\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "        \n",
        "        return {\n",
        "            'image': image,\n",
        "            'prompt': entry['prompt'],\n",
        "            'kanji': entry['kanji'],\n",
        "            'meanings': entry['meanings']\n",
        "        }\n",
        "\n",
        "class AdvancedUNet(nn.Module):\n",
        "    \"\"\"Advanced UNet for better Kanji generation\"\"\"\n",
        "    \n",
        "    def __init__(self, in_channels=3, out_channels=3, image_size=128):\n",
        "        super().__init__()\n",
        "        \n",
        "        # Encoder\n",
        "        self.encoder = nn.Sequential(\n",
        "            # Initial convolution\n",
        "            nn.Conv2d(in_channels, 64, 3, padding=1),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(64, 64, 3, padding=1),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2),\n",
        "            \n",
        "            # Second level\n",
        "            nn.Conv2d(64, 128, 3, padding=1),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(128, 128, 3, padding=1),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2),\n",
        "            \n",
        "            # Third level\n",
        "            nn.Conv2d(128, 256, 3, padding=1),\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(256, 256, 3, padding=1),\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2),\n",
        "            \n",
        "            # Fourth level\n",
        "            nn.Conv2d(256, 512, 3, padding=1),\n",
        "            nn.BatchNorm2d(512),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(512, 512, 3, padding=1),\n",
        "            nn.BatchNorm2d(512),\n",
        "            nn.ReLU(),\n",
        "        )\n",
        "        \n",
        "        # Decoder\n",
        "        self.decoder = nn.Sequential(\n",
        "            # Fourth level\n",
        "            nn.ConvTranspose2d(512, 256, 2, stride=2),\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(256, 256, 3, padding=1),\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.ReLU(),\n",
        "            \n",
        "            # Third level\n",
        "            nn.ConvTranspose2d(256, 128, 2, stride=2),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(128, 128, 3, padding=1),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.ReLU(),\n",
        "            \n",
        "            # Second level\n",
        "            nn.ConvTranspose2d(128, 64, 2, stride=2),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(64, 64, 3, padding=1),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU(),\n",
        "            \n",
        "            # Final output\n",
        "            nn.Conv2d(64, out_channels, 3, padding=1),\n",
        "            nn.Tanh()\n",
        "        )\n",
        "    \n",
        "    def forward(self, x):\n",
        "        encoded = self.encoder(x)\n",
        "        decoded = self.decoder(encoded)\n",
        "        return decoded\n",
        "\n",
        "def create_transforms(image_size=128):\n",
        "    \"\"\"Create transforms for training\"\"\"\n",
        "    return transforms.Compose([\n",
        "        transforms.Resize((image_size, image_size)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
        "    ])\n",
        "\n",
        "def custom_collate_fn(batch):\n",
        "    \"\"\"Custom collate function\"\"\"\n",
        "    images = torch.stack([item['image'] for item in batch])\n",
        "    prompts = [item['prompt'] for item in batch]\n",
        "    kanji_chars = [item['kanji'] for item in batch]\n",
        "    \n",
        "    return {\n",
        "        'image': images,\n",
        "        'prompt': prompts,\n",
        "        'kanji': kanji_chars,\n",
        "    }\n",
        "\n",
        "def save_checkpoint(model, optimizer, epoch, loss, save_dir, filename):\n",
        "    \"\"\"Save model checkpoint\"\"\"\n",
        "    checkpoint = {\n",
        "        'epoch': epoch,\n",
        "        'model_state_dict': model.state_dict(),\n",
        "        'optimizer_state_dict': optimizer.state_dict(),\n",
        "        'loss': loss,\n",
        "        'timestamp': datetime.now().isoformat()\n",
        "    }\n",
        "    \n",
        "    checkpoint_path = save_dir / filename\n",
        "    torch.save(checkpoint, checkpoint_path)\n",
        "    print(f\"\ud83d\udcbe Checkpoint saved: {checkpoint_path}\")\n",
        "\n",
        "def load_checkpoint(model, optimizer, checkpoint_path):\n",
        "    \"\"\"Load model checkpoint\"\"\"\n",
        "    checkpoint = torch.load(checkpoint_path, map_location='cpu')\n",
        "    model.load_state_dict(checkpoint['model_state_dict'])\n",
        "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "    epoch = checkpoint['epoch']\n",
        "    loss = checkpoint['loss']\n",
        "    \n",
        "    print(f\"\ud83d\udcc2 Checkpoint loaded: {checkpoint_path}\")\n",
        "    print(f\"   \u2022 Epoch: {epoch}\")\n",
        "    print(f\"   \u2022 Loss: {loss:.6f}\")\n",
        "    \n",
        "    return epoch, loss\n",
        "\n",
        "def plot_training_progress(train_losses, val_losses, save_dir):\n",
        "    \"\"\"Plot training progress\"\"\"\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    \n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(train_losses, label='Training Loss', color='blue')\n",
        "    plt.plot(val_losses, label='Validation Loss', color='red')\n",
        "    plt.title('Training Progress')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    \n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(train_losses, label='Training Loss', color='blue')\n",
        "    plt.title('Training Loss Detail')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    \n",
        "    # Save plot\n",
        "    plot_path = save_dir / \"training_progress.png\"\n",
        "    plt.savefig(plot_path, dpi=150, bbox_inches='tight')\n",
        "    print(f\"\ud83d\udcca Training progress saved: {plot_path}\")\n",
        "    \n",
        "    plt.show()\n",
        "\n",
        "def full_training():\n",
        "    \"\"\"Run full training on the complete dataset\"\"\"\n",
        "    \n",
        "    print(\"\ud83c\udf8c Full Kanji Training\")\n",
        "    print(\"=\" * 50)\n",
        "    \n",
        "    # Configuration\n",
        "    config = {\n",
        "        'image_size': 128,\n",
        "        'batch_size': 4,\n",
        "        'learning_rate': 2e-4,\n",
        "        'num_epochs': 5,\n",
        "        'device': 'cpu',\n",
        "        'save_dir': 'full_training_results',\n",
        "        'train_split': 0.9,\n",
        "        'save_every': 1,\n",
        "        'log_every': 100\n",
        "    }\n",
        "    \n",
        "    print(f\"\ud83d\udcca Configuration:\")\n",
        "    print(f\"   \u2022 Image size: {config['image_size']}x{config['image_size']}\")\n",
        "    print(f\"   \u2022 Batch size: {config['batch_size']}\")\n",
        "    print(f\"   \u2022 Learning rate: {config['learning_rate']}\")\n",
        "    print(f\"   \u2022 Epochs: {config['num_epochs']}\")\n",
        "    print(f\"   \u2022 Device: {config['device']}\")\n",
        "    \n",
        "    # Create save directory\n",
        "    save_dir = Path(config['save_dir'])\n",
        "    save_dir.mkdir(exist_ok=True)\n",
        "    \n",
        "    # Create datasets and dataloaders\n",
        "    transform = create_transforms(config['image_size'])\n",
        "    \n",
        "    train_dataset = KanjiDataset(\"data/fixed_kanji_dataset\", transform=transform, split='train', train_split=config['train_split'])\n",
        "    val_dataset = KanjiDataset(\"data/fixed_kanji_dataset\", transform=transform, split='val', train_split=config['train_split'])\n",
        "    \n",
        "    train_loader = DataLoader(\n",
        "        train_dataset,\n",
        "        batch_size=config['batch_size'],\n",
        "        shuffle=True,\n",
        "        num_workers=0,\n",
        "        collate_fn=custom_collate_fn\n",
        "    )\n",
        "    \n",
        "    val_loader = DataLoader(\n",
        "        val_dataset,\n",
        "        batch_size=config['batch_size'],\n",
        "        shuffle=False,\n",
        "        num_workers=0,\n",
        "        collate_fn=custom_collate_fn\n",
        "    )\n",
        "    \n",
        "    print(f\"\\n\ud83d\udcda Dataset Info:\")\n",
        "    print(f\"   \u2022 Training samples: {len(train_dataset)}\")\n",
        "    print(f\"   \u2022 Validation samples: {len(val_dataset)}\")\n",
        "    print(f\"   \u2022 Training batches: {len(train_loader)}\")\n",
        "    print(f\"   \u2022 Validation batches: {len(val_loader)}\")\n",
        "    \n",
        "    # Create model\n",
        "    model = AdvancedUNet(\n",
        "        in_channels=3,\n",
        "        out_channels=3,\n",
        "        image_size=config['image_size']\n",
        "    )\n",
        "    model.to(config['device'])\n",
        "    \n",
        "    # Loss and optimizer\n",
        "    criterion = nn.MSELoss()\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=config['learning_rate'], weight_decay=0.01)\n",
        "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=2, gamma=0.5)\n",
        "    \n",
        "    print(f\"\\n\ud83c\udfd7\ufe0f Model Info:\")\n",
        "    print(f\"   \u2022 Parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
        "    print(f\"   \u2022 Trainable: {sum(p.numel() for p in model.parameters() if p.requires_grad):,}\")\n",
        "    \n",
        "    # Training loop\n",
        "    print(f\"\\n\ud83c\udfaf Starting Training...\")\n",
        "    start_time = time.time()\n",
        "    \n",
        "    train_losses = []\n",
        "    val_losses = []\n",
        "    best_val_loss = float('inf')\n",
        "    \n",
        "    for epoch in range(config['num_epochs']):\n",
        "        epoch_start = time.time()\n",
        "        \n",
        "        # Training phase\n",
        "        model.train()\n",
        "        train_loss = 0\n",
        "        train_steps = 0\n",
        "        \n",
        "        for batch_idx, batch in enumerate(train_loader):\n",
        "            images = batch['image'].to(config['device'])\n",
        "            \n",
        "            # Forward pass\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, images)\n",
        "            \n",
        "            # Backward pass\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "            optimizer.step()\n",
        "            \n",
        "            train_loss += loss.item()\n",
        "            train_steps += 1\n",
        "            \n",
        "            # Progress update\n",
        "            if batch_idx % config['log_every'] == 0:\n",
        "                print(f\"   Epoch {epoch+1}/{config['num_epochs']}, \"\n",
        "                      f\"Batch {batch_idx+1}/{len(train_loader)}, \"\n",
        "                      f\"Loss: {loss.item():.6f}\")\n",
        "        \n",
        "        avg_train_loss = train_loss / train_steps\n",
        "        train_losses.append(avg_train_loss)\n",
        "        \n",
        "        # Validation phase\n",
        "        model.eval()\n",
        "        val_loss = 0\n",
        "        val_steps = 0\n",
        "        \n",
        "        with torch.no_grad():\n",
        "            for batch in val_loader:\n",
        "                images = batch['image'].to(config['device'])\n",
        "                outputs = model(images)\n",
        "                loss = criterion(outputs, images)\n",
        "                val_loss += loss.item()\n",
        "                val_steps += 1\n",
        "        \n",
        "        avg_val_loss = val_loss / val_steps\n",
        "        val_losses.append(avg_val_loss)\n",
        "        \n",
        "        # Update learning rate\n",
        "        scheduler.step()\n",
        "        \n",
        "        epoch_time = time.time() - epoch_start\n",
        "        \n",
        "        print(f\"\\n\u2705 Epoch {epoch+1} completed:\")\n",
        "        print(f\"   \u2022 Training Loss: {avg_train_loss:.6f}\")\n",
        "        print(f\"   \u2022 Validation Loss: {avg_val_loss:.6f}\")\n",
        "        print(f\"   \u2022 Time: {epoch_time:.1f} seconds\")\n",
        "        print(f\"   \u2022 Learning Rate: {scheduler.get_last_lr()[0]:.2e}\")\n",
        "        \n",
        "        # Save checkpoint\n",
        "        if (epoch + 1) % config['save_every'] == 0:\n",
        "            checkpoint_name = f\"full_training_epoch_{epoch+1}.pth\"\n",
        "            save_checkpoint(model, optimizer, epoch+1, avg_val_loss, save_dir, checkpoint_name)\n",
        "        \n",
        "        # Save best model\n",
        "        if avg_val_loss < best_val_loss:\n",
        "            best_val_loss = avg_val_loss\n",
        "            save_checkpoint(model, optimizer, epoch+1, avg_val_loss, save_dir, \"best_model.pth\")\n",
        "            print(f\"   \ud83c\udfc6 New best model saved!\")\n",
        "    \n",
        "    total_time = time.time() - start_time\n",
        "    \n",
        "    print(f\"\\n\ud83c\udf89 Full Training Completed!\")\n",
        "    print(f\"   \u2022 Total time: {total_time:.1f} seconds ({total_time/3600:.1f} hours)\")\n",
        "    print(f\"   \u2022 Final training loss: {avg_train_loss:.6f}\")\n",
        "    print(f\"   \u2022 Final validation loss: {avg_val_loss:.6f}\")\n",
        "    print(f\"   \u2022 Best validation loss: {best_val_loss:.6f}\")\n",
        "    print(f\"   \u2022 Results saved in: {save_dir}\")\n",
        "    \n",
        "    # Plot training progress\n",
        "    plot_training_progress(train_losses, val_losses, save_dir)\n",
        "    \n",
        "    # Save training summary\n",
        "    summary = {\n",
        "        'config': config,\n",
        "        'train_losses': train_losses,\n",
        "        'val_losses': val_losses,\n",
        "        'best_val_loss': best_val_loss,\n",
        "        'total_time': total_time,\n",
        "        'final_train_loss': avg_train_loss,\n",
        "        'final_val_loss': avg_val_loss\n",
        "    }\n",
        "    \n",
        "    summary_path = save_dir / \"training_summary.json\"\n",
        "    with open(summary_path, 'w') as f:\n",
        "        json.dump(summary, f, indent=2)\n",
        "    \n",
        "    print(f\"\ud83d\udccb Training summary saved: {summary_path}\")\n",
        "    \n",
        "    return model, save_dir\n",
        "\n",
        "def main():\n",
        "    \"\"\"Main function\"\"\"\n",
        "    \n",
        "    print(\"\ud83c\udf8c Full Kanji Training Script\")\n",
        "    print(\"=\" * 50)\n",
        "    \n",
        "    # Check if dataset exists\n",
        "    dataset_path = Path(\"data/fixed_kanji_dataset\")\n",
        "    if not dataset_path.exists():\n",
        "        print(\"\u274c Dataset not found! Please run fix_kanji_dataset.py first.\")\n",
        "        return\n",
        "    \n",
        "    # Run full training\n",
        "    model, save_dir = full_training()\n",
        "    \n",
        "    print(f\"\\n\ud83c\udfaf Training Summary:\")\n",
        "    print(f\"   \u2022 Model trained on full dataset\")\n",
        "    print(f\"   \u2022 Checkpoints saved in: {save_dir}\")\n",
        "    print(f\"   \u2022 Best model: {save_dir}/best_model.pth\")\n",
        "    print(f\"   \u2022 Ready for generation!\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "outputs": [],
      "execution_count": null
    }
  ],
  "metadata": {
    "anaconda-cloud": {},
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}