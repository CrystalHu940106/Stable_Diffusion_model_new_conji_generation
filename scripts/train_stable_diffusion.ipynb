{
  "cells": [
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#!/usr/bin/env python3\n",
        "\"\"\"\n",
        "Complete Stable Diffusion Training Script\n",
        "\"\"\"\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import json\n",
        "from pathlib import Path\n",
        "import os\n",
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "from stable_diffusion_kanji import VAE, UNet2DConditionModel, DDPMScheduler, StableDiffusionPipeline\n",
        "\n",
        "def get_optimal_batch_size(device):\n",
        "    \"\"\"\u667a\u80fd\u9009\u62e9\u6700\u4f18\u6279\u5904\u7406\u5927\u5c0f\"\"\"\n",
        "    if torch.cuda.is_available():\n",
        "        gpu_memory = torch.cuda.get_device_properties(0).total_memory / (1024**3)  # GB\n",
        "        if gpu_memory > 8:\n",
        "            return 16\n",
        "        elif gpu_memory > 4:\n",
        "            return 8\n",
        "        else:\n",
        "            return 4\n",
        "    elif hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():\n",
        "        return 4\n",
        "    else:\n",
        "        return 2\n",
        "\n",
        "class EMAModel:\n",
        "    \"\"\"\u6307\u6570\u79fb\u52a8\u5e73\u5747\u6a21\u578b\"\"\"\n",
        "    def __init__(self, model, decay=0.9999):\n",
        "        self.model = model\n",
        "        self.decay = decay\n",
        "        self.shadow = {}\n",
        "        self.backup = {}\n",
        "        self.register()\n",
        "\n",
        "    def register(self):\n",
        "        for name, param in self.model.named_parameters():\n",
        "            if param.requires_grad:\n",
        "                self.shadow[name] = param.data.clone()\n",
        "\n",
        "    def update(self):\n",
        "        for name, param in self.model.named_parameters():\n",
        "            if param.requires_grad:\n",
        "                assert name in self.shadow\n",
        "                new_average = (1.0 - self.decay) * param.data + self.decay * self.shadow[name]\n",
        "                self.shadow[name] = new_average.clone()\n",
        "\n",
        "    def apply_shadow(self):\n",
        "        for name, param in self.model.named_parameters():\n",
        "            if param.requires_grad:\n",
        "                assert name in self.shadow\n",
        "                self.backup[name] = param.data\n",
        "                param.data = self.shadow[name]\n",
        "\n",
        "    def restore(self):\n",
        "        for name, param in self.model.named_parameters():\n",
        "            if param.requires_grad:\n",
        "                assert name in self.backup\n",
        "                param.data = self.backup[name]\n",
        "        self.backup = {}\n",
        "\n",
        "class KanjiDataset(Dataset):\n",
        "    def __init__(self, dataset_path, transform=None, max_samples=None):\n",
        "        self.dataset_path = Path(dataset_path)\n",
        "        self.transform = transform\n",
        "        \n",
        "        # Load metadata\n",
        "        metadata_path = self.dataset_path / \"metadata\" / \"dataset.json\"\n",
        "        with open(metadata_path, 'r', encoding='utf-8') as f:\n",
        "            self.data = json.load(f)\n",
        "        \n",
        "        # Limit samples if specified\n",
        "        if max_samples:\n",
        "            self.data = self.data[:max_samples]\n",
        "        \n",
        "        print(f\"\ud83d\udcda Loaded {len(self.data)} Kanji entries\")\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        entry = self.data[idx]\n",
        "        \n",
        "        # Load image\n",
        "        image_path = self.dataset_path / \"images\" / entry['image_file']\n",
        "        image = Image.open(image_path).convert('RGB')\n",
        "        \n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "        \n",
        "        # Create prompt from meanings\n",
        "        prompt = f\"kanji character meaning: {', '.join(entry['meanings'][:3])}\"\n",
        "        \n",
        "        return {\n",
        "            'image': image,\n",
        "            'prompt': prompt,\n",
        "            'kanji': entry['kanji']\n",
        "        }\n",
        "\n",
        "class StableDiffusionTrainer:\n",
        "    def __init__(self, device='cuda' if torch.cuda.is_available() else 'cpu'):\n",
        "        self.device = device\n",
        "        \n",
        "        # Initialize models with larger capacity\n",
        "        self.vae = VAE(hidden_dims=[128, 256, 512, 1024]).to(device)\n",
        "        self.unet = UNet2DConditionModel(\n",
        "            model_channels=256,  # Increased from 128\n",
        "            num_res_blocks=3,    # Increased from 2\n",
        "            channel_mult=(1, 2, 4, 8),  # Reduced to match VAE latent space\n",
        "            attention_resolutions=(8,),  # Only at 8x8 resolution\n",
        "            num_heads=16         # Increased from 8\n",
        "        ).to(device)\n",
        "        \n",
        "        # Initialize scheduler\n",
        "        self.scheduler = DDPMScheduler(num_train_timesteps=1000)\n",
        "        \n",
        "        # CLIP text encoder\n",
        "        from transformers import CLIPTokenizer, CLIPTextModel\n",
        "        self.tokenizer = CLIPTokenizer.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
        "        self.text_encoder = CLIPTextModel.from_pretrained(\"openai/clip-vit-base-patch32\").to(device)\n",
        "        self.text_encoder.eval()\n",
        "        \n",
        "        # Optimizers with different learning rates\n",
        "        self.vae_optimizer = torch.optim.AdamW(self.vae.parameters(), lr=1e-4, weight_decay=1e-6)\n",
        "        self.unet_optimizer = torch.optim.AdamW(self.unet.parameters(), lr=1e-5, weight_decay=1e-6)\n",
        "        \n",
        "        # Learning rate schedulers\n",
        "        self.vae_scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(self.vae_optimizer, T_max=100)\n",
        "        self.unet_scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(self.unet_optimizer, T_max=100)\n",
        "        \n",
        "        # Loss functions\n",
        "        self.mse_loss = nn.MSELoss()\n",
        "        \n",
        "        # EMA models for better quality\n",
        "        self.vae_ema = EMAModel(self.vae)\n",
        "        self.unet_ema = EMAModel(self.unet)\n",
        "        \n",
        "        # Mixed precision training (GPU only)\n",
        "        self.use_amp = torch.cuda.is_available()\n",
        "        if self.use_amp:\n",
        "            self.scaler = torch.cuda.amp.GradScaler()\n",
        "        \n",
        "        # Gradient accumulation\n",
        "        self.accumulation_steps = 4\n",
        "        \n",
        "        print(f\"\ud83d\ude80 Trainer initialized on {device}\")\n",
        "        print(f\"   VAE parameters: {sum(p.numel() for p in self.vae.parameters()):,}\")\n",
        "        print(f\"   UNet parameters: {sum(p.numel() for p in self.unet.parameters()):,}\")\n",
        "        print(f\"   Mixed Precision: {'\u2705' if self.use_amp else '\u274c'}\")\n",
        "        print(f\"   Gradient Accumulation: {self.accumulation_steps} steps\")\n",
        "    \n",
        "    def encode_text(self, prompts):\n",
        "        \"\"\"Encode text prompts to embeddings\"\"\"\n",
        "        if isinstance(prompts, str):\n",
        "            prompts = [prompts]\n",
        "        \n",
        "        tokens = self.tokenizer(prompts, padding=True, truncation=True, max_length=77, return_tensors=\"pt\")\n",
        "        tokens = {k: v.to(self.device) for k, v in tokens.items()}\n",
        "        \n",
        "        with torch.no_grad():\n",
        "            text_embeddings = self.text_encoder(**tokens).last_hidden_state\n",
        "        \n",
        "        return text_embeddings\n",
        "    \n",
        "    def train_step(self, images, prompts, timesteps, step_idx):\n",
        "        \"\"\"Single training step with mixed precision and gradient accumulation\"\"\"\n",
        "        batch_size = images.shape[0]\n",
        "        \n",
        "        # Encode images to latent space with KL loss\n",
        "        latents, mu, logvar, kl_loss = self.vae.encode(images)\n",
        "        \n",
        "        # Encode text prompts\n",
        "        text_embeddings = self.encode_text(prompts)\n",
        "        \n",
        "        # Add noise to latents\n",
        "        noise = torch.randn_like(latents)\n",
        "        noisy_latents = self.scheduler.add_noise(latents, noise, timesteps)\n",
        "        \n",
        "        # Predict noise using UNet\n",
        "        noise_pred = self.unet(noisy_latents, timesteps, text_embeddings)\n",
        "        \n",
        "        # Calculate losses\n",
        "        noise_loss = self.mse_loss(noise_pred, noise)\n",
        "        \n",
        "        # VAE reconstruction loss\n",
        "        reconstructed = self.vae.decode(latents)\n",
        "        recon_loss = self.mse_loss(reconstructed, images)\n",
        "        \n",
        "        # Total loss with KL divergence\n",
        "        total_loss = noise_loss + 0.1 * recon_loss + 0.01 * kl_loss\n",
        "        \n",
        "        # Scale loss for gradient accumulation\n",
        "        total_loss = total_loss / self.accumulation_steps\n",
        "        \n",
        "        # Backward pass with mixed precision\n",
        "        if self.use_amp:\n",
        "            self.scaler.scale(total_loss).backward()\n",
        "        else:\n",
        "            total_loss.backward()\n",
        "        \n",
        "        # Gradient accumulation\n",
        "        if (step_idx + 1) % self.accumulation_steps == 0:\n",
        "            if self.use_amp:\n",
        "                # Gradient clipping\n",
        "                self.scaler.unscale_(self.vae_optimizer)\n",
        "                self.scaler.unscale_(self.unet_optimizer)\n",
        "                torch.nn.utils.clip_grad_norm_(self.vae.parameters(), max_norm=1.0)\n",
        "                torch.nn.utils.clip_grad_norm_(self.unet.parameters(), max_norm=1.0)\n",
        "                \n",
        "                # Optimizer step\n",
        "                self.scaler.step(self.vae_optimizer)\n",
        "                self.scaler.step(self.unet_optimizer)\n",
        "                self.scaler.update()\n",
        "            else:\n",
        "                # Gradient clipping\n",
        "                torch.nn.utils.clip_grad_norm_(self.vae.parameters(), max_norm=1.0)\n",
        "                torch.nn.utils.clip_grad_norm_(self.unet.parameters(), max_norm=1.0)\n",
        "                \n",
        "                # Optimizer step\n",
        "                self.vae_optimizer.step()\n",
        "                self.unet_optimizer.step()\n",
        "            \n",
        "            # Zero gradients\n",
        "            self.vae_optimizer.zero_grad()\n",
        "            self.unet_optimizer.zero_grad()\n",
        "            \n",
        "            # Update EMA models\n",
        "            self.vae_ema.update()\n",
        "            self.unet_ema.update()\n",
        "        \n",
        "        return {\n",
        "            'total_loss': total_loss.item() * self.accumulation_steps,\n",
        "            'noise_loss': noise_loss.item(),\n",
        "            'recon_loss': recon_loss.item(),\n",
        "            'kl_loss': kl_loss.item()\n",
        "        }\n",
        "    \n",
        "    def train_epoch(self, dataloader, epoch):\n",
        "        \"\"\"Train for one epoch with optimized settings\"\"\"\n",
        "        self.vae.train()\n",
        "        self.unet.train()\n",
        "        \n",
        "        total_loss = 0\n",
        "        total_noise_loss = 0\n",
        "        total_recon_loss = 0\n",
        "        total_kl_loss = 0\n",
        "        \n",
        "        progress_bar = tqdm(dataloader, desc=f\"Epoch {epoch}\")\n",
        "        \n",
        "        for batch_idx, batch in enumerate(progress_bar):\n",
        "            images = batch['image'].to(self.device)\n",
        "            prompts = batch['prompt']\n",
        "            \n",
        "            # Generate random timesteps\n",
        "            timesteps = torch.randint(0, self.scheduler.num_train_timesteps, (images.shape[0],), device=self.device)\n",
        "            \n",
        "            # Training step\n",
        "            losses = self.train_step(images, prompts, timesteps, batch_idx)\n",
        "            \n",
        "            # Update metrics\n",
        "            total_loss += losses['total_loss']\n",
        "            total_noise_loss += losses['noise_loss']\n",
        "            total_recon_loss += losses['recon_loss']\n",
        "            total_kl_loss += losses['kl_loss']\n",
        "            \n",
        "            # Update progress bar\n",
        "            progress_bar.set_postfix({\n",
        "                'Loss': f\"{losses['total_loss']:.6f}\",\n",
        "                'Noise': f\"{losses['noise_loss']:.6f}\",\n",
        "                'Recon': f\"{losses['recon_loss']:.6f}\",\n",
        "                'KL': f\"{losses['kl_loss']:.6f}\"\n",
        "            })\n",
        "        \n",
        "        # Update learning rates\n",
        "        self.vae_scheduler.step()\n",
        "        self.unet_scheduler.step()\n",
        "        \n",
        "        # Calculate averages\n",
        "        num_batches = len(dataloader)\n",
        "        avg_loss = total_loss / num_batches\n",
        "        avg_noise_loss = total_noise_loss / num_batches\n",
        "        avg_recon_loss = total_recon_loss / num_batches\n",
        "        avg_kl_loss = total_kl_loss / num_batches\n",
        "        \n",
        "        print(f\"\\n\ud83d\udcca Epoch {epoch} Training Results:\")\n",
        "        print(f\"   \u2022 Average Total Loss: {avg_loss:.6f}\")\n",
        "        print(f\"   \u2022 Average Noise Loss: {avg_noise_loss:.6f}\")\n",
        "        print(f\"   \u2022 Average Recon Loss: {avg_recon_loss:.6f}\")\n",
        "        print(f\"   \u2022 Average KL Loss: {avg_kl_loss:.6f}\")\n",
        "        print(f\"   \u2022 VAE LR: {self.vae_scheduler.get_last_lr()[0]:.2e}\")\n",
        "        print(f\"   \u2022 UNet LR: {self.unet_scheduler.get_last_lr()[0]:.2e}\")\n",
        "        \n",
        "        return {\n",
        "            'train_loss': avg_loss,\n",
        "            'noise_loss': avg_noise_loss,\n",
        "            'recon_loss': avg_recon_loss,\n",
        "            'kl_loss': avg_kl_loss\n",
        "        }\n",
        "    \n",
        "    def validate(self, dataloader):\n",
        "        \"\"\"Validate the model using EMA models\"\"\"\n",
        "        self.vae_ema.apply_shadow()\n",
        "        self.unet_ema.apply_shadow()\n",
        "        \n",
        "        self.vae.eval()\n",
        "        self.unet.eval()\n",
        "        \n",
        "        total_loss = 0\n",
        "        total_noise_loss = 0\n",
        "        total_recon_loss = 0\n",
        "        total_kl_loss = 0\n",
        "        \n",
        "        with torch.no_grad():\n",
        "            for batch in tqdm(dataloader, desc=\"Validation\"):\n",
        "                images = batch['image'].to(self.device)\n",
        "                prompts = batch['prompt']\n",
        "                \n",
        "                # Generate random timesteps\n",
        "                timesteps = torch.randint(0, self.scheduler.num_train_timesteps, (images.shape[0],), device=self.device)\n",
        "                \n",
        "                # Forward pass\n",
        "                latents, mu, logvar, kl_loss = self.vae.encode(images)\n",
        "                text_embeddings = self.encode_text(prompts)\n",
        "                \n",
        "                noise = torch.randn_like(latents)\n",
        "                noisy_latents = self.scheduler.add_noise(latents, noise, timesteps)\n",
        "                noise_pred = self.unet(noisy_latents, timesteps, text_embeddings)\n",
        "                \n",
        "                # Calculate losses\n",
        "                noise_loss = self.mse_loss(noise_pred, noise)\n",
        "                reconstructed = self.vae.decode(latents)\n",
        "                recon_loss = self.mse_loss(reconstructed, images)\n",
        "                total_loss_val = noise_loss + 0.1 * recon_loss + 0.01 * kl_loss\n",
        "                \n",
        "                total_loss += total_loss_val.item()\n",
        "                total_noise_loss += noise_loss.item()\n",
        "                total_recon_loss += recon_loss.item()\n",
        "                total_kl_loss += kl_loss.item()\n",
        "        \n",
        "        # Restore original models\n",
        "        self.vae_ema.restore()\n",
        "        self.unet_ema.restore()\n",
        "        \n",
        "        # Calculate averages\n",
        "        num_batches = len(dataloader)\n",
        "        avg_loss = total_loss / num_batches\n",
        "        avg_noise_loss = total_noise_loss / num_batches\n",
        "        avg_recon_loss = total_recon_loss / num_batches\n",
        "        avg_kl_loss = total_kl_loss / num_batches\n",
        "        \n",
        "        print(f\"\\n\ud83d\udcca Validation Results (EMA):\")\n",
        "        print(f\"   \u2022 Average Total Loss: {avg_loss:.6f}\")\n",
        "        print(f\"   \u2022 Average Noise Loss: {avg_noise_loss:.6f}\")\n",
        "        print(f\"   \u2022 Average Recon Loss: {avg_recon_loss:.6f}\")\n",
        "        print(f\"   \u2022 Average KL Loss: {avg_kl_loss:.6f}\")\n",
        "        \n",
        "        return {\n",
        "            'val_loss': avg_loss,\n",
        "            'noise_loss': avg_noise_loss,\n",
        "            'recon_loss': avg_recon_loss,\n",
        "            'kl_loss': avg_kl_loss\n",
        "        }\n",
        "    \n",
        "    def test_diffusion_components(self):\n",
        "        \"\"\"Test all diffusion components before training\"\"\"\n",
        "        print(\"\ud83e\uddea Testing diffusion components...\")\n",
        "        \n",
        "        # Test VAE\n",
        "        test_image = torch.randn(2, 3, 128, 128).to(self.device)\n",
        "        try:\n",
        "            latents, mu, logvar, kl_loss = self.vae.encode(test_image)\n",
        "            reconstructed = self.vae.decode(latents)\n",
        "            print(f\"\u2705 VAE: input {test_image.shape} \u2192 latents {latents.shape} \u2192 output {reconstructed.shape}\")\n",
        "        except Exception as e:\n",
        "            print(f\"\u274c VAE test failed: {e}\")\n",
        "            return False\n",
        "        \n",
        "        # Test UNet\n",
        "        test_latents = torch.randn(2, 4, 16, 16).to(self.device)\n",
        "        test_timesteps = torch.randint(0, 1000, (2,)).to(self.device)\n",
        "        test_context = torch.randn(2, 77, 512).to(self.device)\n",
        "        \n",
        "        try:\n",
        "            output = self.unet(test_latents, test_timesteps, test_context)\n",
        "            print(f\"\u2705 UNet: input {test_latents.shape} \u2192 output {output.shape}\")\n",
        "        except Exception as e:\n",
        "            print(f\"\u274c UNet test failed: {e}\")\n",
        "            return False\n",
        "        \n",
        "        # Test scheduler\n",
        "        try:\n",
        "            noise = torch.randn_like(test_latents)\n",
        "            noisy = self.scheduler.add_noise(test_latents, noise, test_timesteps)\n",
        "            denoised = self.scheduler.step(noise, test_timesteps, noisy)\n",
        "            print(f\"\u2705 Scheduler: noise addition and denoising successful\")\n",
        "        except Exception as e:\n",
        "            print(f\"\u274c Scheduler test failed: {e}\")\n",
        "            return False\n",
        "        \n",
        "        print(\"\ud83c\udf89 All component tests passed!\")\n",
        "        return True\n",
        "    \n",
        "    def save_checkpoint(self, epoch, metrics, filename=None):\n",
        "        \"\"\"Save model checkpoint with EMA models\"\"\"\n",
        "        if filename is None:\n",
        "            filename = f\"checkpoint_epoch_{epoch}.pth\"\n",
        "        \n",
        "        checkpoint = {\n",
        "            'epoch': epoch,\n",
        "            'vae_state_dict': self.vae.state_dict(),\n",
        "            'unet_state_dict': self.unet.state_dict(),\n",
        "            'vae_ema_state_dict': self.vae_ema.shadow,\n",
        "            'unet_ema_state_dict': self.unet_ema.shadow,\n",
        "            'vae_optimizer_state_dict': self.vae_optimizer.state_dict(),\n",
        "            'unet_optimizer_state_dict': self.unet_optimizer.state_dict(),\n",
        "            'vae_scheduler_state_dict': self.vae_scheduler.state_dict(),\n",
        "            'unet_scheduler_state_dict': self.unet_scheduler.state_dict(),\n",
        "            'metrics': metrics,\n",
        "            'scheduler_config': {\n",
        "                'num_train_timesteps': self.scheduler.num_train_timesteps,\n",
        "                'beta_start': self.scheduler.beta_start,\n",
        "                'beta_end': self.scheduler.beta_end\n",
        "            }\n",
        "        }\n",
        "        \n",
        "        torch.save(checkpoint, filename)\n",
        "        print(f\"\ud83d\udcbe Checkpoint saved: {filename}\")\n",
        "    \n",
        "    def load_checkpoint(self, checkpoint_path):\n",
        "        \"\"\"Load model checkpoint with EMA models\"\"\"\n",
        "        checkpoint = torch.load(checkpoint_path, map_location=self.device)\n",
        "        \n",
        "        self.vae.load_state_dict(checkpoint['vae_state_dict'])\n",
        "        self.unet.load_state_dict(checkpoint['unet_state_dict'])\n",
        "        self.vae_ema.shadow = checkpoint['vae_ema_state_dict']\n",
        "        self.unet_ema.shadow = checkpoint['unet_ema_state_dict']\n",
        "        self.vae_optimizer.load_state_dict(checkpoint['vae_optimizer_state_dict'])\n",
        "        self.unet_optimizer.load_state_dict(checkpoint['unet_optimizer_state_dict'])\n",
        "        self.vae_scheduler.load_state_dict(checkpoint['vae_scheduler_state_dict'])\n",
        "        self.unet_scheduler.load_state_dict(checkpoint['unet_scheduler_state_dict'])\n",
        "        \n",
        "        print(f\"\ud83d\udcc2 Checkpoint loaded: {checkpoint_path}\")\n",
        "        return checkpoint['epoch'], checkpoint['metrics']\n",
        "\n",
        "def main():\n",
        "    \"\"\"Main training function with performance optimizations\"\"\"\n",
        "    print(\"\ud83c\udf8c Stable Diffusion Kanji Training - Performance Optimized\")\n",
        "    print(\"=\" * 60)\n",
        "    \n",
        "    # Configuration with performance optimizations\n",
        "    if torch.cuda.is_available():\n",
        "        device = 'cuda'\n",
        "    elif hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():\n",
        "        device = 'mps'\n",
        "    else:\n",
        "        device = 'cpu'\n",
        "    batch_size = get_optimal_batch_size(device)\n",
        "    num_epochs = 25  # Increased from 10 for better quality\n",
        "    learning_rate = 1e-4\n",
        "    validation_frequency = 5  # Reduce validation frequency for speed\n",
        "    \n",
        "    print(f\"\ud83d\udd27 Configuration:\")\n",
        "    print(f\"   \u2022 Device: {device}\")\n",
        "    print(f\"   \u2022 Batch Size: {batch_size} (auto-optimized)\")\n",
        "    print(f\"   \u2022 Epochs: {num_epochs}\")\n",
        "    print(f\"   \u2022 Learning Rate: {learning_rate}\")\n",
        "    print(f\"   \u2022 Validation Frequency: Every {validation_frequency} epochs\")\n",
        "    \n",
        "    # Data transforms\n",
        "    transform = transforms.Compose([\n",
        "        transforms.Resize((128, 128)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
        "    ])\n",
        "    \n",
        "    # Load dataset\n",
        "    dataset_path = \"data/fixed_kanji_dataset\"\n",
        "    dataset = KanjiDataset(dataset_path, transform=transform, max_samples=2000)  # Increased for better training\n",
        "    \n",
        "    # Split dataset\n",
        "    train_size = int(0.8 * len(dataset))\n",
        "    val_size = len(dataset) - train_size\n",
        "    train_dataset, val_dataset = torch.utils.data.random_split(dataset, [train_size, val_size])\n",
        "    \n",
        "    # Create dataloaders with optimized settings\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=4, pin_memory=True)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=4, pin_memory=True)\n",
        "    \n",
        "    print(f\"\ud83d\udcca Dataset: {len(dataset)} total, {len(train_dataset)} train, {len(val_dataset)} val\")\n",
        "    \n",
        "    # Initialize trainer\n",
        "    trainer = StableDiffusionTrainer(device=device)\n",
        "    \n",
        "    # Test components before training\n",
        "    if not trainer.test_diffusion_components():\n",
        "        print(\"\u274c Component tests failed. Exiting.\")\n",
        "        return\n",
        "    \n",
        "    # Training loop with performance optimizations\n",
        "    best_val_loss = float('inf')\n",
        "    train_metrics = []\n",
        "    val_metrics = []\n",
        "    \n",
        "    print(f\"\\n\ud83d\ude80 Starting optimized training for {num_epochs} epochs...\")\n",
        "    \n",
        "    for epoch in range(1, num_epochs + 1):\n",
        "        print(f\"\\n{'='*20} Epoch {epoch}/{num_epochs} {'='*20}\")\n",
        "        \n",
        "        # Train\n",
        "        train_results = trainer.train_epoch(train_loader, epoch)\n",
        "        train_metrics.append(train_results)\n",
        "        \n",
        "        # Validate less frequently for speed\n",
        "        if epoch % validation_frequency == 0 or epoch == num_epochs:\n",
        "            val_results = trainer.validate(val_loader)\n",
        "            val_metrics.append(val_results)\n",
        "            \n",
        "            # Save best model\n",
        "            if val_results['val_loss'] < best_val_loss:\n",
        "                best_val_loss = val_results['val_loss']\n",
        "                trainer.save_checkpoint(epoch, val_results, \"best_model.pth\")\n",
        "        \n",
        "        # Save regular checkpoint\n",
        "        if epoch % 10 == 0:\n",
        "            trainer.save_checkpoint(epoch, train_results)\n",
        "    \n",
        "    print(f\"\\n\ud83c\udf89 Training complete!\")\n",
        "    print(f\"   \u2022 Best validation loss: {best_val_loss:.6f}\")\n",
        "    \n",
        "    # Save final model\n",
        "    trainer.save_checkpoint(num_epochs, train_results[-1], \"final_model.pth\")\n",
        "    \n",
        "    # Plot training curves\n",
        "    try:\n",
        "        epochs = range(1, len(train_metrics) + 1)\n",
        "        \n",
        "        plt.figure(figsize=(15, 5))\n",
        "        \n",
        "        # Loss plot\n",
        "        plt.subplot(1, 3, 1)\n",
        "        plt.plot(epochs, [m['train_loss'] for m in train_metrics], 'b-', label='Train')\n",
        "        if val_metrics:\n",
        "            val_epochs = [i for i in epochs if i % validation_frequency == 0 or i == num_epochs]\n",
        "            plt.plot(val_epochs, [m['val_loss'] for m in val_metrics], 'r-', label='Validation')\n",
        "        plt.title('Total Loss')\n",
        "        plt.xlabel('Epoch')\n",
        "        plt.ylabel('Loss')\n",
        "        plt.legend()\n",
        "        plt.grid(True)\n",
        "        \n",
        "        # Noise loss plot\n",
        "        plt.subplot(1, 3, 2)\n",
        "        plt.plot(epochs, [m['noise_loss'] for m in train_metrics], 'b-', label='Train')\n",
        "        if val_metrics:\n",
        "            plt.plot(val_epochs, [m['noise_loss'] for m in val_metrics], 'r-', label='Validation')\n",
        "        plt.title('Noise Prediction Loss')\n",
        "        plt.xlabel('Epoch')\n",
        "        plt.ylabel('Loss')\n",
        "        plt.legend()\n",
        "        plt.grid(True)\n",
        "        \n",
        "        # KL loss plot\n",
        "        plt.subplot(1, 3, 3)\n",
        "        plt.plot(epochs, [m['kl_loss'] for m in train_metrics], 'b-', label='Train')\n",
        "        if val_metrics:\n",
        "            plt.plot(val_epochs, [m['kl_loss'] for m in val_metrics], 'r-', label='Validation')\n",
        "        plt.title('KL Divergence Loss')\n",
        "        plt.xlabel('Epoch')\n",
        "        plt.ylabel('Loss')\n",
        "        plt.legend()\n",
        "        plt.grid(True)\n",
        "        \n",
        "        plt.tight_layout()\n",
        "        plt.savefig('training_curves_optimized.png', dpi=300, bbox_inches='tight')\n",
        "        plt.show()\n",
        "        \n",
        "        print(\"\ud83d\udcc8 Training curves saved as 'training_curves_optimized.png'\")\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"\u26a0\ufe0f  Could not plot training curves: {e}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "outputs": [],
      "execution_count": null
    }
  ],
  "metadata": {
    "anaconda-cloud": {},
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}