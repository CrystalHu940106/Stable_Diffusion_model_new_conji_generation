{
  "cells": [
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#!/usr/bin/env python3\n",
        "\"\"\"\n",
        "Generate Kanji for Specific Concepts\n",
        "Generate kanji for: success, failure, novel, funny, culturally meaningful\n",
        "\"\"\"\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "import json\n",
        "from pathlib import Path\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import random\n",
        "\n",
        "class SimpleUNet(nn.Module):\n",
        "    \"\"\"Simple UNet for generation\"\"\"\n",
        "    \n",
        "    def __init__(self, in_channels=3, out_channels=3, image_size=64):\n",
        "        super().__init__()\n",
        "        \n",
        "        # Simple encoder-decoder structure\n",
        "        self.encoder = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, 32, 3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(32, 64, 3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2),\n",
        "            \n",
        "            nn.Conv2d(64, 64, 3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(64, 128, 3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2),\n",
        "        )\n",
        "        \n",
        "        self.decoder = nn.Sequential(\n",
        "            nn.ConvTranspose2d(128, 64, 2, stride=2),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(64, 64, 3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            \n",
        "            nn.ConvTranspose2d(64, 32, 2, stride=2),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(32, out_channels, 3, padding=1),\n",
        "            nn.Tanh()\n",
        "        )\n",
        "    \n",
        "    def forward(self, x):\n",
        "        encoded = self.encoder(x)\n",
        "        decoded = self.decoder(encoded)\n",
        "        return decoded\n",
        "\n",
        "def load_trained_model():\n",
        "    \"\"\"Load the trained model\"\"\"\n",
        "    model = SimpleUNet(in_channels=3, out_channels=3, image_size=64)\n",
        "    \n",
        "    checkpoint_path = Path(\"quick_test_results/quick_test_epoch_2.pth\")\n",
        "    if not checkpoint_path.exists():\n",
        "        print(\"\u274c Checkpoint not found! Please run quick_train_test.py first.\")\n",
        "        return None\n",
        "    \n",
        "    checkpoint = torch.load(checkpoint_path, map_location='cpu')\n",
        "    model.load_state_dict(checkpoint['model_state_dict'])\n",
        "    model.eval()\n",
        "    \n",
        "    print(f\"\u2705 Model loaded from epoch {checkpoint['epoch']}\")\n",
        "    print(f\"   \u2022 Loss: {checkpoint['loss']:.6f}\")\n",
        "    \n",
        "    return model\n",
        "\n",
        "def create_concept_noise(concept, image_size=64, batch_size=4):\n",
        "    \"\"\"Create concept-specific noise for generation\"\"\"\n",
        "    \n",
        "    # Set random seed based on concept for consistency\n",
        "    concept_seeds = {\n",
        "        'success': 42,\n",
        "        'failure': 123,\n",
        "        'novel': 456,\n",
        "        'funny': 789,\n",
        "        'culturally_meaningful': 999\n",
        "    }\n",
        "    \n",
        "    seed = concept_seeds.get(concept, random.randint(1, 1000))\n",
        "    torch.manual_seed(seed)\n",
        "    \n",
        "    # Create different types of noise based on concept\n",
        "    if concept == 'success':\n",
        "        # More structured, upward patterns\n",
        "        noise = torch.randn(batch_size, 3, image_size, image_size) * 0.5\n",
        "        # Add some upward gradient\n",
        "        for i in range(batch_size):\n",
        "            for c in range(3):\n",
        "                for y in range(image_size):\n",
        "                    noise[i, c, y, :] += (y / image_size) * 0.3\n",
        "    elif concept == 'failure':\n",
        "        # More chaotic, downward patterns\n",
        "        noise = torch.randn(batch_size, 3, image_size, image_size) * 0.8\n",
        "        # Add some downward gradient\n",
        "        for i in range(batch_size):\n",
        "            for c in range(3):\n",
        "                for y in range(image_size):\n",
        "                    noise[i, c, y, :] -= (y / image_size) * 0.3\n",
        "    elif concept == 'novel':\n",
        "        # Creative, asymmetric patterns\n",
        "        noise = torch.randn(batch_size, 3, image_size, image_size) * 0.6\n",
        "        # Add some asymmetry\n",
        "        for i in range(batch_size):\n",
        "            for c in range(3):\n",
        "                noise[i, c, :, :image_size//2] *= 1.2\n",
        "    elif concept == 'funny':\n",
        "        # Playful, curved patterns\n",
        "        noise = torch.randn(batch_size, 3, image_size, image_size) * 0.7\n",
        "        # Add some wave-like patterns\n",
        "        for i in range(batch_size):\n",
        "            for c in range(3):\n",
        "                for y in range(image_size):\n",
        "                    wave = np.sin(y * 0.2) * 0.2\n",
        "                    noise[i, c, y, :] += wave\n",
        "    elif concept == 'culturally_meaningful':\n",
        "        # Balanced, harmonious patterns\n",
        "        noise = torch.randn(batch_size, 3, image_size, image_size) * 0.4\n",
        "        # Add some symmetry\n",
        "        for i in range(batch_size):\n",
        "            for c in range(3):\n",
        "                left_half = noise[i, c, :, :image_size//2]\n",
        "                right_half = torch.flip(left_half, dims=[1])\n",
        "                noise[i, c, :, image_size//2:] = right_half\n",
        "    else:\n",
        "        # Default random noise\n",
        "        noise = torch.randn(batch_size, 3, image_size, image_size)\n",
        "    \n",
        "    return noise\n",
        "\n",
        "def denormalize(tensor):\n",
        "    \"\"\"Denormalize tensor back to [0, 1] range\"\"\"\n",
        "    return (tensor + 1) / 2\n",
        "\n",
        "def generate_concept_kanji(model, concept, num_samples=4):\n",
        "    \"\"\"Generate kanji for a specific concept\"\"\"\n",
        "    \n",
        "    print(f\"\\n\ud83c\udfaf Generating Kanji for '{concept}'...\")\n",
        "    \n",
        "    # Create concept-specific noise\n",
        "    noise_input = create_concept_noise(concept, batch_size=num_samples)\n",
        "    \n",
        "    # Generate\n",
        "    with torch.no_grad():\n",
        "        generated = model(noise_input)\n",
        "    \n",
        "    print(f\"   \u2022 Generated {num_samples} samples\")\n",
        "    print(f\"   \u2022 Input shape: {noise_input.shape}\")\n",
        "    print(f\"   \u2022 Output shape: {generated.shape}\")\n",
        "    \n",
        "    return generated\n",
        "\n",
        "def save_concept_images(generated, concept, save_dir=\"generated_results\"):\n",
        "    \"\"\"Save generated images for a concept\"\"\"\n",
        "    \n",
        "    save_path = Path(save_dir)\n",
        "    save_path.mkdir(exist_ok=True)\n",
        "    \n",
        "    # Convert from [-1, 1] to [0, 1] range\n",
        "    generated = denormalize(generated)\n",
        "    generated = torch.clamp(generated, 0, 1)\n",
        "    \n",
        "    # Convert to PIL images and save\n",
        "    saved_paths = []\n",
        "    for i in range(generated.shape[0]):\n",
        "        # Convert tensor to numpy\n",
        "        img_array = generated[i].permute(1, 2, 0).numpy()\n",
        "        \n",
        "        # Convert to PIL image\n",
        "        img = Image.fromarray((img_array * 255).astype(np.uint8))\n",
        "        \n",
        "        # Save image\n",
        "        img_path = save_path / f\"{concept}_generated_{i+1}.png\"\n",
        "        img.save(img_path)\n",
        "        saved_paths.append(img_path)\n",
        "        print(f\"   \u2022 Saved: {img_path}\")\n",
        "    \n",
        "    return saved_paths\n",
        "\n",
        "def display_concept_results(all_generated, concepts):\n",
        "    \"\"\"Display all generated results\"\"\"\n",
        "    \n",
        "    num_concepts = len(concepts)\n",
        "    num_samples = all_generated[0].shape[0]\n",
        "    \n",
        "    fig, axes = plt.subplots(num_concepts, num_samples, figsize=(4*num_samples, 4*num_concepts))\n",
        "    fig.suptitle('Generated Kanji for Different Concepts', fontsize=16)\n",
        "    \n",
        "    for i, (concept, generated) in enumerate(zip(concepts, all_generated)):\n",
        "        for j in range(num_samples):\n",
        "            if num_concepts == 1:\n",
        "                ax = axes[j]\n",
        "            else:\n",
        "                ax = axes[i, j]\n",
        "            \n",
        "            # Convert to image\n",
        "            img_array = denormalize(generated[j]).permute(1, 2, 0).numpy()\n",
        "            img_array = np.clip(img_array, 0, 1)\n",
        "            \n",
        "            ax.imshow(img_array)\n",
        "            ax.set_title(f'{concept.replace(\"_\", \" \").title()} #{j+1}', fontsize=10)\n",
        "            ax.axis('off')\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    \n",
        "    # Save combined results\n",
        "    output_path = \"all_concept_results.png\"\n",
        "    plt.savefig(output_path, dpi=150, bbox_inches='tight')\n",
        "    print(f\"\\n\ud83d\udcbe Combined results saved to: {output_path}\")\n",
        "    \n",
        "    # Show the plot\n",
        "    plt.show()\n",
        "\n",
        "def analyze_existing_kanji():\n",
        "    \"\"\"Analyze existing kanji that might match our concepts\"\"\"\n",
        "    \n",
        "    print(\"\ud83d\udcda Analyzing existing Kanji for our concepts...\")\n",
        "    \n",
        "    # Load dataset\n",
        "    dataset_path = Path(\"data/fixed_kanji_dataset/metadata/dataset.json\")\n",
        "    if not dataset_path.exists():\n",
        "        print(\"\u274c Dataset not found!\")\n",
        "        return\n",
        "    \n",
        "    with open(dataset_path, 'r', encoding='utf-8') as f:\n",
        "        dataset = json.load(f)\n",
        "    \n",
        "    # Define concept keywords\n",
        "    concept_keywords = {\n",
        "        'success': ['success', 'achieve', 'accomplish', 'complete', 'win', 'victory', 'triumph', 'succeed', 'prosper', 'flourish'],\n",
        "        'failure': ['fail', 'lose', 'defeat', 'error', 'mistake', 'wrong', 'bad', 'negative', 'defeat', 'loss'],\n",
        "        'novel': ['new', 'novel', 'original', 'creative', 'unique', 'different', 'innovative', 'fresh', 'modern'],\n",
        "        'funny': ['funny', 'humorous', 'amusing', 'entertaining', 'comical', 'laugh', 'joke', 'playful', 'witty'],\n",
        "        'culturally_meaningful': ['culture', 'tradition', 'heritage', 'meaningful', 'significant', 'important', 'sacred', 'spiritual', 'philosophy', 'wisdom']\n",
        "    }\n",
        "    \n",
        "    # Find matching kanji for each concept\n",
        "    concept_matches = {}\n",
        "    \n",
        "    for concept, keywords in concept_keywords.items():\n",
        "        matching_kanji = []\n",
        "        for entry in dataset:\n",
        "            meanings = [meaning.lower() for meaning in entry['meanings']]\n",
        "            for keyword in keywords:\n",
        "                if keyword in meanings:\n",
        "                    matching_kanji.append({\n",
        "                        'kanji': entry['kanji'],\n",
        "                        'meanings': entry['meanings'],\n",
        "                        'unicode': entry['unicode']\n",
        "                    })\n",
        "                    break\n",
        "        \n",
        "        concept_matches[concept] = matching_kanji\n",
        "        print(f\"\\n   \ud83d\udcd6 {concept.replace('_', ' ').title()}: {len(matching_kanji)} matches\")\n",
        "        for i, kanji_info in enumerate(matching_kanji[:5]):  # Show first 5\n",
        "            print(f\"     {i+1}. {kanji_info['kanji']} ({kanji_info['unicode']}): {', '.join(kanji_info['meanings'][:3])}\")\n",
        "    \n",
        "    return concept_matches\n",
        "\n",
        "def main():\n",
        "    \"\"\"Main function\"\"\"\n",
        "    \n",
        "    print(\"\ud83c\udf8c Generate Kanji for Specific Concepts\")\n",
        "    print(\"=\" * 50)\n",
        "    \n",
        "    # Analyze existing kanji\n",
        "    concept_matches = analyze_existing_kanji()\n",
        "    \n",
        "    # Load model\n",
        "    model = load_trained_model()\n",
        "    if model is None:\n",
        "        return\n",
        "    \n",
        "    # Define concepts\n",
        "    concepts = ['success', 'failure', 'novel', 'funny', 'culturally_meaningful']\n",
        "    \n",
        "    # Generate for each concept\n",
        "    all_generated = []\n",
        "    all_saved_paths = []\n",
        "    \n",
        "    for concept in concepts:\n",
        "        generated = generate_concept_kanji(model, concept, num_samples=4)\n",
        "        saved_paths = save_concept_images(generated, concept)\n",
        "        \n",
        "        all_generated.append(generated)\n",
        "        all_saved_paths.extend(saved_paths)\n",
        "    \n",
        "    # Display results\n",
        "    display_concept_results(all_generated, concepts)\n",
        "    \n",
        "    # Summary\n",
        "    print(f\"\\n\ud83c\udf89 Generation Complete!\")\n",
        "    print(f\"   \u2022 Generated kanji for {len(concepts)} concepts\")\n",
        "    print(f\"   \u2022 Created {len(all_saved_paths)} images total\")\n",
        "    print(f\"   \u2022 Images saved in: generated_results/\")\n",
        "    \n",
        "    print(f\"\\n\ud83d\udcca Concept Summary:\")\n",
        "    for concept in concepts:\n",
        "        print(f\"   \u2022 {concept.replace('_', ' ').title()}: 4 generated + {len(concept_matches[concept])} existing matches\")\n",
        "    \n",
        "    print(f\"\\n\ud83d\udca1 Analysis:\")\n",
        "    print(f\"   \u2022 Compare generated kanji with existing matches\")\n",
        "    print(f\"   \u2022 Look for patterns that might represent each concept\")\n",
        "    print(f\"   \u2022 Consider cultural and visual elements\")\n",
        "    print(f\"   \u2022 Generated kanji show the model's understanding of visual patterns\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "outputs": [],
      "execution_count": null
    }
  ],
  "metadata": {
    "anaconda-cloud": {},
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}