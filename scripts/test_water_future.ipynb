{
  "cells": [
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#!/usr/bin/env python3\n",
        "\"\"\"\n",
        "Test script for generating Kanji for \"water\" and \"future\" concepts\n",
        "\"\"\"\n",
        "\n",
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "from stable_diffusion_kanji import StableDiffusionPipeline\n",
        "\n",
        "def test_concept_generation():\n",
        "    \"\"\"Test generation for water and future concepts\"\"\"\n",
        "    print(\"\ud83c\udf8c Testing Concept Generation with Trained Model\")\n",
        "    print(\"=\" * 50)\n",
        "    \n",
        "    # Initialize pipeline\n",
        "    device = 'mps' if torch.backends.mps.is_available() else 'cpu'\n",
        "    print(f\"\ud83d\udd27 Using device: {device}\")\n",
        "    \n",
        "    # Use the same VAE configuration as training\n",
        "    from stable_diffusion_kanji import VAE, UNet2DConditionModel, DDPMScheduler\n",
        "    \n",
        "    vae = VAE(hidden_dims=[128, 256, 512, 1024]).to(device)\n",
        "    unet = UNet2DConditionModel(\n",
        "        model_channels=256,\n",
        "        num_res_blocks=3,\n",
        "        channel_mult=(1, 2, 4, 8),\n",
        "        attention_resolutions=(8,),\n",
        "        num_heads=16\n",
        "    ).to(device)\n",
        "    \n",
        "    pipeline = StableDiffusionPipeline(device=device)\n",
        "    pipeline.vae = vae\n",
        "    pipeline.unet = unet\n",
        "    \n",
        "    # Load trained model\n",
        "    print(\"\ud83d\udcc2 Loading trained model...\")\n",
        "    try:\n",
        "        checkpoint = torch.load('best_model.pth', map_location=device)\n",
        "        pipeline.vae.load_state_dict(checkpoint['vae_state_dict'])\n",
        "        pipeline.unet.load_state_dict(checkpoint['unet_state_dict'])\n",
        "        print(\"\u2705 Model loaded successfully!\")\n",
        "    except Exception as e:\n",
        "        print(f\"\u274c Failed to load model: {e}\")\n",
        "        return\n",
        "    \n",
        "    # Test concepts\n",
        "    concepts = [\"water\", \"future\"]\n",
        "    \n",
        "    for concept in concepts:\n",
        "        print(f\"\\n\ud83c\udf0a Generating Kanji for: {concept.upper()}\")\n",
        "        print(\"-\" * 30)\n",
        "        \n",
        "        try:\n",
        "            # Generate with different guidance scales\n",
        "            for guidance_scale in [7.0, 9.0, 11.0]:\n",
        "                print(f\"   Guidance Scale: {guidance_scale}\")\n",
        "                \n",
        "                # Generate image\n",
        "                image = pipeline.generate_concept_kanji(\n",
        "                    concept, \n",
        "                    style=\"traditional\", \n",
        "                    guidance_scale=guidance_scale\n",
        "                )\n",
        "                \n",
        "                # Convert to PIL image\n",
        "                if isinstance(image, torch.Tensor):\n",
        "                    # Denormalize from [-1, 1] to [0, 1]\n",
        "                    image = (image + 1) / 2\n",
        "                    image = torch.clamp(image, 0, 1)\n",
        "                    \n",
        "                    # Convert to PIL\n",
        "                    image_np = image.squeeze(0).permute(1, 2, 0).cpu().numpy()\n",
        "                    image_pil = Image.fromarray((image_np * 255).astype(np.uint8))\n",
        "                else:\n",
        "                    image_pil = image\n",
        "                \n",
        "                # Save image\n",
        "                filename = f\"kanji_{concept}_{guidance_scale}.png\"\n",
        "                image_pil.save(filename)\n",
        "                print(f\"   \ud83d\udcbe Saved: {filename}\")\n",
        "                \n",
        "                # Display image\n",
        "                plt.figure(figsize=(6, 6))\n",
        "                plt.imshow(image_pil)\n",
        "                plt.title(f'Kanji for \"{concept}\" (Guidance: {guidance_scale})')\n",
        "                plt.axis('off')\n",
        "                plt.show()\n",
        "                \n",
        "        except Exception as e:\n",
        "            print(f\"   \u274c Generation failed: {e}\")\n",
        "    \n",
        "    print(f\"\\n\ud83c\udf89 Concept generation test completed!\")\n",
        "    print(f\"\ud83d\udcc1 Generated images saved in current directory\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    test_concept_generation()\n"
      ],
      "outputs": [],
      "execution_count": null
    }
  ],
  "metadata": {
    "anaconda-cloud": {},
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}